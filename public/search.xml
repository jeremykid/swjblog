<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[partial-plots]]></title>
    <url>%2F2019%2F03%2F14%2Fpartial-plots%2F</url>
    <content type="text"><![CDATA[Partial Dependence Plots部分特征图 While feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions. 当特征重要性显示出哪一个特征对预测结果影响最大，部分特征图显示出一个特征如何影响预测结果。 This is useful to answer questions like: 这对回答以下问题非常有帮助。 Controlling for all other house features, what impact do longitude and latitude have on home prices? To restate this, how would similarly sized houses be priced in different areas? 在其他特征都相同的情况下，经纬度是如何影响房价? 换句话说在不同的地区，相同的房子的价格是如何不同的? Are predicted health differences between two groups due to differences in their diets, or due to some other factor? 两个人群中，健康因素是由于饮食不同而变化，还是由于其他因素? If you are familiar with linear or logistic regression models, partial dependence plots can be interepreted similarly to the coefficients in those models. Though, partial dependence plots on sophisticated models can capture more complex patterns than coefficients from simple models. If you aren’t familiar with linear or logistic regressions, don’t worry about this comparison. 如果你对线性回归或者逻辑回归模型熟悉的话，部分特征图可以解释为这些模型特征前面的系数。但是部分特征图可以解决更复杂的模型，而不局限于只是简单模型中的系数。如果你不熟悉线性回归或者逻辑回归模型，也不必担心。 We will show a couple examples, explain the interpretation of these plots, and then review the code to create these plots. 我们会展示几个案例，然后去解释这些图形包括生成这些图形的代码。 How it Works如何运行 Like permutation importance, partial dependence plots are calculated after a model has been fit. The model is fit on real data that has not been artificially manipulated in any way. 和排列重要性类似，部分特征图也必须在模型训练结束之后开始计算。这个模型必须完全基于真实数据（不能有人为的更改）。 In our soccer example, teams may differ in many ways. How many passes they made, shots they took, goals they scored, etc. At first glance, it seems difficult to disentangle the effect of these features. 在我们的足球例子中，每个足球队都有很多方面不同。比如多少次传球，射门，得分等。一开始，我们会觉得很难去解开每个特征对结果的影响。 To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal. 我们可以从一条数据开始去了解部分特征图如何去解决每个特征的影响的。比如这一条数据，这个队伍控球50%，传了100次，射了10次门，并且得了1分。 We will use the fitted model to predict our outcome (probability their player won “man of the game”). But we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis). 我们会用训练好的模型去预测结果(大概是哪位队员会赢得本场比赛的mvp)。但是我们重复改变其中的一个特征来得出一系列预测。我们可以预测如果一个队只控球40%比赛时间。我们继续预测如果一个队只控球50%比赛时间，然后预测60%，以此类推。我们把预测结果放Y轴，然后我们在X轴上把控球从小往大移动。 In this description, we used only a single row of data. Interactions between features may cause the plot for a single row to be atypical. So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis. 解释中我们只用了一条数据。得出来的结果并不是最典型的。所以我们重复这种思想实验并且更改原来的数据。我们可以画出预测结果的平均数在Y轴上。 Code ExampleModel building isn’t our focus, so we won’t focus on the data exploration or model building code. 123456789101112import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifierdata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')y = (data['Man of the Match'] == "Yes") # Convert from string "Yes"/"No" to binaryfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]X = data[feature_names]train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y) For the sake of explanation, our first example uses a Decision Tree which you can see below. In practice, you’ll use more sophistated models for real-world applications. 我们的第一个例子用了决策树。在现实中你会遇到更复杂的模型。 12345from sklearn import treeimport graphviztree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)graphviz.Source(tree_graph) As guidance to read the tree: Leaves with children show their splitting criterion on the top The pair of values at the bottom show the count of True values and False values for the target respectively, of data points in that node of the tree. 为了更方便去理解决策树 叶子上面的解释了如何从顶端分离下来的标准 在最底部上面的值表示了正确还是错误的个数分别有多少 Here is the code to create the Partial Dependence Plot using the PDPBox library. 123456789from matplotlib import pyplot as pltfrom pdpbox import pdp, get_dataset, info_plots# Create the data that we will plotpdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')# plot itpdp.pdp_plot(pdp_goals, 'Goal Scored')plt.show() Here is another example plot: 123456789from matplotlib import pyplot as pltfrom pdpbox import pdp, get_dataset, info_plots# Create the data that we will plotpdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=&apos;Goal Scored&apos;)# plot itpdp.pdp_plot(pdp_goals, &apos;Goal Scored&apos;)plt.show() A few items are worth pointing out as you interpret this plot The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value. A blue shaded area indicates level of confidence 有一些值得在你图上解释的点： Y 轴被解释成从baseline 或者最左边的值开始 预测上的变量 蓝色阴影代表了不同程度的置信区间 From this particular graph, we see that scoring a goal substantially increases your chances of winning “Player of The Game.” But extra goals beyond that appear to have little impact on predictions. 从这一部分的图，我们不难发现进球得分基本上会增加你得本场比赛MVP的概率。但是多出来的进球反而对增加这个概率帮助并不是太大。 Here is another example plot: 12345feature_to_plot = &apos;Distance Covered (Kms)&apos;pdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)pdp.pdp_plot(pdp_dist, feature_to_plot)plt.show() This graph seems too simple to represent reality. But that’s because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model’s structure. You can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model. 这个图看起来吧现实解释得很简单。但是这是因为模型简单。你应该去关注这个是如何解释模型结构的而不局限在决策树。 你可以很容易得比较不同模型。这里是随机森林模型的图。 1234567# Build Random Forest modelrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)pdp.pdp_plot(pdp_dist, feature_to_plot)plt.show() This model thinks you are more likely to win Player of The Game if your players run a total of 100km over the course of the game. Though running much more causes lower predictions. In general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model. Though this dataset is small enough that we would be careful in how we interpret any model. 在这个模型中，你会更倾向于整场比赛跑了超过100km的队员得MVP，其实这个是比较低准确率的预测。 普遍来说，这种平稳的图形比决策树模型更加模糊。我们得非常小心去解释一些小数据模型 2D Partial Dependence Plots 2维的部分特征图 If you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify what this. We will again use the Decision Tree model for this graph. It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself. 如果你对特征之间如何互相影响有兴趣的话，2维的部分特征图会非常有帮助。这里有一个例子可以解释这个。 我们会重复使用决策树模型。它会创造一个简单的图形，虽然简单，但是足够我们去对比到决策树图形本身。 123456# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plotfeatures_to_plot = [&apos;Goal Scored&apos;, &apos;Distance Covered (Kms)&apos;]inter1 = pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type=&apos;contour&apos;)plt.show() This graph shows predictions for any combination of Goals Scored and Distance covered. For example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn’t matter. Can you see this by tracing through the decision tree with 0 goals? But distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too? 图形展示了所有的得分与跑动距离的组合。 比如说我们看到最有可能得mvp的是至少进一个球并且跑步距离接近100km。如果他们得0分的话, 那么跑动距离将没有意义。你能在决策树上去搜索得分0吗？ 一旦他们得分了，那么跑动距离将会影响结果。在2维的部分特征图上你能看到这些，你是否也能在决策树上找到这个呢？ https://www.kaggle.com/dansbecker/partial-plots]]></content>
  </entry>
  <entry>
    <title><![CDATA[Permutation-Importance]]></title>
    <url>%2F2019%2F02%2F26%2Fpermutation-Importance%2F</url>
    <content type="text"><![CDATA[Intro 简介One of the most basic questions we might ask of a model is What features have the biggest impact on predictions? 在数据模型中有一个很基础的问题：哪一个特征对预测结果影响最大。 This concept is called feature importance. I’ve seen feature importance used effectively many times for every purpose in the list of use cases above. 这种概念被叫做特征重要性（我觉得跟特征选择或者特征工程这一系列的概念有联系。）根据以往经验，特征重要性这个概念被很有效的利用在跟中不同的项目上。 There are multiple ways to measure feature importance. Some approaches answer subtly different versions of the question above. Other approaches have documented shortcomings. 有许多种可以测量特征重要性的方法，有一些方法对问题有不同的解释方法，其他的方法有明显的缺点。（总而言之没有一种万能的方法） In this lesson, we’ll focus on permutation importance. Compared to most other approaches, permutation importance is: 在这门课中，我们会介绍交换排列计算重要性的方法。对比其他方法，交换排列计算重要性可以 Fast to calculate 更快计算 Widely used and understood 应用更普遍 Consistent with properties we would want a feature importance measure to have 保持特征重要性测量的一致性 How it Works 应用Permutation importance uses models differently than anything you’ve seen so far, and many people find it confusing at first. So we’ll start with an example to make it more concrete. 交换排列计算重要性和我们之前见到的用model的方法不一样，一开始很多人都会觉得这个很难懂。所以我们会举一个例子让这个更具体，更容易让人接受。 Consider data with the following format: We want to predict a person’s height when they become 20 years old, using data that is available at age 10. 我们想用这些人在十岁的数据来越策他们20岁的身高。 Our data includes useful features (height at age 10), features with little predictive power (socks owned), as well as some other features we won’t focus on in this explanation. 我们的数据包括很多很有用的特征比如10岁的身高，以及一些很弱的特征，比如他们拥有多少袜子，以及其他，我们并不详述。 Permutation importance is calculated after a model has been fitted. So we won’t change the model or change what predictions we’d get for a given value of height, sock-count, etc. 交换排列计算重要性在训练数据之后被计算。所以我们并不会更改数据或是更改预测结果。 Instead we will ask the following question: If I randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data? 相反我们会问以下的问题，如果我让一列随机排列这些预测数据，然后让其他的数据，特征量都不变，然后比较与原结果有多少准确度上的不同。 Randomly re-ordering a single column should cause less accurate predictions, since the resulting data no longer corresponds to anything observed in the real world. Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions. In this case, shuffling height at age 10 would cause terrible predictions. If we shuffled socks owned instead, the resulting predictions wouldn’t suffer nearly as much. 当我们随机重新排列期中一列上的数据，往往会带来更低准确度的预测，因为这个数据和真实世界并不符合。准确度会根据重新排列的那一行对结果影响的权重而降低不同级别。于是我们知道如果重新排列10岁身高哪一行会带来更差的结果预测。如果我们重新排列拥有袜子数量那一列，并不会对结果有多少影响。 With this insight, the process is as follows: 根据我们的观察，过程应该是 Get a trained model 得到一个训练好的模型 Shuffle the values in a single column, make predictions using the resulting dataset. Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. That performance deterioration measures the importance of the variable you just shuffled. 随机排列单独一列的数据，然后来预测结果。通过损失函数来计算有多接近原来的结果。根据我们之前所推论的比较原有结果，表现越差的就代表这个特征对结果正面影响越大。 Return the data to the original order (undoing the shuffle from step 2.) Now repeat step 2 with the next column in the dataset, until you have calculated the importance of each column. 然后还原数据，重复第2步直到我们计算出所有特征的结果影响的重要性。 Code ExampleOur example will use a model that predicts whether a soccer/football team will have the “Man of the Game” winner based on the team’s statistics. The “Man of the Game” award is given to the best player in the game. Model-building isn’t our current focus, so the cell below loads the data and builds a rudimentary model. 关于预测足球队里谁能得到足球先生的预测。 1234567891011import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierdata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')y = (data['Man of the Match'] == "Yes") # Convert from string "Yes"/"No" to binaryfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]X = data[feature_names]train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) Here is how to calculate and show importances with the eli5 library: 12345import eli5from eli5.sklearn import PermutationImportanceperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)eli5.show_weights(perm, feature_names = val_X.columns.tolist()) Interpreting Permutation Importances 解释交换排列计算重要性The values towards the top are the most important features, and those towards the bottom matter least. 在这张图中 最上面的是最重要的特征，反之最下面的是最不重要的特征。 The first number in each row shows how much model performance decreased with a random shuffling (in this case, using “accuracy” as the performance metric). 每一行中的第一个数字显示在一个随机排列之后整个模型的准确度下降了多少。 Like most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next. 和许多在数据科学中发生的事情类似，有很多随机的事情会在随机排列一个特征列的时候发生，我们可以增加随机性通过更多次的随机排列。毕竟每次的随机排列都不会得到同样的结果。 You’ll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn’t matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance. 有时候你甚至很偶然的可以得到一些负值（意思是随机排列那一列的时候准确度反而上升了）。这种情况往往是因为这个被随机排列这里一列本身就是一些无用的数据。当你用更大的数据去训练，这种幸运也会越来越难发生。 In our example, the most important feature was Goals scored. That seems sensible. Soccer fans may have some intuition about whether the orderings of other variables are surprising or not. 在我们的例子中，最重要的特征是进球得分。这非常合理，球迷还是很理性的会把进球数当作选足球先生第一个的参考值。 Referencehttps://www.kaggle.com/dansbecker/permutation-importance]]></content>
  </entry>
  <entry>
    <title><![CDATA[Use Cases for Model Insights]]></title>
    <url>%2F2019%2F02%2F21%2FUse-Cases-for-Model-Insights%2F</url>
    <content type="text"><![CDATA[Use Cases for Model Insights 在data sciense 里用到的直觉 Debugging Information Feature engineering Directing future data collection Informing human decision-making Building Trust 建立信任 DebuggingThe world has a lot of unreliable, disorganized and generally dirty data. You add a potential source of errors as you write preprocessing code. Add in the potential for target leakage and it is the norm rather than the exception to have errors at some point in a real data science projects. 这个世界有许多不现实的，没有很好组织的，普遍的脏数据。当你对代码进行预处理的时候，你要去做一些规则去预防数据上潜在的错误，以及Data Leakage（Data Leakage一些不能用的数据，但是强行用了，使因果关系颠倒）。在真实的数据科学项目上，我们都应该提前制定规则去预防而不是在训练中或者结束用一些exception去去除。 Given the frequency and potentially disastrous consequences of bugs, debugging is one of the most valuable skills in data science. Understanding the patterns a model is finding will help you identify when those are at odds with your knowledge of the real world, and this is typically the first step in tracking down bugs. 考虑到这些数据bug经常会引起一系列的灾难，debugging 在数据科学中是最有价值的技能之一。你需要理解这个数据模型以及一部分真实世界的常识去作为定位bug的第一步 Informing Feature EngineeringFeature engineering is usually the most effective way to improve model accuracy. Feature engineering usually involves repeatedly creating new features using transformations of your raw data or features you have previously created. 特征上的分析运算往往是加强数据模型最有效的方法。Feature Engineering包括重复不停去利用这些特征做一些变化组合来得到新的特征 Sometimes you can go through this process using nothing but intuition about the underlying topic. But you’ll need more direction when you have 100s of raw features or when you lack background knowledge about the topic you are working on. 有时候你只要用你对某一领域或者课题的直觉，但是当你缺少这个领域的知识的时候且有很多特征量，你需要试验更多的方向。 A Kaggle competition to predict loan defaults gives an extreme example. This competition had 100s of raw features. For privacy reasons, the features had names like f1, f2, f3 rather than common English names. This simulated a scenario where you have little intuition about the raw data. 有一个kaggle 的竞赛去预测loan defaults（默认贷款之类的），给了一个极端的例子。这次竞赛有一百个原始特征，但是由于一些隐私问题，所有的特征都被命名为f1,f2,f3, 而不是常规的特征名字。这个时候就模拟了一种你并不拥有原始数据的背景知识的情况。 One competitor found that the difference between two of the features, specificallyf527 - f528, created a very powerful new feature. Models including that difference as a feature were far better than models without it. But how might you think of creating this variable when you start with hundreds of variables? 有一个参赛者发现有两个特征的差，尤其是f527 - f528，这个差值或者创造的新的特征对结果的影响非常大。但是这也只能说是一个巧合当你需要去分析成百上千的特征 The techniques you’ll learn in this course would make it transparent that f527 and f528 are important features, and that their role is tightly entangled. This will direct you to consider transformations of these two variables, and likely find the “golden feature” of f527 - f528. 你从这门课程学到的技巧可以更轻易的分辨出f527 和 f528 是非常重要的特征。并且他们的角色是互相纠缠影响的。这个方向就有利于我们去思考怎么对这两个特征进行变形，然后有可能我们就可以发现那种极品的特征 比如f527 - f528 As an increasing number of datasets start with 100s or 1000s of raw features, this approach is becoming increasingly important. 随着特征量的增加从100 到1000 数量级，这个方法变得尤为重要。 Directing Future Data Collection对收集新数据的指向 You have no control over datasets you download online. But many businesses and organizations using data science have opportunities to expand what types of data they collect. Collecting new types of data can be expensive or inconvenient, so they only want to do this if they know it will be worthwhile. Model-based insights give you a good understanding of the value of features you currently have, which will help you reason about what new values may be most helpful. 我们并不能控制我们从网上下载的数据库。但是很多公司或者一些机构会利用数据科学找机会去拓展一部分新数据，当然刚开始获取新数据的代价是非常高的。所以他们往往需要模型结构的帮助去分辨获取新数据是否值得，以及新的数据能给整个项目带来多大的变化。 Informing Human Decision-Making一些人类的决定 Some decisions are made automatically by models. Amazon doesn’t have humans (or elves) scurry to decide what to show you whenever you go to their website. But many important decisions are made by humans. For these decisions, insights can be more valuable than predictions. 模型会自动做出一些决定。Amazon 并没有神仙来时时刻刻决定你到Amazon网站看到了神马。但是很多重要的决定还是由人类决定，直觉有的时候比机器预测做出更有价值的决定。 Building Trust建立信任 Many people won’t assume they can trust your model for important decisions without verifying some basic facts. This is a smart precaution given the frequency of data errors. In practice, showing insights that fit their general understanding of the problem will help build trust, even among people with little deep knowledge of data science. 许多人并不依靠我们的模型去做一些重要的决定，如果我们的模型没办法符合很基础的事实。人类本能就会预防一些未知的错误，包括数据的错误。在测试中，我们会用直觉去理解这个问题，让整个数据结构变得合理并却让一些没有数据科学训练的人也容易接受。 Reference: https://www.kaggle.com/dansbecker/use-cases-for-model-insights]]></content>
  </entry>
  <entry>
    <title><![CDATA[加拿大发现外星信号有感]]></title>
    <url>%2F2019%2F01%2F22%2F%E5%8A%A0%E6%8B%BF%E5%A4%A7%E5%8F%91%E7%8E%B0%E5%A4%96%E6%98%9F%E4%BF%A1%E5%8F%B7%E6%9C%89%E6%84%9F%2F</url>
    <content type="text"><![CDATA[人類發展史與外星文明 經過2019 一月份的加拿大天文臺收到疑爲外星文明信號事件，和朋友聊了一下關於爲什麽人類這樣高智慧型的文明存在的最重要的元素。 由於我并不是個文科生，也沒有豐富的人類學，歷史學等積澱與素養。我的觀點有點奇葩，我覺得就是幸運或者是運氣。可能部分符合大過濾器的一些猜想。 太陽這顆恆星非常的穩定，并卻地球在太陽系的位置非常有利於液態水的存在。原諒我是一個三體迷，我深深得同意大劉，地球的確是一個得天獨厚的天堂這一觀點。稀有地球理論 接下來這一點比較奇葩，也是我不太贊同大劉的所有觀點的地方。 如果按照大劉的兩條邏輯鏈：技術爆炸和黑暗森林法則。那麽我覺得完全按照這兩個邏輯生長的文明社會反而不會擁有最强大的科技。也是被金庸的想法所影響，在天龍八部裏，鳩摩智用小無相功驅動少林七十二絕技。但是按照掃地僧所説的每一種佛法都對應一種絕技，如果只練絕技而不修行佛法就會傷及自身。 愚以爲這是一種比較大的智慧，如果把這個思想對比到人類社會思想文明和科技水平。如果只是針對科技發展，而不等待社會文明的進步。那麽這個生物文明就會陷入一種岌岌可危自我毀滅的可能性。 我知道這個比較難以闡述，比如説人人都有可能造出毀滅地球的一種武器，無論是核武器還是生化武器，如果思想道德還沒有到達那種水平比如西方的政治正確人人平等或者是東方孔子的己所不欲勿施於人。那麽一些極端主義就容易生成那種老子活不好你們也別想活的地步。他們手裏又非常容易擁有那種高級的科技。我本人非常相信這種可能性。如同現在小部分的恐怖分子又或者極端主義 儅大劉的邏輯鏈成立的時候，他是把文明當成個體,但是如果換成每個人都是黑暗森林法則中的個體的時候，科技文明就只剩下隱藏自己，以及儅森林獵人這兩種。 所以我認爲人類的幸運在於我們的道德文明，又或者社會文明走在了科技文明的前面，我們并沒有2000年前大規模使用電器，1000年前發展出核武器。并沒有讓一個部落文明或者奴隸制社會擁有大規模殺傷性武器。 當然我朋友也跟我爭論，他認爲社會文明，道德文明是科技文明發展的基礎。我覺得不完全正確，比如歐洲當時很多貴族在容易滿足生理需求的前提下，也經常去研究數學以及其他科學。甚至我認爲即使是奴隸制的社會架構下也是可以誕生高科技產物的。但是這個保留有意見。 可以預想如果成千上萬的奴隸主都掌握核心技術，和強破壞性武器，按概率學上來説，只要有一個發瘋想制裁別人，那麽這個文明就亡了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker — 从入门到实践]]></title>
    <url>%2F2018%2F12%2F14%2FDocker-%E2%80%94-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[Docker 有三个基本概念： 镜像 Image 容器 Container 仓库 Repository 镜像Docker 镜像（Image）就相当于是一个 root 文件系统。Docker 镜像 是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 分层存储镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 容器https://yeasy.gitbooks.io/docker_practice]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning Note 1]]></title>
    <url>%2F2018%2F12%2F14%2FReinforcement-Learning-Note-1%2F</url>
    <content type="text"></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学手册]]></title>
    <url>%2F2018%2F07%2F30%2F%E6%95%B0%E5%AD%A6%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[PCA的数学原理PCA（Principal Component Analysis）是一种常用的数据分析方法(降维) 1. 数据的向量表示及降维问题我们要在降维的同时让数据信息资源的损失尽可能降低. 朴素降维思考，比如男女column， M F，可以去掉一列，因为非黑即白。 eg: 数据记录为 (日期, 浏览量, 访客数, 下单数, 成交数, 成交金额) =&gt; （500，240，25，13，2312.15）T 比如浏览量和访客数是有关联的，可以简单降维 2. 向量的表示及基变换3. 内积与投影内积, 高中学的叫点乘，或者还看见有人叫点积（Dot Product） $ \vec A \vec B = a_1 x b_1 + … + a_n x b_n $ 在几何定义为向量A 和 向量B的长度 乘以 cos（夹角）如果B的模长度为1，那么AB 的点乘就是A 投影在B上的长度 $ \vec A \vec B = \left\lvert \vec A \vec B \right\rvert x cos(\theta) $ 4. 基 basis) 原理上是在x轴上的投影为3，在y轴上的投影为2。 从代数原理上，（3，2） = 3x（1，0）+2x（0，1） （1，0） 和 （0，1） 就是二维空间中的一组基，基的模长度往往为1 我们可以变换这组基的方向，比如（1，1）， （-1，1），然后把他转化为基，即为 (\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})和(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})。 基变换 （二维）顺时针 theta 角度 cos(theta) -sin(theta)sin(theta) cos(theta) 5. 基变换的矩阵表示原坐标 [1 0], [0 1] [1 0] [3] =&gt; [3][0 1] [2] [2] 新坐标$$ (\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}) $$ $$ (-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}) $$ 当把M个N维向量变换到R个N维向量表示的新空间中去，数学表达式为 R是可以小于N的，R是基的数量，可以讲N维数据变换到更低的维度中去。这种矩阵相乘表示为降维变换。 6. 协方差矩阵及优化目标关于如何选择最优的基，比如从N维向量降维到K。 例子 把五条2维向量 降维到 1维空间 (1,1) (1,3) (2,3) (4,4) (2,4) 先减去均值 (-1,-2) (-1,0) (0,0) (2,1) (0,1) 如果选一条线当一维坐标，如果选X轴(0,1) (0,0) 重叠， 如果选y轴(-1,0), (0,0) 重叠 我的理解是应该选一种空间 让每个投影都尽量分散 7. 方差$$ Var(a)=\frac{1}{m}\sum_{i=1}^m{(a_i-\mu)^2} $$ 上面提到的问题变成寻找一个一维基上投影的点方差最大 8. 协方差如果降到二维，还是坚持最大方差的话那么第二条线会与第一条线重合，但是最理想情况是第二条线和第一条线独立。 协方差$$ Cov(a,b)=\frac{1}{m}\sum_{i=1}^m{a_ib_i} $$ 当协方差为0的时候两个向量完全独立，也就是正交（是垂直吗？） 至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。 9. 协方差矩阵（数学真是太有意思了） 假设我们有两个字段a,b, 我们把它们按行组成矩阵 X $$ X=\begin{pmatrix} a_1 &amp; a_2 &amp; \cdots &amp; a_m \ b_1 &amp; b_2 &amp; \cdots &amp; b_m \end{pmatrix} $$ 然后自己相乘并乘上系数 $$ \frac{1}{m}XX^\mathsf{T}=\begin{pmatrix} \frac{1}{m}\sum_{i=1}^m{ai^2} &amp; \frac{1}{m}\sum{i=1}^m{a_ibi} \ \frac{1}{m}\sum{i=1}^m{a_ibi} &amp; \frac{1}{m}\sum{i=1}^m{b_i^2} \end{pmatrix} $$ 结果大概是[a方差， 协方差][协方差, b方差] 10. 协方差矩阵对角化于是根据上面的推导，我们要让对角线上的数据尽可能的高，因为方差变大，然后非对角线的协方差尽可能的小，因为每个基要独立。这样就达成了优化的条件。 接下来有点难懂：我先把每个数据都列下来 m为字段数(我的理解是需要投影的点) n为基的维度 (我的理解是原数据的维度：可能错) X 为基组成的矩阵 m x n C 为一个对称矩阵 P 为行向量基组成的矩阵 r x m (r是什么：r可能是下降后的基的数目) Y = PX Y是P投影在空间X上，X对P做的基变换 $$ \begin{array}{l l l} D &amp; = &amp; \frac{1}{m}YY^\mathsf{T} \ &amp; = &amp; \frac{1}{m}(PX)(PX)^\mathsf{T} \ &amp; = &amp; \frac{1}{m}PXX^\mathsf{T}P^\mathsf{T} \ &amp; = &amp; P(\frac{1}{m}XX^\mathsf{T})P^\mathsf{T} \ &amp; = &amp; PCP^\mathsf{T} \end{array} $$ 要找到P，满足 $$ PCP^\mathsf{T} $$ 是一个对角矩阵 $$ PCP^\mathsf{T} $$ 对角元素从大到小排列，降到K维就取K行 实对称矩阵协方差矩阵 C 是一个实对成矩阵(还未消化) 1 - 实对称矩阵不同特征值对应的特征向量必然正交。 2 - 设特征向量lambda重数为r，则必然存在r个线性无关的特征向量对应于lambda，因此可以将这r个特征向量单位正交化。 P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量, P按照D中特征值的从大到小排列，P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要降维后的矩阵Y 11. 算法及实例总结一下PCA的算法步骤： 设有m条n维数据。 1）将原始数据按列组成n行m列矩阵X 2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 3）$$ 求出协方差矩阵C=\frac{1}{m}XX^\mathsf{T} $$ 4）求出协方差矩阵的特征值及对应的特征向量 特征值向量求法 5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P 6）Y=PX即为降维到k维后的数据 举例那个去reference里面看更清楚，准备翻译成 IPython 更清晰解释PCA 降维 Reference知乎PCA的数学原理 貌似也是转的，找不到原link]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n-note-6]]></title>
    <url>%2F2018%2F07%2F29%2Fcs231n-note-6%2F</url>
    <content type="text"><![CDATA[神经网络笔记2设置数据和模型神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算, 这些做法共同定义了评分函数(score function) 数据预处理3个常用的符号，数据矩阵X，假设其尺寸是[N x D]（N是数据样本的数量，D是数据的维度） 均值减法(Mean subtraction):每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点. numpy代码： 1X -= np.mean(X, axis=0) 归一化（Normalization）数据的所有维度都归一化，使其数值范围都近似相等. 有两种方法： 第一种是先对数据做零中心化,然后每个维度都除以其标准差 numpy 代码 12X -= np.mean(X, axis=0)X /= np.std(X, axis=0) 第二种方法是对每个维度都做归一化，每个维度最大值1，最小值-1 PCA和白化（Whitening）To learning123# 假设输入数据矩阵X的尺寸为[N x D]X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵 Reference:神经网络笔记 2]]></content>
  </entry>
  <entry>
    <title><![CDATA[cs231n-note-5]]></title>
    <url>%2F2018%2F07%2F16%2Fcs231n-note-5%2F</url>
    <content type="text"><![CDATA[神经网络笔记1对比线性代数算法基础的线性代数算法 s = Wx, x 是一个输入矩阵 [n 1], W 是一个权重矩阵 [种类 n]，s 为一个评分矩阵。 神经网络算法简介神经网络算法是 s = W_2 max(0, W_1x). 在这种情况下 W_1 广度增加，比如可能为 [100 * n], max 是通过设置熵值来过滤掉所有的小于0的score。其实有其他方法来过滤。 W_2 是一个[种类 * 100] 的矩阵， 权重W 通过梯度下降来学习。 单个神经元 建模神经网络是从生物上得到的启发。 生物动机与连接!()[https://pic4.zhimg.com/80/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg] 当多个信号传进神经元，与神经元内权重相乘并且相加，如果超过某一个阈值，那么激活神经元。激活函数最早接触的sigmoid，最大优势是将数据控制在【0，1】之间。 一个神经元前向传播的代码是： 1234567class Neuron(object): # ... def forward(inputs): &quot;&quot;&quot; 假设输入和权重是1-D的numpy数组，偏差是一个数字 &quot;&quot;&quot; cell_body_sum = np.sum(inputs * self.weights) + self.bias firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid激活函数 return firing_rate —- Todo —– 作为线性分类器的单个神经元分类器我的理解还是吧score 转化为正确概率。然后进行一个loss 的计算, 前面有公式 二分类Softmax分类器二分类SVM分类器 分类器 公式 优点 缺点 Softmax分类器 softmax函数 和为1 较慢的学习曲线，所有的结果都有loss SVM分类器 在0和得分差中选最大值 更快的去除一些失误值 有时候学习会卡住 常用激活函数我的理解，激活函数就是通过score 得到概率。 激活函数 优点 缺点 Sigmoid 简单，易理解 Sigmoid函数饱和使梯度消失，输出不是零中心的 Tanh 输出不是零中心的，易理解 有时候会 ReLU 1. 线性，非饱和的公式，随机梯度下降的收敛有巨大的加速 2，耗费较少计算资源的操作 学习率太高的时候，这个ReLU单元在训练中将不可逆转的死亡 Leaky ReLU 同上，并且解决了ReLu单元死亡的问题 然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰（不太懂） Maxout 有以上所有的优点 参数过多 神经网络结构最普通的层的类型是全连接层（fully-connected layer） 命名规则 N层神经网络 = hidden layer + output layer 输出层 大多用于表示分类评分值 网络尺寸 标准主要有两个：一个是神经元的个数，另一个是参数的个数 前向传播123456# 一个3层神经网络的前向传播:f = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)h1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)h2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)out = np.dot(W3, h2) + b3 # 神经元输出(1x1) 神经网络最后一层通常是没有激活函数的, 得出一个实数值的评分 表达能力至少拥有一个隐层(hidden)的神经网络是一个通用的近似器，神经网络可以近似任何连续函数。 实践而言，构建更多层的神经网络所表达出来的函数不仅平滑，而且更容易学习（利用最优化） 设置层的数量和尺寸 更大的神经网络可以表达出更复杂的函数,但是缺点是过拟合(overfitting),只是重视数据在复杂情况中的分类，而忽略了潜在关系。 这时候合适的layer可以在测试数据里获得更好的泛化(generalization)能力 然而, 防止神经网络过拟合的方法有很多(To learn), 选择其他过拟合的解决方法，而不应该去选择小的神经网络。 这个是提供的测试的链接convnetjs DEMO Referenceconvnetjs DEMO 神经网络笔记1（上） 神经网络笔记1（下）]]></content>
  </entry>
  <entry>
    <title><![CDATA[中国剩余定理]]></title>
    <url>%2F2018%2F07%2F02%2F%E4%B8%AD%E5%9B%BD%E5%89%A9%E4%BD%99%E5%AE%9A%E7%90%86%2F</url>
    <content type="text"><![CDATA[在去年曾经tutor过一些数学竞赛，很多题目有关一个数除x余y， 然后求这个数是多少的。 比如 一个数当除以3余2，除以5余3，除以7余2，那么这个数是多少 x = 2 mod 3 x = 3 mod 5 x = 2 mod 7 这个时候分成3个mod 部分分别求基础解。 比如第一部分 另外两个mod的乘积为 5 x 7 = 35 然后在mod 3 中求 35 的倒数 35 x 2 = 1 mod 3 再把 倒数 x 一开始余的数 x 另外两个mod 的乘积 = 2 x 2 x 35 = 140 然后同理，求出另外两个mod的结果再加在一起 = 233， 那么所有233 + (3x5x7) x k 的数都满足这个条件 接下来我想尝试写一个python 程序解决所有诸如此类的问题 Todo def getTMod(t, mod): if (mod//t)*t == mod: return false product = t%mod result = 1 while (product != 1 and result &lt; mod): product += t result += 1:]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有趣的生物灵魂]]></title>
    <url>%2F2018%2F06%2F23%2F%E6%9C%89%E8%B6%A3%E7%9A%84%E7%94%9F%E7%89%A9%E7%81%B5%E9%AD%82%2F</url>
    <content type="text"><![CDATA[变性之差一段DNA30 年前就有一支生物团队发现一段Sry 基因，可以使雌性小白鼠发育成雄性。 最近有新的进展发现Sry 基因可以控制合成SOX9的蛋白，动物会在这段蛋白的影响下发育出雄性的生殖器官。 反之如果雄性缺少这段蛋白也能变成雌性。 Reference:(Evidence to exclude SOX9 as a candidate gene for XY sex reversal without skeletal malformation)[http://jmg.bmj.com/content/jmedgenet/33/9/800.full.pdf] 非双螺旋的新DNA结构新的结构命名为“i-motif”, 类似于结。研究人员发现i-motif-随着时间的推移而出现和消失，所以它们正在形成，溶解并再次形成。 I-基序（i-motif） 可能帮助开启或关闭基因，并影响基因是否被主动读取。 另外一种天然的结构 G四联体（G quadruplex，又称G-tetrads或G4-DNA） 其中研究较为深入的区域，是染色体末端的端粒（telomere）。 (DNA可不只有双螺旋结构)[https://zhuanlan.zhihu.com/p/36216255]]]></content>
      <tags>
        <tag>Biology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n-note-4]]></title>
    <url>%2F2018%2F06%2F18%2Fcs231n-note-4%2F</url>
    <content type="text"><![CDATA[反向传播Introduction:反向传播是利用链式法则递归计算表达式的梯度的方法。 根据函数 f(x), 求关于f 关于 x的梯度 gradient 偏导数（我的理解就是当对其中一个param 求导的时候视其他的params为const， 含义为当这个param 增加的时候 对f 的增加量的比值） 链式法则Chain rule 这下面这个sample给的真的是清楚 x = -2; y = 5; z = -4 f(x,y,z) = (x+y)z 绿色的从输入端到输出端 为前向传播 红色的从输出端到输入端 为反向传播 反向传播的理解反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西： 1.这个门的输出值， 2.其输出值关于输入值的局部梯度。 门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。 因为在加法门中 是 y = a+b 所以slope 也就是梯度都是1， 在乘法门中 y = ab, 所以a 的梯度就是b， b的梯度就是a Examplew = [2,-3,-3] # 假设一些随机数据和权重x = [-1, -2] # 前向传播 dot = w[0]*x[0] + w[1]*x[1] + w[2] f = 1.0 / (1 + math.exp(-dot)) # sigmoid函数 # 对神经元反向传播 ddot = (1 - f) * f # 点积变量的梯度, 使用sigmoid函数求导 dx = [w[0] * ddot, w[1] * ddot] # 回传到x dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # 回传到w # 完成！得到输入的梯度 Tip: 对前向传播变量进行缓存：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。 在不同分支的梯度要相加：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。 比如 f（x,y） = sigmal(x) + x balabala 回传流中的模式主要就是 加法门单元: 把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门把梯度2.00不变且相等地路由给了两个输入。 y = 1.0 x a + 1.0 x b 取最大值门单元: 对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了z变量，因为z的值比w高，于是w的梯度保持为0。 y = max(high, low) = 1.0 x high + 0 x low 乘法门单元: 相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.00x2.00=-8.00。 y = ab, a的梯度是b 用向量化操作计算梯度其实就是矩阵相乘之类，作者建议通过维度来推测计算是否正确 Summary：对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。(梯度如何计算) 讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。（不难理解，但是不知道实际操作是否需要，我感觉每一层都会有自己计算梯度的方法） Reference:反向传播]]></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequelize cli NodeJs Learning Notes]]></title>
    <url>%2F2018%2F06%2F04%2Fsequelize-cli-NodeJs-Learning-Notes%2F</url>
    <content type="text"><![CDATA[Sequlize cli 是一个后端对象控制的一个library Introduce首先要install： npm install –save sequelize-cli 然后init： node_modules/.bin/sequelize init 如果你是用Mac 你可以重命名 sequelize: alias sequelize=’node_modules/.bin/sequelize’ 设置 config/config.json 基本上就是列出 database的schema username password 之类的 Migration一种是create migration by model node_modules/.bin/sequelize model:generate --name User 还有一种是直接create migration node_modules/.bin/sequelize migration:create --name add-wechatid-in-user Seed在数据库里生成一些demo 或者 测试dat啊 生成 demo-user 的 seed node_modules/.bin/sequelize seed:generate --name demo-user 在seeder 文件里 &apos;use strict&apos;; module.exports = { up: (queryInterface, Sequelize) =&gt; { return queryInterface.bulkInsert(&apos;Users&apos;, [{ firstName: &apos;John&apos;, lastName: &apos;Doe&apos;, email: &apos;demo@demo.com&apos; }], {}); }, down: (queryInterface, Sequelize) =&gt; { return queryInterface.bulkDelete(&apos;Users&apos;, null, {}); } }; Reference： Sequelize 手册]]></content>
      <tags>
        <tag>NodeJs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n-note-3]]></title>
    <url>%2F2018%2F06%2F04%2Fcs231n-note-3%2F</url>
    <content type="text"><![CDATA[Review 基于参数的评分函数。该函数将原始图像像素映射为分类评分值。 损失函数。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。 Softmax 和 SVM 不应该包括了score function 和 lost function 最优化Optimization。最优化是寻找能使得损失函数值最小化的参数W的过程。 损失函数可视化 在一维尺度上 $$L(W+aW_1)$$ x轴是a， y轴是loss function 在二维尺度上 $$L(W+aW_1+bW_2)$$, a,b 表达x轴, y轴， loss function 用颜色表示 单独的损失函数表示： $$w_j$$ 如果对应分类正确即是负号，错误即是正号 $$L = \sumL/n$$ SVM的损失函数是一种凸函数，可以学习一下如何高效最小化凸函数，在这种损失函数会有一些不可导的点（kinks） 两个新概念： 梯度？ 次梯度？未来学习点 最优化 Optimization对于神经网络的最优化策略有： 策略#1：随即搜索 最差劲的搜索方案（base line）# 假设X_train的每一列都是一个数据样本（比如3073 x 50000） # 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组） # 假设函数L对损失函数进行评价 bestloss = float(&quot;inf&quot;) # Python assigns the highest possible float value for num in xrange(1000): W = np.random.randn(10, 3073) * 0.0001 # generate random parameters loss = L(X_train, Y_train, W) # get the loss over the entire training set if loss &lt; bestloss: # keep track of the best solution bestloss = loss bestW = W print &apos;in attempt %d the loss was %f, best %f&apos; % (num, loss, bestloss) 感觉跟那个monkey sort 差不多 随机生成W weight。 # 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1] scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples # 找到在每列中评分值最大的索引（即预测的分类） Yte_predict = np.argmax(scores, axis = 0) # 以及计算准确率 np.mean(Yte_predict == Yte) # 返回 0.1555 策略是：随机权重开始，然后迭代取优，从而获得更低的损失值。 策略#2：随机本地搜索生成一个随机的扰动 $$ \delta W $$ $$ Wtry = W + \delta W $$ 当 Wtry 的loss 变小的时候， 才决定移动 W = np.random.randn(10, 3073) * 0.001 # 生成随机初始W bestloss = float(&quot;inf&quot;) for i in xrange(1000): step_size = 0.0001 Wtry = W + np.random.randn(10, 3073) * step_size loss = L(Xtr_cols, Ytr, Wtry) if loss &lt; bestloss: W = Wtry bestloss = loss print &apos;iter %d loss is %f&apos; % (i, bestloss) 策略#3：跟随梯度策略1 和 策略2 都是尝试好几个方向来找减少loss的方向，其实可以用梯度（gradient）来找到最陡峭的方向减少loss， 一维求导公式： d(fx)/dx 当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。 梯度计算有两种方法计算梯度： 数值梯度法 （实现简单 但是缓慢）def eval_numerical_gradient(f, x): &quot;&quot;&quot; 一个f在x处的数值梯度法的简单实现 - f是只有一个参数的函数 - x是计算梯度的点 &quot;&quot;&quot; fx = f(x) # 在原点计算函数值 grad = np.zeros(x.shape) h = 0.00001 # 对x中所有的item索引进行迭代 it = np.nditer(x, flags=[&apos;multi_index&apos;], op_flags=[&apos;readwrite&apos;]) while not it.finished: # 计算x+h处的函数值 ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # 增加h fxh = f(x) # 计算f(x + h) x[ix] = old_value # 存到前一个值中 (非常重要) # 计算偏导数 grad[ix] = (fxh - fx) / h # 坡度 it.iternext() # 到下个维度 return grad 实际中用中心差值公式（centered difference formula）[f(x+h)-f(x-h)]/2h 效果较好 Numerical_differentiation # 要使用上面的代码我们需要一个只有一个参数的函数 # (在这里参数就是权重)所以也包含了X_train和Y_train def CIFAR10_loss_fun(W): return L(X_train, Y_train, W) W = np.random.rand(10, 3073) * 0.001 # 随机权重向量 df = eval_numerical_gradient(CIFAR10_loss_fun, W) # 得到梯度 loss_original = CIFAR10_loss_fun(W) # 初始损失值print ‘original loss: %f’ % (loss_original, ) # 查看不同步长的效果 for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]: step_size = 10 ** step_size_log W_new = W - step_size * df # 权重空间中的新位置 loss_new = CIFAR10_loss_fun(W_new) print &apos;for step size %f new loss: %f&apos; % (step_size, loss_new) # 输出: # original loss: 2.200718 # for step size 1.000000e-10 new loss: 2.200652 # for step size 1.000000e-09 new loss: 2.200057 # for step size 1.000000e-08 new loss: 2.194116 # for step size 1.000000e-07 new loss: 2.135493 # for step size 1.000000e-06 new loss: 1.647802 # for step size 1.000000e-05 new loss: 2.844355 # for step size 1.000000e-04 new loss: 25.558142 # for step size 1.000000e-03 new loss: 254.086573 # for step size 1.000000e-02 new loss: 2539.370888 # for step size 1.000000e-01 new loss: 25392.214036 步长的影响：梯度指明了函数在哪个方向?是变化率最大的 步长(也叫作学习率) 小步长下降稳定但进度慢 &lt;-&gt; 大步长进展快但是风险更大 在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度, 效率太低 分析梯度法 （计算迅速，结果精确） 微分分析计算梯度用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错. 于是我们需要将分析梯度法的结果于数值梯度法作比较， 这个步骤叫做梯度检查。 eg: SVM lossfunction 对W_yi 进行微分 梯度下降普通版本# 普通的梯度下降 while True: weights_grad = evaluate_gradient(loss_fun, data, weights) weights += - step_size * weights_grad # 进行梯度更新 小批量数据梯度下降（Mini-batch gradient descent）每次小子集往下减少，小批量数据的梯度就是对整个数据集梯度的一个近似， 需要data的数量远大于小批数量 # 普通的小批量数据梯度下降 while True: data_batch = sample_training_data(data, 256) # 256个数据 weights_grad = evaluate_gradient(loss_fun, data_batch, weights) weights += - step_size * weights_grad # 参数更新 随机梯度下降（Stochastic Gradient Descent 简称SGD）如果小批量数据中每个批量只有1个数据样本 Summary: x,y 是给定的，weight 从一个随机开始，可以随时改变。 损失函数包含两个部分：数据损失和正则化损失，在梯度下降中，计算权重的维度实现参数的更新。 Reference：最优化上 最优化下]]></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install python library]]></title>
    <url>%2F2018%2F05%2F24%2FInstall-python-library%2F</url>
    <content type="text"><![CDATA[Pyenv因为之前用nvm node 的版本管理器 所以pyenv 应该也是python 的版本管理器 运行 curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 然后打开 vim ~/.bash_profile 添加了三行 export PATH=&quot;~/.pyenv/bin:$PATH&quot; eval &quot;$(pyenv init -)&quot; eval &quot;$(pyenv virtualenv-init -)&quot; 我原来的是 export XAMPP_HOME=/Applications/xampp/xamppfiles export PATH=${XAMPP_HOME}/bin:${PATH} 最后变成 export XAMPP_HOME=/Applications/xampp/xamppfiles export PATH=/Users/weijiesun/.pyenv/bin:${PATH}:${XAMPP_HOME}/bin eval &quot;$(pyenv init -)&quot; eval &quot;$(pyenv virtualenv-init -)&quot; 然后有个坑踩了好久，就是XAMPP_HOME=/Applications/xampp/xamppfiles 要放在path 的最后一位，不然就是疯狂在找http url 的端口 然后运行 source ~/.bash_profile 就好了 Cheating sheet：reference 安装和查看pyenv 和 python #查看pyenv版本 pyenv –version #查看可用的python version pyenv versions #用pyenv 安装python pyenv install 3.6.3 设置 python version 对所有的Shell全局有效，会把版本号写入到~/.pyenv/version文件中 pyenv global 3.6.3 只对当前目录有效，会在当前目录创建.python-version文件 pyenv local 3.6.3 只在当前会话有效 pyenv shell 3.6.3 我在安装的时候遇到了一下描述的问题 Last 10 log lines: File &quot;/private/var/folders/py/3006ng_x6x3g5c42jx_gznlc0000gn/T/python-build.20180618223416.3806/Python-3.6.3/Lib/ensurepip/__main__.py&quot;, line 4, in &lt;module&gt; ensurepip._main() 解决方法 Kears# GPU 版本 &gt;&gt;&gt; pip install --upgrade tensorflow-gpu # CPU 版本 &gt;&gt;&gt; pip install --upgrade tensorflow # Keras 安装 &gt;&gt;&gt; pip install keras -U --pre import keras #然后就显示tensorflow work on backend pyechartsgithub pyecharts 12345678$ pip install pyecharts$ pip install echarts-countries-pypkg$ pip install echarts-china-provinces-pypkg$ pip install echarts-china-cities-pypkg$ pip install echarts-china-counties-pypkg$ pip install echarts-china-misc-pypkg $ pip install echarts-united-kingdom-pypkg]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n note - 2]]></title>
    <url>%2F2018%2F05%2F21%2Fcs231n-note-2%2F</url>
    <content type="text"><![CDATA[Reference: https://zhuanlan.zhihu.com/p/20918580 https://zhuanlan.zhihu.com/p/20945670 https://zhuanlan.zhihu.com/p/21102293 Linear Classificationscore function (评分函数)原始图片到各个类别的评分情况， 分越高越接近 每个图像都是 [D x 1] D = pixel X pixel X 3(RGB) 参数权重 W (weight) = [k X D] k是样本数 在维度空间里 做旋转变换 偏差向量 b (bias vector) = [k x 1] 在维度空间里 做平移变化 图像数据预处理最常见的就是 normalization 零均值的中心化 把 [0-255] -&gt; [-127-127] -&gt; [-1,1] Loss Function (损失函数)量化分类label 与真实label 之间的一致性 也叫 代价函数Cost Function或目标函数Objective 当 Score Function 与真实结果相差越大，Cost Function输出越大 多类支持向量机损失 Multiclass Support Vector Machine Loss eg: 有三个分类 score 是 s = [13, -7, 11] \delta = 10, 第一个label 是真实正确的 因为 -20 大于 \delta 边界值， 所以最后的损失值为8 线性评分函数 的损失函数公式： 折叶损失（hinge loss）: max (0, -)平方折叶损失SVM（即L2-SVM）： max (0, -) ^2 目的是想要正确类别进入红色区域， 如果其他类别进入红色区域甚至更高的时候，计算loss， 我们的目的是找权重W Regularization 正则化假设有一个数据集和一个权重集W能够正确地分类每个数据， 可能有很多相似的W都能正确地分类所有的数据 比如有一个权重W 调整系数可以改变Loss score。 我们希望給W添加一些偏好 向损失函数增加一个正则化惩罚 egularization penalty 正则化惩罚 R(W) 多类SVM损失函数 L = 数据损失（data loss），即所有样例的的平均损失L_i + 正则化损失（regularization loss） 展开公式 Code: def L_i(x, y, W): &quot;&quot;&quot; unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) &quot;&quot;&quot; delta = 1.0 # see notes about delta later in this section scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class correct_class_score = scores[y] D = W.shape[0] # number of classes, e.g. 10 loss_i = 0.0 for j in xrange(D): # iterate over all wrong classes if j == y: # skip for the true class to only loop over incorrect classes continue # accumulate loss for the i-th example loss_i += max(0, scores[j] - correct_class_score + delta) return loss_i def L_i_vectorized(x, y, W): &quot;&quot;&quot; A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) &quot;&quot;&quot; delta = 1.0 scores = W.dot(x) # compute the margins for all classes in one vector operation margins = np.maximum(0, scores - scores[y] + delta) # on y-th position scores[y] - scores[y] canceled and gave delta. We want # to ignore the y-th position and only consider margin on max wrong class margins[y] = 0 loss_i = np.sum(margins) return loss_i def L(X, y, W): &quot;&quot;&quot; fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) &quot;&quot;&quot; # evaluate loss over all examples in X without using any for loops # left as exercise to reader in the assignment delta = 1.0 score_matrix = W.dot(X) correct_score = score_matrix[y] #Todo, correct_score_matrix = correct_core * 50 #Todo, delta_matrx repeat delta loss_matrix = score_matrix - correct_score_matrix + delta_matrx result = np.sum(loss_matrix, axis=1) return loss_matrix Softmax分类器逻辑回归分类器面对多个分类的一般化归纳 公式： 或者 所有的函数转换成 $$e^z$$ score function =&gt; softmax 函数, 每个元素都在0-1之间并且和为1 概率解释: 我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE） 实现softmax函数计算的时候技巧可以用常数C， 通常$$\log C = -maxf_j$$ f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大 p = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸 # 那么将f中的值平移到最大值为0： f -= np.max(f) # f becomes [-666, -333, 0] p = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果 折叶损失（hinge loss）替换为交叉熵损失（cross-entropy loss） SVM和Softmax的比较Softmax Summary SVM 和 Softmax 基于 weight W 和 bias b define Loss Function (损失函数) 用来更好的定义更好的预测模型 —-sad—-Test Mathjax, 但是公式实在太复杂。 $$L{i}=f{yi}+\log (\sum{j} e^{f_j})$$ $L_{i} = -\log \frac {e^{f_y}} {e^{f_j}}$ $\sum_{j=0}$ $$a+b=c$$ TODO]]></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n note - 1]]></title>
    <url>%2F2018%2F05%2F20%2Fcs231n-note-1%2F</url>
    <content type="text"><![CDATA[Reference： https://zhuanlan.zhihu.com/p/20894041 https://zhuanlan.zhihu.com/p/20900216 图像分类颜色channel 有三个 red green blue， RGB 流程就是 输入 -&gt; 学习 -&gt; 评价 K Nearest Neighbor 分类器k 越高 generalization 能力越好 用距离来分类L1范数 和 L2范数 L1 L2 distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) L1和L2比较。。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在p-norm常用的特殊形式。 hyperparameter: 超参数，需要去测试调整validation set: 测试集 只能使用一次Cross validation: train &lt;-&gt; test##k Nearest Neighbor 优缺点: Advantange:易于理解，实现简单。 Disadvantage: 算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率]]></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Notes - 1]]></title>
    <url>%2F2018%2F05%2F20%2FMachine-Learning-Notes-1%2F</url>
    <content type="text"><![CDATA[Reference: https://zhuanlan.zhihu.com/p/36287950 Todo List [X] - 01 - OverviewP1-15 Machine learning background and flow P16-42 Machine learning flow [ ] - 02 - Business Understanding [X] - 03 - Data Understanding p56 - 62 To learn about data analysis, it is right that each of us try many things that do not work – that we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed an opportunity to learn more [ ] - 04 - Data Preparation -p63 Prepare data is time consuming several methods to deal with missing data and outliers normalize Data Data binning 分级 Reduce Data, Clean Data Feature Engineering: 从raw data 提取出 feature Feature Selection: Filter/Wrapper/Embedded methodTraditional approaches 传统方法 Forward selection一开始模型里面没有variable， 然后往里面加入variable，直到accuracy 没有任何的增长 Backward elimination和前一种相反，一开始有所有的variable, 然后去除，直到accuracy 有明显的下降 Stepwise regression这种是用在选k-best feature， 一开始有k个，然后加入更好的，并且去除最差的，直到经历过所有的feature P100]]></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Notes]]></title>
    <url>%2F2017%2F06%2F13%2FDocker-Notes%2F</url>
    <content type="text"><![CDATA[Containers List Containers docker ps (show all the running container) docker ps -a (show all the container include which is not running) Start container docker start {containerId} (start one container) Stop container docker stop {containerId} (Stop one container) Docker-Compose Create a docker-compose.yml document To be continue]]></content>
  </entry>
  <entry>
    <title><![CDATA[Learn-PyMOL-for-Biology-Research-1]]></title>
    <url>%2F2016%2F08%2F06%2FLearn-PyMOL-for-Biology-Research-1%2F</url>
    <content type="text"><![CDATA[Updated in Aug/22/2016Here I will teach How to set the environment in Mac (install the Xcode and Xcode-command-tool.)[http://railsapps.github.io/xcode-command-line-tools.html] (install the MacPort)[https://www.macports.org/install.php] update MacPort by this command 1sudo port -v selfupdate Tried to install PyMOL by MacPort 1sudo port install pymol And following the tips in the warning to install the python 1find ~/ -type f -name &quot;_cmd.so&quot; eg: return 1/opt//local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pymol/_cmd.so Try to add the path of folder of pymol in the PYTHONPATH eg: /opt//local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ 12PYTHONPATH=&quot;/opt//local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages&quot;export PYTHONPATH Abstract:This Series of blog pages is about the Script of PyMOL. If you are an expert of PyMOL/Python and give me some suggestions or find some errors in this page, I will be very happpppy. This is my First time to use PyMOL Since the Dr.Marcelo Marcet-Palacios only provide a exist file of PyMOL software without a PyMOL API for Python. I am starting to setup the environment of Open-Source PyMOL, which take a long time(take care). Setup Environment.Dr.Marcelo Marcet-Palacios helped me to install the PyMOL by a Mac-package. However, I found the python script cannot load the PyMOL module. I started to find the reason. https://www.biostars.org/p/113328/ helps a lot. If you want to1import pymol in your script, you wish you can install the pymol following the steps of installing an Open-Source PyMOL. So I started to follow the http://www.pymolwiki.org/index.php/MAC_Install to install an Open-Source PyMOL. If your mac is over 10.5, my feedback is not use the fink, which is not easy to install. Try this1234sudo port install tcl -corefoundationsudo port install tk -quartzsudo port install pymol The other issue make me so annoying is that12345678Traceback (most recent call last): File &quot;/usr/local/Cellar/pymol/1.8.2.1/libexec/lib/python2.7/site-packages/pymol/__init__.py&quot;, line 72, in &lt;module&gt; import pymol File &quot;/usr/local/Cellar/pymol/1.8.2.1/libexec/lib/python2.7/site-packages/pymol/__init__.py&quot;, line 536, in &lt;module&gt; import pymol._cmdImportError: dlopen(/usr/local/Cellar/pymol/1.8.2.1/libexec/lib/python2.7/site-packages/pymol/_cmd.so, 2): Library not loaded: /usr/local/opt/glew/lib/libGLEW.2.0.0.dylib Referenced from: /usr/local/Cellar/pymol/1.8.2.1/libexec/lib/python2.7/site-packages/pymol/_cmd.so Reason: image not found I in my1/usr/local/opt/glew/lib It only has these following files:123libGLEW.1.13.0.dylib libGLEW.dylib libGLEWmx.alibGLEW.1.13.dylib libGLEWmx.1.13.0.dylib libGLEWmx.dyliblibGLEW.a libGLEWmx.1.13.dylib pkgconfig The brew cannot upgrade the openGL. I have to update the XOs to the newest version. So I am downloading the package for 6 GB. See you tomorrow. Hopefully, I could setup the environment. Referencehttps://www.biostars.org/p/113328/http://www.pymolwiki.org/index.php/MAC_Install]]></content>
      <tags>
        <tag>Research</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Project[1]-Weijie Sun]]></title>
    <url>%2F2016%2F08%2F02%2FMachine-Learning-Project-1-Weijie-Sun%2F</url>
    <content type="text"><![CDATA[IntroductionI have taken a Machine Learning Course at the University of Alberta. Our professor is Russell Greiner. We need to make a project and do some researches by Machine Learning method with our coach Koosha Golmohammadi. Since this is my first Machine Learning project. Soooo, this is not as good as some projects made by Machine Learning experts. However, we achieve a result is 90% accuracy which is closed to some the U.S agency in Boston. 1 Scraping Data.We scraped some real estate data from real estate data website from Edmonton and marked in Edmonton Map. 2 Matching DataThen we tried to match the real estate data with over 110 features in Open City Edmonton Data It is a tough work there are different names for an address which is easier for the human to understand than a machine. After a little of the algorithm in nature language process(very basic). we got the following useful data. This is our first step for collating data and scraping data. I will keep updating for Machine learning.See you next time. Our code resource]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My Ionic-Hybrid Experience[1]]]></title>
    <url>%2F2016%2F07%2F27%2FMy%20Ionic-Hybrid%20Experience%5B1%5D%2F</url>
    <content type="text"><![CDATA[Ionic Development experienceWhen I was in 3rd year in Undergraduate, I has an opportunity to develop a Mobile Application. Since I have the experience of developing Android Mobile Application which is a CMPUT 301 course project. I build a team which from 2 different teams in 301 course. The First Plan for us is build an Android Application by Java and an iOS Application by Object-C. The Customer asked us to finish in a half year. We don’t think that is enough to develop two Applications and test two. We were looking for some solutions. The first solutions is Cordova Phonegap. However we searched some Phonegap products. The User experience and User Interface is much worse than an Application developed by Java or Object-C. Then one of my team member ask his sister’s boyfriend who works in Google. He recommand the Ionic + Parse for our mobile application. That is the first time I heard about the Ionic. Then I use Ionic + Parse to develop 3 Mobile Applications inculde one course project. It is not easy until we fully understand what the structure in the Ionic. Trust me! Ionic is a very clear Structure. The StructureIn my Understanding the Ionic Stucture is like this form. See you in next time.]]></content>
      <tags>
        <tag>Web, Mobile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Algorithm - Weijie Sun]]></title>
    <url>%2F2016%2F07%2F18%2FAlgorithm%2F</url>
    <content type="text"><![CDATA[integer factorization algorithm1 Introduction Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) trial_division.py note0 Euler’s factorization method euler_trivial.py note2 note1 Fermat’s factorization method fermatfactor_trivial.py fermatfactor_improved_prime.py Pollard’s rho algorithm Pollards_rho_trivial.py Pollards_rho_improved_prime.py note3 note0 : Trivial Division use the trivial prime, which cannot use Miller-Rabin primality test note1 : Euler’s factorization method stops the algorithmwhen a number cannot found number = a^2 + b^2 = c^2 + d^2. Miller-Rabin is only judge the prime not find the factors. note2 : Since I have not find a better way to find a number = a^2 + b^2 = c^2 + d^2. So Euler&apos;s spend on the this function I will list after that note3 : In Pollards_rho Miller-Rabin primality test, inorder to handle some base cases. I use the trial_division to handle some special cases. #Integer factorization: 1.1Trial division: (Baseline)This is a base line in my integer factorization algorithm project. It is usinga prime sieve for prime number generation which can judge a number is a prime orcontinue do factorization. Running time: in the worst case it is O(2^(n/2)/((n/2)*ln2)) n is base-2 n digit number. 1.2Euler’s factorization method:This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Euler’s factorization is ifa number = a^2 + b^2 = c^2 + d^2. There is a quickly way to seperate into 2 factors. Running time: It is very slow, worst case is greater than trial division,only quick in some special cases and has potient quick. 1.3Fermat’s factorization method:This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Fermat’s factorization method: is if a number = a^2 - b^2 There is a quickly way to seperate into 2 factors. Running time: Worst case is O(N^{1/2}) General case is O(N^{1/3}) time. 1.4Pollard’s rho algorithm:This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Pollard’s rho algorithm is if a findthe abs(x^2+1-x) mod N if not 1 then it is a factor of N. Running time: General case by the Birthday paradox in O(\sqrt p)\ &lt;= O(n^{1/4}) but this is a heuristic claim, and rigorous analysis of the algorithm remains open. 2 A description of what sort of tests I have included1 St: 2 primes production (each prime &gt; million)When the prime is very big, test the speed of each methods. ###1.1The testcases: 15485867*32452867 = 502560782130689 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 31.0308229923 null Euler’s factorization method 32.3473279476+3.276534002 null Fermat’s factorization method 2.5645339489 3.19916701317 Pollard’s rho algorithm 0.0414018630981 0.0358607769012 ###1.2The testcases: 15487019*15487469 = 239854726664911 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 22.700715065 null Euler’s factorization method 21.358424902+3.0871624554 null Fermat’s factorization method 3.99884200096 2.206387043 Pollard’s rho algorithm 0.0165410041809 0.0110921859741 ###1.3The testcases: 15490781*67869427 = 1051350430252487 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 53.9460468292 null Euler’s factorization method 47.7404336929+8.194948196 null Fermat’s factorization method 8.05504488945 9.30907988548 Pollard’s rho algorithm 0.0999021530151 0.0918011665344 2 nd: 4 primes production (each prime between[1000,9999])When the prime is big and more factors, test the speed of each methods. ###2.1The testcases: 8147 8369 8623 * 7127 = 4190216175859403 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 109.196480989 null Euler’s factorization method 105.0888099666+5.89233399 null Fermat’s factorization method 2.5645339489 3.19916701317 Pollard’s rho algorithm 0.0414018630981 0.0358607769012 ###2.2The testcases: 1259 1451 1613 * 1811 = 5336370322687 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 2.96865296364 null Euler’s factorization method 3.3992729187+0.5768110752 null Fermat’s factorization method 5.54617881775 3.116948843 Pollard’s rho algorithm 0.00448799133301 0.00167012214661 ###2.3The testcases: 6277 5351 8831 * 9733 = 2886979418455921 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 77.4180650711 null Euler’s factorization method 72.7111520767+13.2122049 null Fermat’s factorization method 2.87196207047 3.24289989471 Pollard’s rho algorithm 0.479516983032 0.00454497337341 3 rd: over 6 small prime productWhen the prime is not that big, but we have more factors for the number which need to be factorization. ###3.1The testcases: 13 127 263 419 17 131 269 = 108990674873561 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 12.9187300205 null Euler’s factorization method 12.9247641563+2.445067882 null Fermat’s factorization method 3.29201412201 2.98240017891 Pollard’s rho algorithm 0.00494313240051 0.00559782981873 ###3.2The testcases: 13 17 2 1123 1426499 * 5 = 3540328013170 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 4.11624193192 null Euler’s factorization method 4.38335967064+0.452334165 null Fermat’s factorization method 3.39200806618 2.18938994408 Pollard’s rho algorithm 0.00471496582031 0.00225687026978 ###3.3The testcases: 547 701 29 149 5 * 2 = 16568744870 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 3.92766094208 null Euler’s factorization method 2.42819094658+0.031641006 null Fermat’s factorization method 3.00680112839 4.95704817772 Pollard’s rho algorithm 0.0637471675873 0.000675916671753 3 How to run the code:There is a easy way to run byrunner.py```123Then following the introduction. Firstly, you will see this introduction format. integer factorization algorithmtype the number you want to run the algorithmeg: 1then will run the Trial division1.Trial division2.Euler’s factorization method3.Fermat’s factorization method4.Pollard’s rho algorithminput the number:123Then input the id of factorization method.for the 3rd and 4th, you also need to choose the id of the prime generate method. runner.py -+—-1 “Trial division”—-+—“type number of testcases” | +—“input the number which needs factorization” | | +—-2 “Euler’s factorization method”—+—“type number of testcases” | +—“input the number which needs factorization” | | +—-3 “Fermat’s factorization method”—+—“type the prime generate method” | +—1 “trivial prime” | + +—“type number of testcases” | | +—“input the number which needs factorization” | | | +—2 “Miller-Rabin primality test” | +—“type number of testcases” | +—“input the number which needs factorization” | | +—-4 “Pollard’s rho algorithm”—+—“type the prime generate method” +—1 “trivial prime” | +—“type number of testcases” | +—“input the number which needs factorization” | +—2 “Miller-Rabin primality test” +—“type number of testcases” +—“input the number which needs factorization”```Or You can runner each single filewhich has been listed: Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) trial_division.py note0 Euler’s factorization method euler_trivial.py note2 note1 Fermat’s factorization method fermatfactor_trivial.py fermatfactor_improved_prime.py Pollard’s rho algorithm Pollards_rho_trivial.py Pollards_rho_improved_prime.py Anything else you deem relevant.1 I used the trivial prime: which judge if a number is a prime. 2 I used another prime test called Miller-Rabin primality test: which is much quicker and I have referenced in all my test cases. Prime test is correct. Referencehttp://www.bigprimes.net/archive/prime/10/ https://rosettacode.org/wiki/Miller–Rabin_primality_test#Python:_Probably_correct_answers https://en.wikipedia.org/wiki/Integer_factorization https://en.wikipedia.org/wiki/Trial_division https://en.wikipedia.org/wiki/Fermat%27s_factorization_method https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm https://en.wikipedia.org/wiki/Euler%27s_factorization_method http://mathworld.wolfram.com/PollardRhoFactorizationMethod.html http://kadinumberprops.blogspot.ca/2012/11/fermats-factorization-running-time.html]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HIV_project - Weijie Sun]]></title>
    <url>%2F2016%2F07%2F18%2FHIV-project%2F</url>
    <content type="text"><![CDATA[HIV virus’s protine order like hexagon.In the previous research, they working on the 2 dimension. However, in 3 dimension, the protine fills spherical surface. This prgramming project is used to help the biology researcher to plot the HIV virus and get a list of all surface protine coordinations by setting the different radius. The main structure.If the point B rotated point A and get point C, I named points A is the father or root of B and C. As I mentioned that HIV virus’s surfaces protines order like a hexagen, protine will rotated 120 degree to get another protine. There are a K_keepers list of points which generated and a Pivot list of points. Use a Queue push in the points generated by the other 2 points, and push out the points which is already generated 2 points. When the distance is less than the protine’s radius, then the protines should not be placed at that coordination. Reference:https://en.wikipedia.org/wiki/Rotation_matrix]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2016%2F07%2F15%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
