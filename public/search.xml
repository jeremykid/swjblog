<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[sequelize cli NodeJs Learning Notes]]></title>
    <url>%2F2018%2F06%2F04%2Fsequelize-cli-NodeJs-Learning-Notes%2F</url>
    <content type="text"><![CDATA[Sequlize cli 是一个后端对象控制的一个library Introduce首先要install： npm install –save sequelize-cli 然后init： node_modules/.bin/sequelize init 如果你是用Mac 你可以重命名 sequelize: alias sequelize=’node_modules/.bin/sequelize’ 设置 config/config.json 基本上就是列出 database的schema username password 之类的 Migration一种是create migration by model node_modules/.bin/sequelize model:generate --name User 还有一种是直接create migration node_modules/.bin/sequelize migration:create --name add-wechatid-in-user Seed在数据库里生成一些demo 或者 测试dat啊 生成 demo-user 的 seed node_modules/.bin/sequelize seed:generate --name demo-user 在seeder 文件里 &apos;use strict&apos;; module.exports = { up: (queryInterface, Sequelize) =&gt; { return queryInterface.bulkInsert(&apos;Users&apos;, [{ firstName: &apos;John&apos;, lastName: &apos;Doe&apos;, email: &apos;demo@demo.com&apos; }], {}); }, down: (queryInterface, Sequelize) =&gt; { return queryInterface.bulkDelete(&apos;Users&apos;, null, {}); } }; Reference： Sequelize 手册]]></content>
      <tags>
        <tag>NodeJs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n-note-3]]></title>
    <url>%2F2018%2F06%2F04%2Fcs231n-note-3%2F</url>
    <content type="text"><![CDATA[最优化上 最优化下 Review 基于参数的评分函数。该函数将原始图像像素映射为分类评分值。 损失函数。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。 Softmax 和 SVM 不应该包括了score function 和 lost function 最优化Optimization。最优化是寻找能使得损失函数值最小化的参数W的过程。 损失函数可视化]]></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install pyenv]]></title>
    <url>%2F2018%2F05%2F24%2FInstall-pyenv%2F</url>
    <content type="text"><![CDATA[因为之前用nvm node 的版本管理器 所以pyenv 应该也是python 的版本管理器 运行 curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 然后打开 vim ~/.bash_profile 添加了三行 export PATH=&quot;~/.pyenv/bin:$PATH&quot; eval &quot;$(pyenv init -)&quot; eval &quot;$(pyenv virtualenv-init -)&quot; 我原来的是 export XAMPP_HOME=/Applications/xampp/xamppfiles export PATH=${XAMPP_HOME}/bin:${PATH} 最后变成 export XAMPP_HOME=/Applications/xampp/xamppfiles export PATH=/Users/weijiesun/.pyenv/bin:${PATH}:${XAMPP_HOME}/bin eval &quot;$(pyenv init -)&quot; eval &quot;$(pyenv virtualenv-init -)&quot; 然后有个坑踩了好久，就是XAMPP_HOME=/Applications/xampp/xamppfiles 要放在path 的最后一位，不然就是疯狂在找http url 的端口 然后运行 source ~/.bash_profile 就好了 Kears]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n note - 2]]></title>
    <url>%2F2018%2F05%2F21%2Fcs231n-note-2%2F</url>
    <content type="text"><![CDATA[Reference: https://zhuanlan.zhihu.com/p/20918580 https://zhuanlan.zhihu.com/p/20945670 https://zhuanlan.zhihu.com/p/21102293 Linear Classificationscore function (评分函数)原始图片到各个类别的评分情况， 分越高越接近 每个图像都是 [D x 1] D = pixel X pixel X 3(RGB) 参数权重 W (weight) = [k X D] k是样本数 在维度空间里 做旋转变换 偏差向量 b (bias vector) = [k x 1] 在维度空间里 做平移变化 图像数据预处理最常见的就是 normalization 零均值的中心化 把 [0-255] -&gt; [-127-127] -&gt; [-1,1] Loss Function (损失函数)量化分类label 与真实label 之间的一致性 也叫 代价函数Cost Function或目标函数Objective 当 Score Function 与真实结果相差越大，Cost Function输出越大 多类支持向量机损失 Multiclass Support Vector Machine Loss eg: 有三个分类 score 是 s = [13, -7, 11] \delta = 10, 第一个label 是真实正确的 因为 -20 大于 \delta 边界值， 所以最后的损失值为8 线性评分函数 的损失函数公式： 折叶损失（hinge loss）: max (0, -)平方折叶损失SVM（即L2-SVM）： max (0, -) ^2 目的是想要正确类别进入红色区域， 如果其他类别进入红色区域甚至更高的时候，计算loss， 我们的目的是找权重W Regularization 正则化假设有一个数据集和一个权重集W能够正确地分类每个数据， 可能有很多相似的W都能正确地分类所有的数据 比如有一个权重W 调整系数可以改变Loss score。 我们希望給W添加一些偏好 向损失函数增加一个正则化惩罚 egularization penalty 正则化惩罚 R(W) 多类SVM损失函数 L = 数据损失（data loss），即所有样例的的平均损失L_i + 正则化损失（regularization loss） 展开公式 Code: def L_i(x, y, W): &quot;&quot;&quot; unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) &quot;&quot;&quot; delta = 1.0 # see notes about delta later in this section scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class correct_class_score = scores[y] D = W.shape[0] # number of classes, e.g. 10 loss_i = 0.0 for j in xrange(D): # iterate over all wrong classes if j == y: # skip for the true class to only loop over incorrect classes continue # accumulate loss for the i-th example loss_i += max(0, scores[j] - correct_class_score + delta) return loss_i def L_i_vectorized(x, y, W): &quot;&quot;&quot; A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) &quot;&quot;&quot; delta = 1.0 scores = W.dot(x) # compute the margins for all classes in one vector operation margins = np.maximum(0, scores - scores[y] + delta) # on y-th position scores[y] - scores[y] canceled and gave delta. We want # to ignore the y-th position and only consider margin on max wrong class margins[y] = 0 loss_i = np.sum(margins) return loss_i def L(X, y, W): &quot;&quot;&quot; fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) &quot;&quot;&quot; # evaluate loss over all examples in X without using any for loops # left as exercise to reader in the assignment delta = 1.0 score_matrix = W.dot(X) correct_score = score_matrix[y] #Todo, correct_score_matrix = correct_core * 50 #Todo, delta_matrx repeat delta loss_matrix = score_matrix - correct_score_matrix + delta_matrx result = np.sum(loss_matrix, axis=1) return loss_matrix Softmax分类器逻辑回归分类器面对多个分类的一般化归纳 公式： 或者 所有的函数转换成 $$e^z$$ score function =&gt; softmax 函数, 每个元素都在0-1之间并且和为1 概率解释: 我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE） 实现softmax函数计算的时候技巧可以用常数C， 通常$$\log C = -maxf_j$$ f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大 p = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸 # 那么将f中的值平移到最大值为0： f -= np.max(f) # f becomes [-666, -333, 0] p = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果 折叶损失（hinge loss）替换为交叉熵损失（cross-entropy loss） SVM和Softmax的比较Softmax Summary SVM 和 Softmax 基于 weight W 和 bias b define Loss Function (损失函数) 用来更好的定义更好的预测模型 —-sad—-Test Mathjax, 但是公式实在太复杂。 $$L{i}=f{yi}+\log (\sum{j} e^{f_j})$$ $L_{i} = -\log \frac {e^{f_y}} {e^{f_j}}$ $\sum_{j=0}$ $$a+b=c$$ TODO]]></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n note - 1]]></title>
    <url>%2F2018%2F05%2F20%2Fcs231n-note-1%2F</url>
    <content type="text"><![CDATA[Reference： https://zhuanlan.zhihu.com/p/20894041 https://zhuanlan.zhihu.com/p/20900216 图像分类颜色channel 有三个 red green blue， RGB 流程就是 输入 -&gt; 学习 -&gt; 评价 K Nearest Neighbor 分类器k 越高 generalization 能力越好 用距离来分类L1范数 和 L2范数 L1 L2 distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) L1和L2比较。。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在p-norm常用的特殊形式。 hyperparameter: 超参数，需要去测试调整validation set: 测试集 只能使用一次Cross validation: train &lt;-&gt; test##k Nearest Neighbor 优缺点: Advantange:易于理解，实现简单。 Disadvantage: 算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率]]></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Notes - 1]]></title>
    <url>%2F2018%2F05%2F20%2FMachine-Learning-Notes-1%2F</url>
    <content type="text"><![CDATA[Reference: https://zhuanlan.zhihu.com/p/36287950 Todo List [X] - 01 - OverviewP1-15 Machine learning background and flow P16-42 Machine learning flow [ ] - 02 - Business Understanding [X] - 03 - Data Understanding p56 - 62 To learn about data analysis, it is right that each of us try many things that do not work – that we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed an opportunity to learn more [ ] - 04 - Data Preparation -p63 Prepare data is time consuming several methods to deal with missing data and outliers normalize Data Data binning 分级 Reduce Data, Clean Data Feature Engineering: 从raw data 提取出 feature Feature Selection: Filter/Wrapper/Embedded methodTraditional approaches 传统方法 Forward selection一开始模型里面没有variable， 然后往里面加入variable，直到accuracy 没有任何的增长 Backward elimination和前一种相反，一开始有所有的variable, 然后去除，直到accuracy 有明显的下降 Stepwise regression这种是用在选k-best feature， 一开始有k个，然后加入更好的，并且去除最差的，直到经历过所有的feature P100]]></content>
      <tags>
        <tag>Machine_Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Notes]]></title>
    <url>%2F2017%2F06%2F13%2FDocker-Notes%2F</url>
    <content type="text"><![CDATA[Containers List Containers docker ps (show all the running container) docker ps -a (show all the container include which is not running) Start container docker start {containerId} (start one container) Stop container docker stop {containerId} (Stop one container) Docker-Compose Create a docker-compose.yml document To be continue]]></content>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Project[1]-Weijie Sun]]></title>
    <url>%2F2016%2F08%2F02%2FMachine-Learning-Project-1-Weijie-Sun%2F</url>
    <content type="text"><![CDATA[IntroductionI have taken a Machine Learning Course at the University of Alberta. Our professor is Russell Greiner. We need to make a project and do some researches by Machine Learning method with our coach Koosha Golmohammadi. Since this is my first Machine Learning project. Soooo, this is not as good as some projects made by Machine Learning experts. However, we achieve a result is 90% accuracy which is closed to some the U.S agency in Boston. 1 Scraping Data.We scraped some real estate data from real estate data website from Edmonton and marked in Edmonton Map. 2 Matching DataThen we tried to match the real estate data with over 110 features in Open City Edmonton Data It is a tough work there are different names for an address which is easier for the human to understand than a machine. After a little of the algorithm in nature language process(very basic). we got the following useful data. This is our first step for collating data and scraping data. I will keep updating for Machine learning.See you next time. Our code resource]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My Ionic-Hybrid Experience[1]]]></title>
    <url>%2F2016%2F07%2F27%2FMy%20Ionic-Hybrid%20Experience%5B1%5D%2F</url>
    <content type="text"><![CDATA[Ionic Development experienceWhen I was in 3rd year in Undergraduate, I has an opportunity to develop a Mobile Application. Since I have the experience of developing Android Mobile Application which is a CMPUT 301 course project. I build a team which from 2 different teams in 301 course. The First Plan for us is build an Android Application by Java and an iOS Application by Object-C. The Customer asked us to finish in a half year. We don’t think that is enough to develop two Applications and test two. We were looking for some solutions. The first solutions is Cordova Phonegap. However we searched some Phonegap products. The User experience and User Interface is much worse than an Application developed by Java or Object-C. Then one of my team member ask his sister’s boyfriend who works in Google. He recommand the Ionic + Parse for our mobile application. That is the first time I heard about the Ionic. Then I use Ionic + Parse to develop 3 Mobile Applications inculde one course project. It is not easy until we fully understand what the structure in the Ionic. Trust me! Ionic is a very clear Structure. The StructureIn my Understanding the Ionic Stucture is like this form. See you in next time.]]></content>
      <tags>
        <tag>Web, Mobile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Algorithm - Weijie Sun]]></title>
    <url>%2F2016%2F07%2F18%2FAlgorithm%2F</url>
    <content type="text"><![CDATA[integer factorization algorithm1 Introduction Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) trial_division.py note0 Euler’s factorization method euler_trivial.py note2 note1 Fermat’s factorization method fermatfactor_trivial.py fermatfactor_improved_prime.py Pollard’s rho algorithm Pollards_rho_trivial.py Pollards_rho_improved_prime.py note3 note0 : Trivial Division use the trivial prime, which cannot use Miller-Rabin primality test note1 : Euler’s factorization method stops the algorithmwhen a number cannot found number = a^2 + b^2 = c^2 + d^2. Miller-Rabin is only judge the prime not find the factors. note2 : Since I have not find a better way to find a number = a^2 + b^2 = c^2 + d^2. So Euler&apos;s spend on the this function I will list after that note3 : In Pollards_rho Miller-Rabin primality test, inorder to handle some base cases. I use the trial_division to handle some special cases. #Integer factorization: 1.1Trial division: (Baseline)This is a base line in my integer factorization algorithm project. It is usinga prime sieve for prime number generation which can judge a number is a prime orcontinue do factorization. Running time: in the worst case it is O(2^(n/2)/((n/2)*ln2)) n is base-2 n digit number. 1.2Euler’s factorization method:This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Euler’s factorization is ifa number = a^2 + b^2 = c^2 + d^2. There is a quickly way to seperate into 2 factors. Running time: It is very slow, worst case is greater than trial division,only quick in some special cases and has potient quick. 1.3Fermat’s factorization method:This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Fermat’s factorization method: is if a number = a^2 - b^2 There is a quickly way to seperate into 2 factors. Running time: Worst case is O(N^{1/2}) General case is O(N^{1/3}) time. 1.4Pollard’s rho algorithm:This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Pollard’s rho algorithm is if a findthe abs(x^2+1-x) mod N if not 1 then it is a factor of N. Running time: General case by the Birthday paradox in O(\sqrt p)\ &lt;= O(n^{1/4}) but this is a heuristic claim, and rigorous analysis of the algorithm remains open. 2 A description of what sort of tests I have included1 St: 2 primes production (each prime &gt; million)When the prime is very big, test the speed of each methods. ###1.1The testcases: 15485867*32452867 = 502560782130689 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 31.0308229923 null Euler’s factorization method 32.3473279476+3.276534002 null Fermat’s factorization method 2.5645339489 3.19916701317 Pollard’s rho algorithm 0.0414018630981 0.0358607769012 ###1.2The testcases: 15487019*15487469 = 239854726664911 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 22.700715065 null Euler’s factorization method 21.358424902+3.0871624554 null Fermat’s factorization method 3.99884200096 2.206387043 Pollard’s rho algorithm 0.0165410041809 0.0110921859741 ###1.3The testcases: 15490781*67869427 = 1051350430252487 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 53.9460468292 null Euler’s factorization method 47.7404336929+8.194948196 null Fermat’s factorization method 8.05504488945 9.30907988548 Pollard’s rho algorithm 0.0999021530151 0.0918011665344 2 nd: 4 primes production (each prime between[1000,9999])When the prime is big and more factors, test the speed of each methods. ###2.1The testcases: 8147 8369 8623 * 7127 = 4190216175859403 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 109.196480989 null Euler’s factorization method 105.0888099666+5.89233399 null Fermat’s factorization method 2.5645339489 3.19916701317 Pollard’s rho algorithm 0.0414018630981 0.0358607769012 ###2.2The testcases: 1259 1451 1613 * 1811 = 5336370322687 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 2.96865296364 null Euler’s factorization method 3.3992729187+0.5768110752 null Fermat’s factorization method 5.54617881775 3.116948843 Pollard’s rho algorithm 0.00448799133301 0.00167012214661 ###2.3The testcases: 6277 5351 8831 * 9733 = 2886979418455921 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 77.4180650711 null Euler’s factorization method 72.7111520767+13.2122049 null Fermat’s factorization method 2.87196207047 3.24289989471 Pollard’s rho algorithm 0.479516983032 0.00454497337341 3 rd: over 6 small prime productWhen the prime is not that big, but we have more factors for the number which need to be factorization. ###3.1The testcases: 13 127 263 419 17 131 269 = 108990674873561 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 12.9187300205 null Euler’s factorization method 12.9247641563+2.445067882 null Fermat’s factorization method 3.29201412201 2.98240017891 Pollard’s rho algorithm 0.00494313240051 0.00559782981873 ###3.2The testcases: 13 17 2 1123 1426499 * 5 = 3540328013170 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 4.11624193192 null Euler’s factorization method 4.38335967064+0.452334165 null Fermat’s factorization method 3.39200806618 2.18938994408 Pollard’s rho algorithm 0.00471496582031 0.00225687026978 ###3.3The testcases: 547 701 29 149 5 * 2 = 16568744870 Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) 3.92766094208 null Euler’s factorization method 2.42819094658+0.031641006 null Fermat’s factorization method 3.00680112839 4.95704817772 Pollard’s rho algorithm 0.0637471675873 0.000675916671753 3 How to run the code:There is a easy way to run byrunner.py```123Then following the introduction. Firstly, you will see this introduction format. integer factorization algorithmtype the number you want to run the algorithmeg: 1then will run the Trial division1.Trial division2.Euler’s factorization method3.Fermat’s factorization method4.Pollard’s rho algorithminput the number:123Then input the id of factorization method.for the 3rd and 4th, you also need to choose the id of the prime generate method. runner.py -+—-1 “Trial division”—-+—“type number of testcases” | +—“input the number which needs factorization” | | +—-2 “Euler’s factorization method”—+—“type number of testcases” | +—“input the number which needs factorization” | | +—-3 “Fermat’s factorization method”—+—“type the prime generate method” | +—1 “trivial prime” | + +—“type number of testcases” | | +—“input the number which needs factorization” | | | +—2 “Miller-Rabin primality test” | +—“type number of testcases” | +—“input the number which needs factorization” | | +—-4 “Pollard’s rho algorithm”—+—“type the prime generate method” +—1 “trivial prime” | +—“type number of testcases” | +—“input the number which needs factorization” | +—2 “Miller-Rabin primality test” +—“type number of testcases” +—“input the number which needs factorization”```Or You can runner each single filewhich has been listed: Algorithm trivial prime Miller-Rabin primality test Trial division(baseline) trial_division.py note0 Euler’s factorization method euler_trivial.py note2 note1 Fermat’s factorization method fermatfactor_trivial.py fermatfactor_improved_prime.py Pollard’s rho algorithm Pollards_rho_trivial.py Pollards_rho_improved_prime.py Anything else you deem relevant.1 I used the trivial prime: which judge if a number is a prime. 2 I used another prime test called Miller-Rabin primality test: which is much quicker and I have referenced in all my test cases. Prime test is correct. Referencehttp://www.bigprimes.net/archive/prime/10/ https://rosettacode.org/wiki/Miller–Rabin_primality_test#Python:_Probably_correct_answers https://en.wikipedia.org/wiki/Integer_factorization https://en.wikipedia.org/wiki/Trial_division https://en.wikipedia.org/wiki/Fermat%27s_factorization_method https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm https://en.wikipedia.org/wiki/Euler%27s_factorization_method http://mathworld.wolfram.com/PollardRhoFactorizationMethod.html http://kadinumberprops.blogspot.ca/2012/11/fermats-factorization-running-time.html]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HIV_project - Weijie Sun]]></title>
    <url>%2F2016%2F07%2F18%2FHIV-project%2F</url>
    <content type="text"><![CDATA[HIV virus’s protine order like hexagon.In the previous research, they working on the 2 dimension. However, in 3 dimension, the protine fills spherical surface. This prgramming project is used to help the biology researcher to plot the HIV virus and get a list of all surface protine coordinations by setting the different radius. The main structure.If the point B rotated point A and get point C, I named points A is the father or root of B and C. As I mentioned that HIV virus’s surfaces protines order like a hexagen, protine will rotated 120 degree to get another protine. There are a K_keepers list of points which generated and a Pivot list of points. Use a Queue push in the points generated by the other 2 points, and push out the points which is already generated 2 points. When the distance is less than the protine’s radius, then the protines should not be placed at that coordination. Reference:https://en.wikipedia.org/wiki/Rotation_matrix]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2016%2F07%2F15%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
