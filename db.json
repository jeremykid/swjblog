{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/avatar.jpeg","path":"images/avatar.jpeg","modified":0,"renderable":0},{"_id":"source/images/ionic-Structure.png","path":"images/ionic-Structure.png","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/LICENSE","path":"lib/canvas-nest/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/README.md","path":"lib/canvas-nest/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"source/images/MachineLearning-data1.png","path":"images/MachineLearning-data1.png","modified":0,"renderable":0},{"_id":"source/images/MachineLearning-data2.png","path":"images/MachineLearning-data2.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"c535694871d259969769326dc962014a195385fc","modified":1527224917609},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1527210540500},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1527210540500},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1527210540500},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1527210540500},{"_id":"themes/next/.gitignore","hash":"a18c2e83bb20991b899b58e6aeadcb87dd8aa16e","modified":1527210540502},{"_id":"themes/next/.stickler.yml","hash":"b7939095038cbdc4883fc10950e163a60a643b43","modified":1527210540502},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1527210540503},{"_id":"themes/next/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1527210540503},{"_id":"themes/next/LICENSE.md","hash":"fc7227c508af3351120181cbf2f9b99dc41f063e","modified":1527210540503},{"_id":"themes/next/README.md","hash":"807c28ad6473b221101251d244aa08e2a61b0d60","modified":1527210540503},{"_id":"themes/next/bower.json","hash":"a8c832da6aad5245052aed7ff26c246f85d68c6c","modified":1527210540505},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1527210540505},{"_id":"themes/next/gulpfile.coffee","hash":"48d2f9fa88a4210308fc41cc7d3f6d53989f71b7","modified":1527210540526},{"_id":"themes/next/package.json","hash":"11a0b27f92da8abf1efbea6e7a0af4271d7bff9e","modified":1527210540599},{"_id":"themes/next/_config.yml","hash":"e364843d315724ce9685d128ea28ebecf569e7ac","modified":1532070232027},{"_id":"source/_drafts/Basic-SEO-1-Weijie-Sun.md","hash":"0009fbf068ca75d8eb94c881645d153facce343f","modified":1469138137000},{"_id":"source/_posts/Docker-Notes.md","hash":"85181dc0297f4bfcd5e8f5b749880a85e2a86810","modified":1497417123000},{"_id":"source/_posts/HIV-project.md","hash":"e9d4b07fc76ed9a2d5071d8962e307499d5a78c3","modified":1468971469000},{"_id":"source/_posts/Algorithm.md","hash":"b6b96d958303ecba8b6ae941cc10a400192b349f","modified":1469085153000},{"_id":"source/_posts/Machine-Learning-Notes-1.md","hash":"f5565d34c4c6013562644bfde7479b36fc46583e","modified":1527145642969},{"_id":"source/_posts/My Ionic-Hybrid Experience[1].md","hash":"fca638134efaba5beae33404eac069ca0d7d4b9b","modified":1469732677000},{"_id":"source/_posts/Machine-Learning-Project-1-Weijie-Sun.md","hash":"2641c1e5d17cc16614599035279ee20afdb179dc","modified":1470176085000},{"_id":"source/_posts/Install-python-library.md","hash":"017980a827a71c0aa6eb766426b1a5df5c118c82","modified":1532903654302},{"_id":"source/_posts/cs231n-note-1.md","hash":"4dba85f0e3fa6fc7faf2ec4016a5e21725b27d88","modified":1527474325950},{"_id":"source/_posts/cs231n-note-2.md","hash":"5697efb2d5ab9c744df98aec338b92ee89c66721","modified":1528094619190},{"_id":"source/_posts/cs231n-note-3.md","hash":"5e217aa098b1f11c044af269bc6fe92b3abeabb9","modified":1529382113413},{"_id":"source/_posts/cs231n-note-5.md","hash":"40e98c4b025da5af785266f5975d581234e13d58","modified":1532897528248},{"_id":"source/_posts/cs231n-note-4.md","hash":"823208d3bdae8f39225efa63f16b1bc94464315e","modified":1531283619544},{"_id":"source/_posts/cs231n-note-6.md","hash":"78e1629da6a74c7ae7cd451ba57c7ccda13c7e88","modified":1532901795859},{"_id":"source/_posts/hello-world.md","hash":"8a02477044e2b77f1b262da2c48c01429e4a32e4","modified":1468622632000},{"_id":"source/_posts/数学手册.md","hash":"ca680f2681049b87dc0eb4647e16b34ae0d7571a","modified":1533702323091},{"_id":"source/_posts/中国剩余定理.md","hash":"463e74ef62ef2c64daa0595d4c92d7b8c0e2cfbc","modified":1532902015565},{"_id":"source/_posts/sequelize-cli-NodeJs-Learning-Notes.md","hash":"ec8d7e4b40da69f8496957b24d24d0095f376892","modified":1528184347250},{"_id":"source/_posts/有趣的生物灵魂.md","hash":"290713231b7aefcb8277ce0c0793d45f138404ff","modified":1531796100244},{"_id":"source/categories/index.md","hash":"d7b76e81b44b77870bef37b9b0ca532f4f2c72b8","modified":1529295963546},{"_id":"source/about/index.md","hash":"048aa4233e3f9d73b9165b3c115493427c02c9fd","modified":1527214444579},{"_id":"source/images/avatar.jpeg","hash":"e9d35b3852716876576dd060d7f83261cfd9d96e","modified":1453612202000},{"_id":"source/tags/index.md","hash":"0e1bc97f8855e0a6ebe12a9acc71db9b9f35486f","modified":1469135011000},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1527210540488},{"_id":"themes/next/.git/config","hash":"e2ca9fa6f115d4406d24bf0df53fc26ce13e0c9b","modified":1527210540493},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1527210539043},{"_id":"themes/next/.git/packed-refs","hash":"03dc16495e8c459df6a75eb9918a4a99e3dd02a7","modified":1527210540484},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"b63696d41f022525e40d7e7870c3785b6bc7536b","modified":1527210540501},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"f846118d7fc68c053df47b24e1f661241645373f","modified":1527210540501},{"_id":"themes/next/.git/index","hash":"cb9aa881719aa4881dde73f03aaeed820fde3ad0","modified":1528186266610},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1527210540501},{"_id":"themes/next/.github/stale.yml","hash":"fd0856f6745db8bd0228079ccb92a662830cc4fb","modified":1527210540502},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"7abbb4c8a29b2c14e576a00f53dbc0b4f5669c13","modified":1527210540502},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1527210540502},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1527210540506},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"141e989844d0b5ae2e09fb162a280715afb39b0d","modified":1527210540507},{"_id":"themes/next/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1527210540508},{"_id":"themes/next/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1527210540509},{"_id":"themes/next/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1527210540508},{"_id":"themes/next/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1527210540510},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"120750c03ec30ccaa470b113bbe39f3d423c67f0","modified":1527210540509},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1527210540515},{"_id":"themes/next/languages/de.yml","hash":"fb478c5040a4e58a4c1ad5fb52a91e5983d65a3a","modified":1527210540527},{"_id":"themes/next/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1527210540528},{"_id":"themes/next/languages/default.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1527210540528},{"_id":"themes/next/languages/fr.yml","hash":"0162a85ae4175e66882a9ead1249fedb89200467","modified":1527210540529},{"_id":"themes/next/docs/MATH.md","hash":"0ae4258950de01a457ea8123a8d13ec6db496e53","modified":1527210540513},{"_id":"themes/next/languages/it.yml","hash":"62ef41d0a9a3816939cb4d93a524e6930ab9c517","modified":1527210540530},{"_id":"themes/next/languages/id.yml","hash":"e7fb582e117a0785036dcdbb853a6551263d6aa6","modified":1527210540529},{"_id":"themes/next/languages/ja.yml","hash":"5f8e54c666393d1ca2e257f6b1e3b4116f6657d8","modified":1527210540531},{"_id":"themes/next/languages/ko.yml","hash":"fae155018ae0efdf68669b2c7dd3f959c2e45cc9","modified":1527210540531},{"_id":"themes/next/languages/nl.yml","hash":"bb9ce8adfa5ee94bc6b5fac6ad24ba4605d180d3","modified":1527210540532},{"_id":"themes/next/languages/pt-BR.yml","hash":"bfc80c8a363fa2e8dde38ea2bc85cd19e15ab653","modified":1527210540536},{"_id":"themes/next/languages/pt.yml","hash":"3cb51937d13ff12fcce747f972ccb664840a9ef3","modified":1527210540536},{"_id":"themes/next/languages/ru.yml","hash":"db0644e738d2306ac38567aa183ca3e859a3980f","modified":1527210540537},{"_id":"themes/next/languages/tr.yml","hash":"c5f0c20743b1dd52ccb256050b1397d023e6bcd9","modified":1527210540537},{"_id":"themes/next/languages/vi.yml","hash":"8da921dd8335dd676efce31bf75fdd4af7ce6448","modified":1527210540538},{"_id":"themes/next/languages/zh-CN.yml","hash":"fbbf3a0b664ae8e927c700b0a813692b94345156","modified":1527210540538},{"_id":"themes/next/languages/zh-HK.yml","hash":"7903b96912c605e630fb695534012501b2fad805","modified":1527210540539},{"_id":"themes/next/languages/zh-TW.yml","hash":"6e6d2cd8f4244cb1b349b94904cb4770935acefd","modified":1527210540539},{"_id":"themes/next/layout/category.swig","hash":"5d955284a42f802a48560b4452c80906a5d1da02","modified":1527210540595},{"_id":"themes/next/layout/archive.swig","hash":"2b6450c6b6d2bcbcd123ad9f59922a5e323d77a5","modified":1527210540594},{"_id":"themes/next/layout/_layout.swig","hash":"09e8a6bfe5aa901c66d314601c872e57f05509e8","modified":1527210540542},{"_id":"themes/next/layout/index.swig","hash":"53300ca42c00cba050bc98b0a3f2d888d71829b1","modified":1527210540596},{"_id":"themes/next/layout/page.swig","hash":"79040bae5ec14291441b33eea341a24a7c0e9f93","modified":1527210540596},{"_id":"themes/next/layout/post.swig","hash":"e7458f896ac33086d9427979f0f963475b43338e","modified":1527210540597},{"_id":"themes/next/scripts/helpers.js","hash":"392cda207757d4c055b53492a98f81386379fc4f","modified":1527210540599},{"_id":"themes/next/layout/tag.swig","hash":"ba402ce8fd55e80b240e019e8d8c48949b194373","modified":1527210540598},{"_id":"themes/next/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1527210540600},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1527210540602},{"_id":"themes/next/layout/schedule.swig","hash":"3e9cba5313bf3b98a38ccb6ef78b56ffa11d66ee","modified":1527210540597},{"_id":"source/images/ionic-Structure.png","hash":"ea92ba8ac9fdad537c6b2f68a62ae8fa598e1d8a","modified":1469732227000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527210540673},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1527210540699},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1527210540699},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1527210540700},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1527210539047},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1527210539044},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1527210539051},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1527210539046},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1527210539052},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1527210539050},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1527210539048},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"18be3eb275c1decd3614e139f5a311b75f1b0ab8","modified":1527210539045},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1527210539049},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1527210539054},{"_id":"themes/next/.git/logs/HEAD","hash":"b1c64aed5fca66e47f1abfe14f757bc87ae3cbbd","modified":1527210540490},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1527210540516},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1527210540516},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1527210540518},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"6855402e2ef59aae307e8bd2a990647d3a605eb8","modified":1527210540519},{"_id":"themes/next/docs/ru/README.md","hash":"712d9a9a557c54dd6638adfb0e1d2bb345b60756","modified":1527210540517},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"a45a791b49954331390d548ac34169d573ea5922","modified":1527210540520},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1527210539042},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"44e4fb7ce2eca20dfa98cdd1700b50d6def4086f","modified":1527210540521},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1527210540523},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"c1ba919f70efe87a39e6217883e1625af0b2c23c","modified":1527210540525},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1527210540522},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"24cf2618d164440b047bb9396263de83bee5b993","modified":1527210540523},{"_id":"themes/next/docs/zh-CN/README.md","hash":"84d349fda6b9973c81a9ad4677db9d9ee1828506","modified":1527210540524},{"_id":"themes/next/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1527210540540},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1527210540540},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1527210540541},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"e03607b608db4aa7d46f6726827c51ac16623339","modified":1527210540524},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"05e67c50a4f3a20fad879ed61b890de8ca6ba4ea","modified":1527210540547},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1527210540547},{"_id":"themes/next/layout/_macro/post-related.swig","hash":"08fe30ce8909b920540231e36c97e28cfbce62b6","modified":1527210540548},{"_id":"themes/next/layout/_macro/reward.swig","hash":"bd5778d509c51f4b1d8da3a2bc35462929f08c75","modified":1527210540549},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"a9e1346b83cf99e06bed59a53fc069279751e52a","modified":1527210540553},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"1f3121ef66a4698fd78f34bf2594ef79a407c92c","modified":1527210540553},{"_id":"themes/next/layout/_macro/post.swig","hash":"686e60ede86547bdd7bc34c3629e4c9dbd134a21","modified":1527210540549},{"_id":"themes/next/layout/_partials/breadcrumb.swig","hash":"6994d891e064f10607bce23f6e2997db7994010e","modified":1527210540554},{"_id":"themes/next/layout/_partials/footer.swig","hash":"1ae77b6a369f83c9986408f2ab448090e37cd2dc","modified":1527210540557},{"_id":"themes/next/layout/_partials/comments.swig","hash":"5df32b286a8265ba82a4ef5e1439ff34751545ad","modified":1527210540555},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1aaf32bed57b976c4c1913fd801be34d4838cc72","modified":1527210540562},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1527210540563},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"0a0129e926c27fffc6e7ef87fe370016bc7a4564","modified":1527210540568},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"6fc63d5da49cb6157b8792f39c7305b55a0d1593","modified":1527210540569},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"ac3ad2c0eccdf16edaa48816d111aaf51200a54b","modified":1527210540569},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"e0bdc723d1dc858b41fd66e44e2786e6519f259f","modified":1527210540571},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"60001c8e08b21bf3a7afaf029839e1455340e95d","modified":1527210540580},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"f532ce257fca6108e84b8f35329c53f272c2ce84","modified":1527210540585},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"a8ab2035654dd06d94faf11a35750529e922d719","modified":1527210540584},{"_id":"themes/next/layout/_third-party/github-banner.swig","hash":"cabd9640dc3027a0b3ac06f5ebce777e50754065","modified":1527210540585},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"6b75c5fd76ae7cf0a7b04024510bd5221607eab3","modified":1527210540589},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"927f19160ae14e7030df306fc7114ba777476282","modified":1527210540589},{"_id":"themes/next/scripts/tags/button.js","hash":"5a61c2da25970a4981fbd65f4a57c5e85db4dcda","modified":1527210540603},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"db70a841e7c1708f95ca97b44413b526b267fa9b","modified":1527210540603},{"_id":"themes/next/scripts/tags/exturl.js","hash":"2b3a4dc15dea33972c0b6d46a1483dabbf06fb5b","modified":1527210540604},{"_id":"themes/next/scripts/tags/full-image.js","hash":"a98fc19a90924f2368e1982f8c449cbc09df8439","modified":1527210540604},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"1b97b1b5364945b8ab3e50813bef84273055234f","modified":1527210540605},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1527210540589},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1527210540590},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"b7600f6b868d8f4f7032126242d9738cd1e6ad71","modified":1527210540605},{"_id":"themes/next/scripts/tags/label.js","hash":"621004f2836040b12c4e8fef77e62cf22c561297","modified":1527210540606},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"460e5e1f305847dcd4bcab9da2038a85f0a1c273","modified":1527210540608},{"_id":"themes/next/scripts/tags/note.js","hash":"4975d4433e11161b2e9a5744b7287c2d667b3c76","modified":1527210540608},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1527210540609},{"_id":"themes/next/source/css/main.styl","hash":"c26ca6e7b5bd910b9046d6722c8e00be672890e0","modified":1527210540672},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"b0ca46e0d1ff4c08cb0a3a8c1994f20d0260cef9","modified":1527210540591},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1527210540673},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1527210540673},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1527210540674},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1527210540674},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1527210540674},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1527210540675},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1527210540675},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1527210540677},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1527210540676},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1527210540676},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1527210540676},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1527210540677},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1527210540677},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1527210540677},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1527210540678},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1527210540678},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1527210540678},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1527210540679},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527210540570},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527210540571},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527210540660},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527210540660},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527210540661},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527210540671},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1527210540672},{"_id":"themes/next/.git/refs/heads/master","hash":"35ef56b48346d32c3e02bdaa3f15f06ef9af7ce6","modified":1527210540490},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1527210540544},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"d1b73c926109145e52605929b75914cc8b60fb89","modified":1527210540546},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1527210540558},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1527210540561},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"fd780171713aada5eb4f4ffed8e714617c8ae6be","modified":1527210540560},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"a7e376b087ae77f2e2a61ba6af81cde5af693174","modified":1527210540558},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"00bf33b3c557b8f7e9faf49b226ea6ff7df5cda0","modified":1527210540559},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"88b4b6051592d26bff59788acb76346ce4e398c2","modified":1527210540562},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"3db735d0cd2d449edf2674310ac1e7c0043cb357","modified":1527210540561},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"a33b29ccbdc2248aedff23b04e0627f435824406","modified":1527210540564},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1527210540565},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1527210540566},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1527210540566},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1527210540566},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1527210540568},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1527210540567},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"cc865af4a3cb6d25a0be171b7fc919ade306bb50","modified":1527210540570},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1527210540568},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1527210540571},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1527210540572},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1527210540570},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1527210540572},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"67f0cb55e6702c492e99a9f697827629da036a0c","modified":1527210540574},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1527210540573},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1527210540575},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1527210540578},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1527210540576},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"beb53371c035b62e1a2c7bb76c63afbb595fe6e5","modified":1527210540577},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"94b26dfbcd1cf2eb87dd9752d58213338926af27","modified":1527210540577},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1527210540579},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"cee047575ae324398025423696b760db64d04e6f","modified":1527210540578},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1527210540579},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1527210540579},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"8878241797f8494a70968756c57cacdfc77b61c7","modified":1527210540581},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1527210540580},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"17a54796f6e03fc834880a58efca45c286e40e40","modified":1527210540582},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"40e3cacbd5fa5f2948d0179eff6dd88053e8648e","modified":1527210540583},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"fe8177e4698df764e470354b6acde8292a3515e0","modified":1527210540582},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"6f340d122a9816ccdf4b45b662880a4b2d087671","modified":1527210540583},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"c0eb6123464d745ac5324ce6deac8ded601f432f","modified":1527210540584},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"a6fc00ec7f5642aabd66aa1cf51c6acc5b10e012","modified":1527210540586},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"42f62695029834d45934705c619035733762309e","modified":1527210540584},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"97dbc2035bcb5aa7eafb80a4202dc827cce34983","modified":1527210540587},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"9b9ff4cc6d5474ab03f09835a2be80e0dba9fe89","modified":1527210540588},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1527210540593},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1527210540594},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1527210540593},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1527210540659},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"b15e10abe85b4270860a56c970b559baa258b2a8","modified":1527210540593},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"2640a54fa63bdd4c547eab7ce2fc1192cf0ccec8","modified":1527210540661},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1527210540660},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"81ca13d6d0beff8b1a4b542a51e3b0fb68f08efd","modified":1527210540661},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"7a2706304465b9e673d5561b715e7c72a238437c","modified":1527210540670},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1527210540671},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"32392d213f5d05bc26b2dc452f2fc6fea9d44f6d","modified":1527210540671},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1527210540680},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1527210540679},{"_id":"themes/next/source/css/_variables/base.styl","hash":"cfb03ec629f13883509eac66e561e9dba562333f","modified":1527210540672},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1527210540680},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1527210540680},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1527210540681},{"_id":"themes/next/source/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1527210540681},{"_id":"themes/next/source/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1527210540681},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1527210540682},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1527210540683},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1527210540685},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1527210540685},{"_id":"themes/next/source/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1527210540683},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1527210540685},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1527210540686},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1527210540686},{"_id":"themes/next/source/lib/canvas-nest/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1532069843852},{"_id":"themes/next/source/lib/canvas-nest/README.md","hash":"bf7819cbb879bb82ec1097513d8f799df8835e0f","modified":1532069843870},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1532069843884},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1527210540698},{"_id":"themes/next/.git/objects/pack/pack-9829b0de980dc403efcab0fab027fd96c783804d.idx","hash":"118683259e5b358ac99d3441dedb260235a098dc","modified":1527210540439},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1527210540698},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1527210540697},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1527210540487},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"b1c64aed5fca66e47f1abfe14f757bc87ae3cbbd","modified":1527210540490},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"6958a97fde63e03983ec2394a4f8e408860fb42b","modified":1527210540592},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1527210540592},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1527210540610},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1527210540610},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1527210540612},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1527210540611},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1527210540611},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1527210540635},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1527210540644},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1527210540656},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"18309b68ff33163a6f76a39437e618bb6ed411f8","modified":1527210540657},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1527210540657},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1527210540658},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1527210540659},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"0810e7c43d6c8adc8434a8fa66eabe0436ab8178","modified":1527210540659},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1527210540658},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1527210540663},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"f362fbc791dafb378807cabbc58abf03e097af6d","modified":1527210540662},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1527210540662},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"f43c821ea272f80703862260b140932fe4aa0e1f","modified":1527210540663},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1527210540662},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1527210540664},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"2212511ae14258d93bec57993c0385e5ffbb382b","modified":1527210540663},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"5e12572b18846250e016a872a738026478ceef37","modified":1527210540664},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1527210540667},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1527210540665},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1527210540666},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"35f093fe4c1861661ac1542d6e8ea5a9bbfeb659","modified":1527210540666},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1527210540668},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"d5e8ea6336bc2e237d501ed0d5bbcbbfe296c832","modified":1527210540667},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"ba1842dbeb97e46c6c4d2ae0e7a2ca6d610ada67","modified":1527210540668},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"05a5abf02e84ba8f639b6f9533418359f0ae4ecb","modified":1527210540669},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1527210540669},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"41f9cdafa00e256561c50ae0b97ab7fcd7c1d6a2","modified":1527210540669},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"5779cc8086b1cfde9bc4f1afdd85223bdc45f0a0","modified":1527210540670},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"ffa870c3fa37a48b01dc6f967e66f5df508d02bf","modified":1527210540669},{"_id":"themes/next/source/lib/canvas-nest/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1532069843808},{"_id":"themes/next/source/lib/canvas-nest/.git/FETCH_HEAD","hash":"1331a42be91f8e6aac6956a4b174c1a0a5e0211d","modified":1532070215187},{"_id":"themes/next/source/lib/canvas-nest/.git/ORIG_HEAD","hash":"5442226ab36d787824e89f50241336839b376133","modified":1532070215219},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1527210540682},{"_id":"themes/next/source/lib/canvas-nest/.git/config","hash":"6aef6bbfcb8ccf9d042bf26defe7d08167b6d81e","modified":1532069843822},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1527210540693},{"_id":"themes/next/source/lib/canvas-nest/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1532069843152},{"_id":"themes/next/source/lib/canvas-nest/.git/index","hash":"9e2deb002f8e559a8c5b340ec319c43572436c9d","modified":1532069843885},{"_id":"themes/next/source/lib/canvas-nest/.git/packed-refs","hash":"60cebf9a6bcd88fd6b2132480cdef3ebf7a7dcf7","modified":1532069843802},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1527210540687},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1527210540687},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1527210540688},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1527210540694},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1527210540694},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1527210540696},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1527210540692},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"b1c64aed5fca66e47f1abfe14f757bc87ae3cbbd","modified":1527210540487},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"39dee82d481dd9d44e33658960ec63e47cd0a715","modified":1527210540612},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"ee37e6c465b9b2a7e39175fccfcbed14f2db039b","modified":1527210540613},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"7cc3f36222494c9a1325c5347d7eb9ae53755a32","modified":1527210540613},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1527210540614},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1527210540615},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1527210540615},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1527210540614},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"17b95828f9db7f131ec0361a8c0e89b0b5c9bff5","modified":1527210540621},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1527210540617},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"7dd9a0378ccff3e4a2003f486b1a34e74c20dac6","modified":1527210540630},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1527210540622},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1527210540625},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"fb451dc4cc0355b57849c27d3eb110c73562f794","modified":1527210540632},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1527210540632},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1527210540634},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1527210540691},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1527210540635},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1527210540635},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1527210540636},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1527210540636},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1527210540637},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"ca89b167d368eac50a4f808fa53ba67e69cbef94","modified":1527210540636},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"417f05ff12a2aaca6ceeac8b7e7eb26e9440c4c3","modified":1527210540637},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1527210540637},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"f4e9f870baa56eae423a123062f00e24cc780be1","modified":1527210540637},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1527210540638},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1527210540639},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1527210540640},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1527210540638},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"c0ac49fadd33ca4a9a0a04d5ff2ac6560d0ecd9e","modified":1527210540640},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8bf095377d28881f63a30bd7db97526829103bf2","modified":1527210540641},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1527210540640},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"bbe0d111f6451fc04e52719fd538bd0753ec17f9","modified":1527210540642},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1527210540642},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"35c0350096921dd8e2222ec41b6c17a4ea6b44f2","modified":1527210540641},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1527210540642},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"4427ed3250483ed5b7baad74fa93474bd1eda729","modified":1527210540643},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1527210540643},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1527210540643},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1527210540644},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"43bc58daa8d35d5d515dc787ceb21dd77633fe49","modified":1527210540644},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1527210540645},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1527210540644},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1527210540646},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1527210540648},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"5d15cc8bbefe44c77a9b9f96bf04a6033a4b35b8","modified":1527210540646},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1527210540647},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1527210540651},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1527210540649},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1527210540649},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1527210540650},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"10e9bb3392826a5a8f4cabfc14c6d81645f33fe6","modified":1527210540652},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1527210540652},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1527210540652},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1527210540652},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1527210540653},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1527210540634},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1527210540653},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1527210540655},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1527210540655},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1c06be422bc41fd35e5c7948cdea2c09961207f6","modified":1527210540656},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1527210540664},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"76937db9702053d772f6758d9cea4088c2a6e2a3","modified":1527210540656},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1527210540665},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1527210540668},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1532069843154},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1532069843166},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1532069843177},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1532069843179},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1532069843164},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1532069843162},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1532069843181},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1532069843176},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1532069843167},{"_id":"themes/next/source/lib/canvas-nest/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1532069843150},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1532069843183},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1527210540690},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/HEAD","hash":"efc3ba2e37ed6e43395f6e52430f40b98441cafd","modified":1532069843817},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/45/9262fe92f0115707bf8d8764f1886bc5e7c9e0","hash":"36040483f8af76775b7e4b6d87cec53729625399","modified":1532069843765},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/2a/f622a4d7df40a2708946e91d6d7a0df1dc468c","hash":"3da7207fb18d361b83c56f4e35f67e9e945abd82","modified":1532069843763},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/5a/69ce9c2e4a1a34f6063ae9a121af1555669c69","hash":"dad25cc0f450e2827b5676975f4a70636e3fd2c8","modified":1532069843753},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/69/39233ece53c9bdb9a1faf3271ed5768b034aad","hash":"5a770d418c1bb7b0f031f4d5416530002032fcf3","modified":1532069843754},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/69/a20d65d83035fdb01734a8eabe3340f740a4cb","hash":"9e95b02d8e43ec92e06bee3f60dffb74e8e7b9fa","modified":1532069843752},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/75/de2b8fa62d52690de32c351c63ab6446104ed5","hash":"52d10122d633ce4895a0690c5955e1b356f5a391","modified":1532069843764},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/86/1c9f4241fe0eb6af02ad770d5ce04c1f68972b","hash":"7005c3e36015a4af30d4b91bd5a849a7861a073e","modified":1532069843755},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/90/f6477118d05f5f96ce0a63c6f18b7b2baea200","hash":"385f58e92981f27fa54eb52bf60424e87c70a9d8","modified":1532069843751},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/98/67d1132e0e50bbb7df754a63358d70741df6d5","hash":"3cb710a1faee73c08036f5e2df7df3a7ce29e9dd","modified":1532069843749},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/aa/da83ad9aa55faa2b34ede31b1d41e16966f80b","hash":"b304541ab95b7969a63ba2ec4f60f5391bd8bb44","modified":1532069843750},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/d4/95d28a8fab74d23908f6ccef9e4db2625fbacb","hash":"59e6067b0a806deee7bda6460b36c0f63e2e1db5","modified":1532069843754},{"_id":"themes/next/source/lib/canvas-nest/.git/refs/heads/master","hash":"5442226ab36d787824e89f50241336839b376133","modified":1532069843817},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/refs/heads/master","hash":"efc3ba2e37ed6e43395f6e52430f40b98441cafd","modified":1532069843817},{"_id":"themes/next/source/lib/canvas-nest/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1532069843806},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/refs/remotes/origin/HEAD","hash":"efc3ba2e37ed6e43395f6e52430f40b98441cafd","modified":1532069843806},{"_id":"themes/next/.git/objects/pack/pack-9829b0de980dc403efcab0fab027fd96c783804d.pack","hash":"9e1759de63e19531772b171ae72c0b4bb3787630","modified":1527210540437},{"_id":"source/images/MachineLearning-data1.png","hash":"dcad404199efb4674bc212e97d13b53ed461e4ab","modified":1470174746000},{"_id":"source/images/MachineLearning-data2.png","hash":"566a011837e32c2c9e0d9842dbcf731fe02c5c04","modified":1470175506000},{"_id":"public/baidusitemap.xml","hash":"f1d529dfa697fd8b9d635f488621043d1d94f571","modified":1533702355683},{"_id":"public/search.xml","hash":"3f4cc696635993ab3ca382b255b94731f08db4c4","modified":1533702355696},{"_id":"public/sitemap.xml","hash":"434dbefc9cdc805ec3ebab2b689a4cfbe36ca316","modified":1533702355697},{"_id":"public/tags/index.html","hash":"d478b05e0d1787cc17df51a3af60409c0c6c46fe","modified":1533271743743},{"_id":"public/categories/index.html","hash":"94914c1d590fb420fdbc763370c6e8642769538d","modified":1533271743745},{"_id":"public/about/index.html","hash":"f814cd973ba29f99f123012e6615789564a51aa1","modified":1533271743745},{"_id":"public/2018/07/16/cs231n-note-5/index.html","hash":"16ea7b01a1bf1a9482e34f8ef00b35a2c3aa6955","modified":1533271743745},{"_id":"public/2018/07/02/中国剩余定理/index.html","hash":"1d4fb1f2ee53af71cbe3970587bd93426a6bf687","modified":1533271743746},{"_id":"public/2018/06/23/有趣的生物灵魂/index.html","hash":"fd6835002425124ccf6078f9cd865a058b9df75e","modified":1533271743746},{"_id":"public/2018/06/18/cs231n-note-4/index.html","hash":"46423f934d37b0f1d90950ec62b516edca4e5959","modified":1533271743746},{"_id":"public/2018/06/04/sequelize-cli-NodeJs-Learning-Notes/index.html","hash":"918dec119753f7caabdacaa0d66dd7153dce63b6","modified":1533271743746},{"_id":"public/2018/06/04/cs231n-note-3/index.html","hash":"51ada607d6a596482b2959f7327a816ab1e91692","modified":1533271743746},{"_id":"public/2018/05/24/Install-python-library/index.html","hash":"9282de17c13dcada9d52dd37c5ed4932a2fce735","modified":1533271743746},{"_id":"public/2018/05/20/cs231n-note-1/index.html","hash":"e3c8181968d6ca8cd42f276a45f6418a2587edc4","modified":1533271743746},{"_id":"public/2018/05/21/cs231n-note-2/index.html","hash":"60d296d528f1dbc0c59bd6f95aff1efdc192bf65","modified":1533271743746},{"_id":"public/2018/05/20/Machine-Learning-Notes-1/index.html","hash":"30a0d9b08bbefbfefa4f50ad98cfcafec39a87c9","modified":1533271743747},{"_id":"public/2017/06/13/Docker-Notes/index.html","hash":"6099bbb12756a6d6a6e02844f4b740ec986533a5","modified":1533271743747},{"_id":"public/2016/08/02/Machine-Learning-Project-1-Weijie-Sun/index.html","hash":"372f930967adbaead20ab7e99f8802a5a7215dd3","modified":1533271743747},{"_id":"public/2016/07/27/My Ionic-Hybrid Experience[1]/index.html","hash":"f170b65483f3374a2e095e3bf2b44c40eb97a218","modified":1533271743747},{"_id":"public/2016/07/18/HIV-project/index.html","hash":"18783153cc9664dbe73595351939d8ca4d446138","modified":1533271743747},{"_id":"public/2016/07/15/hello-world/index.html","hash":"76fd33e65b82b85bc9d47dc2a287307700482f5a","modified":1533271743747},{"_id":"public/2016/07/18/Algorithm/index.html","hash":"cbf6df3a1e3a2562ce817d5115b0df962f6096d7","modified":1533271743748},{"_id":"public/archives/index.html","hash":"055e78243c6a31b930a0570631c43b9036b3da54","modified":1533271743748},{"_id":"public/archives/page/2/index.html","hash":"a9bb2eccaabac39b5ea252fcfbcd537d81c19117","modified":1533271743748},{"_id":"public/archives/2016/index.html","hash":"dbfefa785c1b26be0870bd4d6ffe91b4cd936c75","modified":1533271743748},{"_id":"public/archives/2016/07/index.html","hash":"370db7f20d946bd07c873edd6e3ac15e58577ac9","modified":1533271743748},{"_id":"public/archives/2016/08/index.html","hash":"0627b1ff487c62cf1d7bee89e9fdf1aea68f55f1","modified":1533271743748},{"_id":"public/archives/2017/index.html","hash":"38881964355cf5d7f8d17c038161d9230724238b","modified":1533271743748},{"_id":"public/archives/2017/06/index.html","hash":"ad6bf6d63b07d98a4d1152f6ac242afbf5b4ea50","modified":1533271743748},{"_id":"public/archives/2018/index.html","hash":"0a9b83f844bd882fa52da4e58134801046f2a99c","modified":1533271743748},{"_id":"public/archives/2018/05/index.html","hash":"e504c067ea7141e24b2aaf5c129791548594ccd9","modified":1533271743748},{"_id":"public/archives/2018/06/index.html","hash":"9f52f07b3936dbc9dbc10e1ac0b3d7e8ac7d9b77","modified":1533271743748},{"_id":"public/archives/2018/07/index.html","hash":"a1af25ced59941f398e2eae99947adffb6eec6da","modified":1533271743749},{"_id":"public/categories/Algorithm/index.html","hash":"44e5a7d3b4869142090a3422e283e6f6a38ce725","modified":1533271743749},{"_id":"public/index.html","hash":"bdd1d88e2057e645b704d47cf22b1428d8852f0d","modified":1533702355701},{"_id":"public/page/2/index.html","hash":"99de1d910410925e83902316cae2e1d37403f3da","modified":1533271743749},{"_id":"public/tags/Algorithm/index.html","hash":"9fd16e5ef98762b8fef8737971d0afd3d8c3e732","modified":1533271743749},{"_id":"public/tags/Math/index.html","hash":"e7b7acbc039fe56e572f056d123d1f54317009d8","modified":1533271743749},{"_id":"public/tags/Machine-Learning/index.html","hash":"e17179045ce483ee05373395dd5f451a7b67e072","modified":1533271743749},{"_id":"public/tags/Web-Mobile/index.html","hash":"9d48927a33b39e7092112556eea65787136b542e","modified":1533271743749},{"_id":"public/tags/python/index.html","hash":"fdfdfd5ac25a3d478b78390b9fa7e23185703626","modified":1533271743749},{"_id":"public/tags/NodeJs/index.html","hash":"8706dd7e655cecc1a7099ba1454c48a6b38f5bac","modified":1533271743749},{"_id":"public/tags/Biology/index.html","hash":"4aa057af74a7ec385b99afd91f3be50dcdf2b98b","modified":1533271743749},{"_id":"public/tags/math/index.html","hash":"ed8cd74985b0ee5340d2897467058c49e49900bd","modified":1533271743749},{"_id":"public/2018/07/30/数学手册/index.html","hash":"968ffc8192c7c6f7dbce8a0a3458680dbc2d4e97","modified":1533702355701},{"_id":"public/2018/07/29/cs231n-note-6/index.html","hash":"97a71a92116cdaf7218d2c53e3dc6b5f8921ede8","modified":1533271743755},{"_id":"public/archives/2018/page/2/index.html","hash":"78d5389b639018663090a958cc7ed59bab4a3bf7","modified":1533271743755},{"_id":"public/images/avatar.jpeg","hash":"e9d35b3852716876576dd060d7f83261cfd9d96e","modified":1533271743756},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1533271743756},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1533271743756},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1533271743756},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1533271743756},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1533271743756},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1533271743756},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1533271743756},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1533271743756},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1533271743756},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1533271743756},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1533271743757},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1533271743757},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1533271743757},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1533271743757},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1533271743757},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1533271743757},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1533271743757},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1533271743757},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1533271743757},{"_id":"public/lib/canvas-nest/LICENSE","hash":"b29db4c99aa5b8d574026f68804051ff4b75466e","modified":1533271745092},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1533271745092},{"_id":"public/images/ionic-Structure.png","hash":"ea92ba8ac9fdad537c6b2f68a62ae8fa598e1d8a","modified":1533271745101},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1533271745168},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1533271745168},{"_id":"public/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1533271745171},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1533271745171},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1533271745171},{"_id":"public/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1533271745171},{"_id":"public/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1533271745171},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1533271745171},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1533271745172},{"_id":"public/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1533271745172},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1533271745172},{"_id":"public/css/main.css","hash":"44c7238c8f2407530dd9bf69aa63f33fb57b8a31","modified":1533271745172},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1533271745172},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1533271745172},{"_id":"public/lib/canvas-nest/README.html","hash":"9bb29b415d0859e097fdfcc6689b235c0e699224","modified":1533271745179},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1533271745179},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1533271745179},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1533271745180},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1533271745180},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1533271745180},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1533271745180},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1533271745180},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1533271745180},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1533271745180},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1533271745180},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1533271745180},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1533271745244},{"_id":"public/images/MachineLearning-data1.png","hash":"dcad404199efb4674bc212e97d13b53ed461e4ab","modified":1533271745441},{"_id":"public/images/MachineLearning-data2.png","hash":"566a011837e32c2c9e0d9842dbcf731fe02c5c04","modified":1533271745448}],"Category":[{"name":"Algorithm","_id":"cjkdiekzl0009im0rzqqfm9nu"}],"Data":[],"Page":[{"_content":"\nPlan：\n\n 在学CS231n 37.5%\n\n\n 在用后端: Node expressJs/restify  PHP Laravel Python Django\n 在用前端：AngularJs/ReactJs\n 在用ML research: pymol Tensorflow/Keras/Scikit-learn\n ","source":"categories/index.md","raw":"\nPlan：\n\n 在学CS231n 37.5%\n\n\n 在用后端: Node expressJs/restify  PHP Laravel Python Django\n 在用前端：AngularJs/ReactJs\n 在用ML research: pymol Tensorflow/Keras/Scikit-learn\n ","date":"2018-06-18T04:26:03.546Z","updated":"2018-06-18T04:26:03.546Z","path":"categories/index.html","title":"","comments":1,"layout":"page","_id":"cjkdiekz50001im0r9jnac0i3","content":"<p>Plan：</p>\n<p> 在学CS231n 37.5%</p>\n<p> 在用后端: Node expressJs/restify  PHP Laravel Python Django<br> 在用前端：AngularJs/ReactJs<br> 在用ML research: pymol Tensorflow/Keras/Scikit-learn</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Plan：</p>\n<p> 在学CS231n 37.5%</p>\n<p> 在用后端: Node expressJs/restify  PHP Laravel Python Django<br> 在用前端：AngularJs/ReactJs<br> 在用ML research: pymol Tensorflow/Keras/Scikit-learn</p>\n"},{"title":"All tags","date":"2016-07-21T21:02:24.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: All tags\ndate: 2016-07-21 15:02:24\ntype: \"tags\"\n---\n","updated":"2016-07-21T21:03:31.000Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjkdiekz80003im0rj7b3knuh","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"about","date":"2018-05-25T01:14:20.000Z","_content":"\nWeijie Sun\n\nUniversity of Alberta\n\nSpecialization of Computing Science.\n\n\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2018-05-24 19:14:20\n---\n\nWeijie Sun\n\nUniversity of Alberta\n\nSpecialization of Computing Science.\n\n\n","updated":"2018-05-25T02:14:04.579Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjkdiekzc0005im0r86ginvsd","content":"<p>Weijie Sun</p>\n<p>University of Alberta</p>\n<p>Specialization of Computing Science.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Weijie Sun</p>\n<p>University of Alberta</p>\n<p>Specialization of Computing Science.</p>\n"}],"Post":[{"title":"Basic SEO (1) - Weijie Sun","_content":"","source":"_drafts/Basic-SEO-1-Weijie-Sun.md","raw":"---\ntitle: Basic SEO (1) - Weijie Sun\ntags:\n---\n","slug":"Basic-SEO-1-Weijie-Sun","published":0,"date":"2016-07-21T21:55:37.000Z","updated":"2016-07-21T21:55:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekyz0000im0rz9olxpui","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Docker Notes","date":"2017-06-14T04:40:16.000Z","_content":"\nContainers \n\nList Containers\n\ndocker ps (show all the running container)\n\ndocker ps -a (show all the container include which is not running)\n\nStart container\n\ndocker start {containerId} (start one container)\n\nStop container\n\ndocker stop {containerId} (Stop one container)\n\nDocker-Compose\n\nCreate a docker-compose.yml [document](https://docs.docker.com/compose/compose-file/compose-file-v2/#build)\n\nTo be continue\n","source":"_posts/Docker-Notes.md","raw":"---\ntitle: Docker Notes\ndate: 2017-06-13 22:40:16\ntags:\n---\n\nContainers \n\nList Containers\n\ndocker ps (show all the running container)\n\ndocker ps -a (show all the container include which is not running)\n\nStart container\n\ndocker start {containerId} (start one container)\n\nStop container\n\ndocker stop {containerId} (Stop one container)\n\nDocker-Compose\n\nCreate a docker-compose.yml [document](https://docs.docker.com/compose/compose-file/compose-file-v2/#build)\n\nTo be continue\n","slug":"Docker-Notes","published":1,"updated":"2017-06-14T05:12:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekz70002im0rk7522j33","content":"<p>Containers </p>\n<p>List Containers</p>\n<p>docker ps (show all the running container)</p>\n<p>docker ps -a (show all the container include which is not running)</p>\n<p>Start container</p>\n<p>docker start {containerId} (start one container)</p>\n<p>Stop container</p>\n<p>docker stop {containerId} (Stop one container)</p>\n<p>Docker-Compose</p>\n<p>Create a docker-compose.yml <a href=\"https://docs.docker.com/compose/compose-file/compose-file-v2/#build\" target=\"_blank\" rel=\"noopener\">document</a></p>\n<p>To be continue</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Containers </p>\n<p>List Containers</p>\n<p>docker ps (show all the running container)</p>\n<p>docker ps -a (show all the container include which is not running)</p>\n<p>Start container</p>\n<p>docker start {containerId} (start one container)</p>\n<p>Stop container</p>\n<p>docker stop {containerId} (Stop one container)</p>\n<p>Docker-Compose</p>\n<p>Create a docker-compose.yml <a href=\"https://docs.docker.com/compose/compose-file/compose-file-v2/#build\" target=\"_blank\" rel=\"noopener\">document</a></p>\n<p>To be continue</p>\n"},{"title":"HIV_project - Weijie Sun","date":"2016-07-18T21:32:18.000Z","_content":"\nHIV virus's protine order like hexagon. \nIn the previous research, they working on the 2 dimension. However, in 3 dimension, the protine fills spherical surface.\n\nThis prgramming project is used to help the biology researcher to plot the HIV virus and get a list of all surface protine coordinations by setting the different radius.\n\n## The main structure.\nIf the point B rotated point A and get point C, I named points A is the father or root of B and C. As I mentioned that HIV virus's surfaces protines order like a hexagen, protine will rotated 120 degree to get another protine. \n\nThere are a K_keepers list of points which generated and a Pivot list of points.\n\nUse a Queue push in the points generated by the other 2 points, and push out the points which is already generated 2 points. When the distance is less than the protine's radius, then the protines should not be placed at that coordination.\n\nReference:\nhttps://en.wikipedia.org/wiki/Rotation_matrix\n","source":"_posts/HIV-project.md","raw":"---\ntitle: HIV_project - Weijie Sun\ndate: 2016-07-18 15:32:18\ntags: Algorithm\n---\n\nHIV virus's protine order like hexagon. \nIn the previous research, they working on the 2 dimension. However, in 3 dimension, the protine fills spherical surface.\n\nThis prgramming project is used to help the biology researcher to plot the HIV virus and get a list of all surface protine coordinations by setting the different radius.\n\n## The main structure.\nIf the point B rotated point A and get point C, I named points A is the father or root of B and C. As I mentioned that HIV virus's surfaces protines order like a hexagen, protine will rotated 120 degree to get another protine. \n\nThere are a K_keepers list of points which generated and a Pivot list of points.\n\nUse a Queue push in the points generated by the other 2 points, and push out the points which is already generated 2 points. When the distance is less than the protine's radius, then the protines should not be placed at that coordination.\n\nReference:\nhttps://en.wikipedia.org/wiki/Rotation_matrix\n","slug":"HIV-project","published":1,"updated":"2016-07-19T23:37:49.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekzb0004im0rlqyr3w2s","content":"<p>HIV virus’s protine order like hexagon.<br>In the previous research, they working on the 2 dimension. However, in 3 dimension, the protine fills spherical surface.</p>\n<p>This prgramming project is used to help the biology researcher to plot the HIV virus and get a list of all surface protine coordinations by setting the different radius.</p>\n<h2 id=\"The-main-structure\"><a href=\"#The-main-structure\" class=\"headerlink\" title=\"The main structure.\"></a>The main structure.</h2><p>If the point B rotated point A and get point C, I named points A is the father or root of B and C. As I mentioned that HIV virus’s surfaces protines order like a hexagen, protine will rotated 120 degree to get another protine. </p>\n<p>There are a K_keepers list of points which generated and a Pivot list of points.</p>\n<p>Use a Queue push in the points generated by the other 2 points, and push out the points which is already generated 2 points. When the distance is less than the protine’s radius, then the protines should not be placed at that coordination.</p>\n<p>Reference:<br><a href=\"https://en.wikipedia.org/wiki/Rotation_matrix\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Rotation_matrix</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>HIV virus’s protine order like hexagon.<br>In the previous research, they working on the 2 dimension. However, in 3 dimension, the protine fills spherical surface.</p>\n<p>This prgramming project is used to help the biology researcher to plot the HIV virus and get a list of all surface protine coordinations by setting the different radius.</p>\n<h2 id=\"The-main-structure\"><a href=\"#The-main-structure\" class=\"headerlink\" title=\"The main structure.\"></a>The main structure.</h2><p>If the point B rotated point A and get point C, I named points A is the father or root of B and C. As I mentioned that HIV virus’s surfaces protines order like a hexagen, protine will rotated 120 degree to get another protine. </p>\n<p>There are a K_keepers list of points which generated and a Pivot list of points.</p>\n<p>Use a Queue push in the points generated by the other 2 points, and push out the points which is already generated 2 points. When the distance is less than the protine’s radius, then the protines should not be placed at that coordination.</p>\n<p>Reference:<br><a href=\"https://en.wikipedia.org/wiki/Rotation_matrix\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Rotation_matrix</a></p>\n"},{"title":"Algorithm - Weijie Sun","date":"2016-07-18T22:54:34.000Z","_content":"integer factorization algorithm\n======\n1 Introduction\n------\n| Algorithm        \t\t\t\t      | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |trial_division.py \t\t    |note0 \t\t\t\t\t \t             |\n| Euler's factorization method  |euler_trivial.py note2   |note1\t\t\t\t\t\t             |\n| Fermat's factorization method |fermatfactor_trivial.py  |fermatfactor_improved_prime.py|\n| Pollard's rho algorithm \t\t  |Pollards_rho_trivial.py  |Pollards_rho_improved_prime.py|note3\n\n* note0 : Trivial Division use the trivial prime, which cannot use Miller-Rabin primality test\n* note1 : Euler's factorization method stops the algorithm\n\t\t  when a number cannot found number = a^2 + b^2 = c^2 + d^2. \n\t\t  Miller-Rabin is only judge the prime not find the factors.\n* note2 : Since I have not find a better way to find a number = a^2 + b^2 = c^2 + d^2. \n\t\t  So Euler's spend on the this function I will list after that \n* note3 : In Pollards_rho Miller-Rabin primality test, inorder to handle some base cases. I use the \n          trial_division to handle some special cases.\n\n#Integer factorization:\n\n### 1.1\nTrial division: (Baseline)\nThis is a base line in my integer factorization algorithm project. It is using \na prime sieve for prime number generation which can judge a number is a prime or \ncontinue do factorization.\n\nRunning time: in the worst case it is O(2^(n/2)/((n/2)*ln2)) \n* n is base-2 n digit number.\n\n### 1.2\nEuler's factorization method:\nThis is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Euler's factorization is if \na number = a^2 + b^2 = c^2 + d^2. There is a quickly way to seperate into 2 factors.\n\nRunning time: It is very slow, worst case is greater than trial division,\nonly quick in some special cases and has potient quick.\n\n### 1.3\nFermat's factorization method:\nThis is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Fermat's factorization method:\n is if a number = a^2 - b^2 There is a quickly way to seperate into 2 factors.\n\nRunning time: Worst case is O(N^{1/2})  \n\t\t\t  General case is O(N^{1/3}) time.\n\n### 1.4\nPollard's rho algorithm:\nThis is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Pollard's rho algorithm is if a find \nthe abs(x^2+1-x) mod N if not 1 then it is a factor of N.\n\nRunning time: General case by the Birthday paradox in O(\\sqrt p)\\ <= O(n^{1/4})\n\t\t\t  but this is a heuristic claim, and rigorous analysis of the algorithm remains open.\n\n2 A description of what sort of tests I have included\n------\n# 1 St: 2 primes production (each prime > million)\n\nWhen the prime is very big, test the speed of each methods.\n\n###1.1\nThe testcases: 15485867*32452867 = 502560782130689\n\n| Algorithm        \t\t\t\t      | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |31.0308229923 \t\t  \t    |null \t\t\t\t\t \t             |\n| Euler's factorization method  |32.3473279476+3.276534002|null\t\t\t\t\t\t\t             |\n| Fermat's factorization method |2.5645339489  \t\t\t      |3.19916701317 \t\t\t\t         |\n| Pollard's rho algorithm \t\t  |0.0414018630981          |0.0358607769012 \t\t\t\t       |\n\n###1.2\nThe testcases: 15487019*15487469 = 239854726664911\n\n| Algorithm                     | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |22.700715065 \t\t  \t    |null \t\t\t\t\t \t             |\n| Euler's factorization method  |21.358424902+3.0871624554|null\t\t\t\t\t\t\t             |\n| Fermat's factorization method |3.99884200096  \t\t  |2.206387043\t\t\t\t\t |\n| Pollard's rho algorithm \t\t|0.0165410041809          |0.0110921859741 \t\t\t\t |\n\n###1.3\nThe testcases: 15490781*67869427 = 1051350430252487\n\n| Algorithm                     | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |53.9460468292            |null                          |\n| Euler's factorization method  |47.7404336929+8.194948196|null                          |\n| Fermat's factorization method |8.05504488945            |9.30907988548           |\n| Pollard's rho algorithm       |0.0999021530151          |0.0918011665344         |\n\n# 2 nd: 4 primes production (each prime between[1000,9999])\n\nWhen the prime is big and more factors, test the speed of each methods.\n\n###2.1\nThe testcases: 8147 * 8369 * 8623 * 7127 = 4190216175859403\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |109.196480989 \t\t  \t  |null \t\t\t\t\t \t |\n| Euler's factorization method  |105.0888099666+5.89233399|null\t\t\t\t\t\t\t |\n| Fermat's factorization method |2.5645339489  \t\t\t  |3.19916701317 \t\t\t\t |\n| Pollard's rho algorithm \t\t  |0.0414018630981          |0.0358607769012 \t\t\t\t |\n\n###2.2\nThe testcases: 1259 * 1451 * 1613 * 1811 = 5336370322687\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |2.96865296364 \t\t  \t  |null \t\t\t\t\t \t |\n| Euler's factorization method  |3.3992729187+0.5768110752|null\t\t\t\t\t\t\t |\n| Fermat's factorization method |5.54617881775  \t      |3.116948843 \t\t\t\t \t |\n| Pollard's rho algorithm \t\t  |0.00448799133301         |0.00167012214661 \t\t \t |\n\n###2.3\nThe testcases: 6277 * 5351 * 8831 * 9733 = 2886979418455921\n\n| Algorithm                     | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |77.4180650711          |null              |\n| Euler's factorization method  |72.7111520767+13.2122049|null              |\n| Fermat's factorization method |2.87196207047          |3.24289989471           |\n| Pollard's rho algorithm       |0.479516983032         |0.00454497337341        |\n\n# 3 rd: over 6 small prime product\n\nWhen the prime is not that big, but we have more factors for the number which need to be factorization.\n\n###3.1\nThe testcases: 13 * 127 * 263 * 419 * 17 * 131 * 269 = 108990674873561\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |12.9187300205 \t\t  \t  |null \t\t\t\t\t \t |\n| Euler's factorization method  |12.9247641563+2.445067882|null\t\t\t\t\t\t\t |\n| Fermat's factorization method |3.29201412201  \t\t  |2.98240017891 \t\t\t\t |\n| Pollard's rho algorithm \t\t  |0.00494313240051         |0.00559782981873 \t\t\t |\n\n###3.2\nThe testcases: 13 * 17 * 2 * 1123 * 1426499 * 5 = 3540328013170\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |4.11624193192 \t\t  \t  |null \t\t\t\t\t \t |\n| Euler's factorization method  |4.38335967064+0.452334165|null\t\t\t\t\t\t\t |\n| Fermat's factorization method |3.39200806618 \t\t  \t  |2.18938994408 \t\t\t\t |\n| Pollard's rho algorithm \t\t|0.00471496582031         |0.00225687026978\t\t \t\t |\n\n###3.3\nThe testcases: 547 * 701 * 29 * 149 * 5 * 2 = 16568744870\n\n| Algorithm               | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |3.92766094208          |null              |\n| Euler's factorization method  |2.42819094658+0.031641006|null              |\n| Fermat's factorization method |3.00680112839          |4.95704817772         |\n| Pollard's rho algorithm       |0.0637471675873         |0.000675916671753        |\n\n3 How to run the code:\n------\nThere is a easy way to run by \n```python runner.py```\nThen following the introduction. \n\nFirstly, you will see this introduction format.\n```\ninteger factorization algorithm\ntype the number you want to run the algorithm\neg: 1\nthen will run the Trial division\n1.Trial division\n2.Euler's factorization method\n3.Fermat's factorization method\n4.Pollard's rho algorithm \ninput the number:\n```\nThen input the id of factorization method.\n\nfor the 3rd and 4th, you also need to choose the id of the prime generate method.\n```\nrunner.py \n  -+----1 \"Trial division\"----+---\"type number of testcases\"\n   |\t\t\t\t\t      +---\"input the number which needs factorization\"\n   |\t\t\t\t\t\t  \n   |\n   +----2 \"Euler's factorization method\"---+---\"type number of testcases\"\n   |\t\t\t\t\t\t\t\t\t   +---\"input the number which needs factorization\"\n   |\n   |\n   +----3 \"Fermat's factorization method\"---+---\"type the prime generate method\"\n   |\t\t\t\t\t\t\t\t\t    +---1 \"trivial prime\"\n   |\t\t\t\t\t\t\t\t\t\t  +\t   +---\"type number of testcases\"\n   |\t\t\t\t\t\t\t\t\t    |    +---\"input the number which needs factorization\"\n   |\t\t\t\t\t\t\t\t\t    |\n   |\t\t\t\t\t\t\t\t\t    +---2 \"Miller-Rabin primality test\"\n   |\t\t\t\t\t\t\t\t\t\t\t +---\"type number of testcases\"\n   |\t\t\t\t\t\t\t\t\t         +---\"input the number which needs factorization\"\n   |\n   |\n   +----4 \"Pollard's rho algorithm\"---+---\"type the prime generate method\"\n    \t\t\t\t\t\t\t\t  +---1 \"trivial prime\"\n    \t\t\t\t\t\t  \t\t|\t    +---\"type number of testcases\"\n    \t\t\t\t\t\t\t\t  |     +---\"input the number which needs factorization\"\n    \t\t\t\t\t\t     \t|\n    \t\t\t\t\t\t\t\t  +---2 \"Miller-Rabin primality test\"\n    \t\t\t\t\t\t\t\t  \t \t+---\"type number of testcases\"\n    \t\t\t\t\t\t\t\t\t    +---\"input the number which needs factorization\"\n```\nOr \n\nYou can runner each single file\nwhich has been listed:\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |trial_division.py \t\t  |note0 \t\t\t\t\t \t |\n| Euler's factorization method  |euler_trivial.py note2   |note1\t\t\t\t\t\t |\n| Fermat's factorization method |fermatfactor_trivial.py  |fermatfactor_improved_prime.py|\n| Pollard's rho algorithm \t\t|Pollards_rho_trivial.py  |Pollards_rho_improved_prime.py|\n\n\nAnything else you deem relevant.\n------\n1 I used the trivial prime:\n\twhich judge if a number is a prime.\n\n2 I used another prime test called Miller-Rabin primality test:\n\twhich is much quicker and I have referenced in all my test cases. Prime test is correct.\n\nReference\n------\nhttp://www.bigprimes.net/archive/prime/10/\n\nhttps://rosettacode.org/wiki/Miller–Rabin_primality_test#Python:_Probably_correct_answers\n\nhttps://en.wikipedia.org/wiki/Integer_factorization\n\nhttps://en.wikipedia.org/wiki/Trial_division\n\nhttps://en.wikipedia.org/wiki/Fermat%27s_factorization_method\n\nhttps://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm\n\nhttps://en.wikipedia.org/wiki/Euler%27s_factorization_method\n\nhttp://mathworld.wolfram.com/PollardRhoFactorizationMethod.html\n\nhttp://kadinumberprops.blogspot.ca/2012/11/fermats-factorization-running-time.html\n","source":"_posts/Algorithm.md","raw":"title: Algorithm - Weijie Sun\ndate: 2016-07-18 16:54:34\ntags:\n\t- Algorithm\n\t- Math\ncategories: Algorithm\n---\ninteger factorization algorithm\n======\n1 Introduction\n------\n| Algorithm        \t\t\t\t      | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |trial_division.py \t\t    |note0 \t\t\t\t\t \t             |\n| Euler's factorization method  |euler_trivial.py note2   |note1\t\t\t\t\t\t             |\n| Fermat's factorization method |fermatfactor_trivial.py  |fermatfactor_improved_prime.py|\n| Pollard's rho algorithm \t\t  |Pollards_rho_trivial.py  |Pollards_rho_improved_prime.py|note3\n\n* note0 : Trivial Division use the trivial prime, which cannot use Miller-Rabin primality test\n* note1 : Euler's factorization method stops the algorithm\n\t\t  when a number cannot found number = a^2 + b^2 = c^2 + d^2. \n\t\t  Miller-Rabin is only judge the prime not find the factors.\n* note2 : Since I have not find a better way to find a number = a^2 + b^2 = c^2 + d^2. \n\t\t  So Euler's spend on the this function I will list after that \n* note3 : In Pollards_rho Miller-Rabin primality test, inorder to handle some base cases. I use the \n          trial_division to handle some special cases.\n\n#Integer factorization:\n\n### 1.1\nTrial division: (Baseline)\nThis is a base line in my integer factorization algorithm project. It is using \na prime sieve for prime number generation which can judge a number is a prime or \ncontinue do factorization.\n\nRunning time: in the worst case it is O(2^(n/2)/((n/2)*ln2)) \n* n is base-2 n digit number.\n\n### 1.2\nEuler's factorization method:\nThis is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Euler's factorization is if \na number = a^2 + b^2 = c^2 + d^2. There is a quickly way to seperate into 2 factors.\n\nRunning time: It is very slow, worst case is greater than trial division,\nonly quick in some special cases and has potient quick.\n\n### 1.3\nFermat's factorization method:\nThis is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Fermat's factorization method:\n is if a number = a^2 - b^2 There is a quickly way to seperate into 2 factors.\n\nRunning time: Worst case is O(N^{1/2})  \n\t\t\t  General case is O(N^{1/3}) time.\n\n### 1.4\nPollard's rho algorithm:\nThis is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Pollard's rho algorithm is if a find \nthe abs(x^2+1-x) mod N if not 1 then it is a factor of N.\n\nRunning time: General case by the Birthday paradox in O(\\sqrt p)\\ <= O(n^{1/4})\n\t\t\t  but this is a heuristic claim, and rigorous analysis of the algorithm remains open.\n\n2 A description of what sort of tests I have included\n------\n# 1 St: 2 primes production (each prime > million)\n\nWhen the prime is very big, test the speed of each methods.\n\n###1.1\nThe testcases: 15485867*32452867 = 502560782130689\n\n| Algorithm        \t\t\t\t      | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |31.0308229923 \t\t  \t    |null \t\t\t\t\t \t             |\n| Euler's factorization method  |32.3473279476+3.276534002|null\t\t\t\t\t\t\t             |\n| Fermat's factorization method |2.5645339489  \t\t\t      |3.19916701317 \t\t\t\t         |\n| Pollard's rho algorithm \t\t  |0.0414018630981          |0.0358607769012 \t\t\t\t       |\n\n###1.2\nThe testcases: 15487019*15487469 = 239854726664911\n\n| Algorithm                     | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |22.700715065 \t\t  \t    |null \t\t\t\t\t \t             |\n| Euler's factorization method  |21.358424902+3.0871624554|null\t\t\t\t\t\t\t             |\n| Fermat's factorization method |3.99884200096  \t\t  |2.206387043\t\t\t\t\t |\n| Pollard's rho algorithm \t\t|0.0165410041809          |0.0110921859741 \t\t\t\t |\n\n###1.3\nThe testcases: 15490781*67869427 = 1051350430252487\n\n| Algorithm                     | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |53.9460468292            |null                          |\n| Euler's factorization method  |47.7404336929+8.194948196|null                          |\n| Fermat's factorization method |8.05504488945            |9.30907988548           |\n| Pollard's rho algorithm       |0.0999021530151          |0.0918011665344         |\n\n# 2 nd: 4 primes production (each prime between[1000,9999])\n\nWhen the prime is big and more factors, test the speed of each methods.\n\n###2.1\nThe testcases: 8147 * 8369 * 8623 * 7127 = 4190216175859403\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |109.196480989 \t\t  \t  |null \t\t\t\t\t \t |\n| Euler's factorization method  |105.0888099666+5.89233399|null\t\t\t\t\t\t\t |\n| Fermat's factorization method |2.5645339489  \t\t\t  |3.19916701317 \t\t\t\t |\n| Pollard's rho algorithm \t\t  |0.0414018630981          |0.0358607769012 \t\t\t\t |\n\n###2.2\nThe testcases: 1259 * 1451 * 1613 * 1811 = 5336370322687\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |2.96865296364 \t\t  \t  |null \t\t\t\t\t \t |\n| Euler's factorization method  |3.3992729187+0.5768110752|null\t\t\t\t\t\t\t |\n| Fermat's factorization method |5.54617881775  \t      |3.116948843 \t\t\t\t \t |\n| Pollard's rho algorithm \t\t  |0.00448799133301         |0.00167012214661 \t\t \t |\n\n###2.3\nThe testcases: 6277 * 5351 * 8831 * 9733 = 2886979418455921\n\n| Algorithm                     | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |77.4180650711          |null              |\n| Euler's factorization method  |72.7111520767+13.2122049|null              |\n| Fermat's factorization method |2.87196207047          |3.24289989471           |\n| Pollard's rho algorithm       |0.479516983032         |0.00454497337341        |\n\n# 3 rd: over 6 small prime product\n\nWhen the prime is not that big, but we have more factors for the number which need to be factorization.\n\n###3.1\nThe testcases: 13 * 127 * 263 * 419 * 17 * 131 * 269 = 108990674873561\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |12.9187300205 \t\t  \t  |null \t\t\t\t\t \t |\n| Euler's factorization method  |12.9247641563+2.445067882|null\t\t\t\t\t\t\t |\n| Fermat's factorization method |3.29201412201  \t\t  |2.98240017891 \t\t\t\t |\n| Pollard's rho algorithm \t\t  |0.00494313240051         |0.00559782981873 \t\t\t |\n\n###3.2\nThe testcases: 13 * 17 * 2 * 1123 * 1426499 * 5 = 3540328013170\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |4.11624193192 \t\t  \t  |null \t\t\t\t\t \t |\n| Euler's factorization method  |4.38335967064+0.452334165|null\t\t\t\t\t\t\t |\n| Fermat's factorization method |3.39200806618 \t\t  \t  |2.18938994408 \t\t\t\t |\n| Pollard's rho algorithm \t\t|0.00471496582031         |0.00225687026978\t\t \t\t |\n\n###3.3\nThe testcases: 547 * 701 * 29 * 149 * 5 * 2 = 16568744870\n\n| Algorithm               | trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |3.92766094208          |null              |\n| Euler's factorization method  |2.42819094658+0.031641006|null              |\n| Fermat's factorization method |3.00680112839          |4.95704817772         |\n| Pollard's rho algorithm       |0.0637471675873         |0.000675916671753        |\n\n3 How to run the code:\n------\nThere is a easy way to run by \n```python runner.py```\nThen following the introduction. \n\nFirstly, you will see this introduction format.\n```\ninteger factorization algorithm\ntype the number you want to run the algorithm\neg: 1\nthen will run the Trial division\n1.Trial division\n2.Euler's factorization method\n3.Fermat's factorization method\n4.Pollard's rho algorithm \ninput the number:\n```\nThen input the id of factorization method.\n\nfor the 3rd and 4th, you also need to choose the id of the prime generate method.\n```\nrunner.py \n  -+----1 \"Trial division\"----+---\"type number of testcases\"\n   |\t\t\t\t\t      +---\"input the number which needs factorization\"\n   |\t\t\t\t\t\t  \n   |\n   +----2 \"Euler's factorization method\"---+---\"type number of testcases\"\n   |\t\t\t\t\t\t\t\t\t   +---\"input the number which needs factorization\"\n   |\n   |\n   +----3 \"Fermat's factorization method\"---+---\"type the prime generate method\"\n   |\t\t\t\t\t\t\t\t\t    +---1 \"trivial prime\"\n   |\t\t\t\t\t\t\t\t\t\t  +\t   +---\"type number of testcases\"\n   |\t\t\t\t\t\t\t\t\t    |    +---\"input the number which needs factorization\"\n   |\t\t\t\t\t\t\t\t\t    |\n   |\t\t\t\t\t\t\t\t\t    +---2 \"Miller-Rabin primality test\"\n   |\t\t\t\t\t\t\t\t\t\t\t +---\"type number of testcases\"\n   |\t\t\t\t\t\t\t\t\t         +---\"input the number which needs factorization\"\n   |\n   |\n   +----4 \"Pollard's rho algorithm\"---+---\"type the prime generate method\"\n    \t\t\t\t\t\t\t\t  +---1 \"trivial prime\"\n    \t\t\t\t\t\t  \t\t|\t    +---\"type number of testcases\"\n    \t\t\t\t\t\t\t\t  |     +---\"input the number which needs factorization\"\n    \t\t\t\t\t\t     \t|\n    \t\t\t\t\t\t\t\t  +---2 \"Miller-Rabin primality test\"\n    \t\t\t\t\t\t\t\t  \t \t+---\"type number of testcases\"\n    \t\t\t\t\t\t\t\t\t    +---\"input the number which needs factorization\"\n```\nOr \n\nYou can runner each single file\nwhich has been listed:\n\n| Algorithm        \t\t\t\t| trivial prime           | Miller-Rabin primality test  |\n| ----------------------------- |:-----------------------:| ----------------------------:|\n| Trial division(baseline)      |trial_division.py \t\t  |note0 \t\t\t\t\t \t |\n| Euler's factorization method  |euler_trivial.py note2   |note1\t\t\t\t\t\t |\n| Fermat's factorization method |fermatfactor_trivial.py  |fermatfactor_improved_prime.py|\n| Pollard's rho algorithm \t\t|Pollards_rho_trivial.py  |Pollards_rho_improved_prime.py|\n\n\nAnything else you deem relevant.\n------\n1 I used the trivial prime:\n\twhich judge if a number is a prime.\n\n2 I used another prime test called Miller-Rabin primality test:\n\twhich is much quicker and I have referenced in all my test cases. Prime test is correct.\n\nReference\n------\nhttp://www.bigprimes.net/archive/prime/10/\n\nhttps://rosettacode.org/wiki/Miller–Rabin_primality_test#Python:_Probably_correct_answers\n\nhttps://en.wikipedia.org/wiki/Integer_factorization\n\nhttps://en.wikipedia.org/wiki/Trial_division\n\nhttps://en.wikipedia.org/wiki/Fermat%27s_factorization_method\n\nhttps://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm\n\nhttps://en.wikipedia.org/wiki/Euler%27s_factorization_method\n\nhttp://mathworld.wolfram.com/PollardRhoFactorizationMethod.html\n\nhttp://kadinumberprops.blogspot.ca/2012/11/fermats-factorization-running-time.html\n","slug":"Algorithm","published":1,"updated":"2016-07-21T07:12:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekzd0006im0re5f9zrz1","content":"<h1 id=\"integer-factorization-algorithm\"><a href=\"#integer-factorization-algorithm\" class=\"headerlink\" title=\"integer factorization algorithm\"></a>integer factorization algorithm</h1><h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 Introduction\"></a>1 Introduction</h2><table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">trial_division.py</td>\n<td style=\"text-align:right\">note0</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">euler_trivial.py note2</td>\n<td style=\"text-align:right\">note1</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">fermatfactor_trivial.py</td>\n<td style=\"text-align:right\">fermatfactor_improved_prime.py</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">Pollards_rho_trivial.py</td>\n<td style=\"text-align:right\">Pollards_rho_improved_prime.py</td>\n<td>note3</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>note0 : Trivial Division use the trivial prime, which cannot use Miller-Rabin primality test</li>\n<li>note1 : Euler’s factorization method stops the algorithm<pre><code>when a number cannot found number = a^2 + b^2 = c^2 + d^2. \nMiller-Rabin is only judge the prime not find the factors.\n</code></pre></li>\n<li>note2 : Since I have not find a better way to find a number = a^2 + b^2 = c^2 + d^2. <pre><code>So Euler&apos;s spend on the this function I will list after that \n</code></pre></li>\n<li>note3 : In Pollards_rho Miller-Rabin primality test, inorder to handle some base cases. I use the <pre><code>trial_division to handle some special cases.\n</code></pre></li>\n</ul>\n<p>#Integer factorization:</p>\n<h3 id=\"1-1\"><a href=\"#1-1\" class=\"headerlink\" title=\"1.1\"></a>1.1</h3><p>Trial division: (Baseline)<br>This is a base line in my integer factorization algorithm project. It is using<br>a prime sieve for prime number generation which can judge a number is a prime or<br>continue do factorization.</p>\n<p>Running time: in the worst case it is O(2^(n/2)/((n/2)*ln2)) </p>\n<ul>\n<li>n is base-2 n digit number.</li>\n</ul>\n<h3 id=\"1-2\"><a href=\"#1-2\" class=\"headerlink\" title=\"1.2\"></a>1.2</h3><p>Euler’s factorization method:<br>This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Euler’s factorization is if<br>a number = a^2 + b^2 = c^2 + d^2. There is a quickly way to seperate into 2 factors.</p>\n<p>Running time: It is very slow, worst case is greater than trial division,<br>only quick in some special cases and has potient quick.</p>\n<h3 id=\"1-3\"><a href=\"#1-3\" class=\"headerlink\" title=\"1.3\"></a>1.3</h3><p>Fermat’s factorization method:<br>This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Fermat’s factorization method:<br> is if a number = a^2 - b^2 There is a quickly way to seperate into 2 factors.</p>\n<p>Running time: Worst case is O(N^{1/2})<br>              General case is O(N^{1/3}) time.</p>\n<h3 id=\"1-4\"><a href=\"#1-4\" class=\"headerlink\" title=\"1.4\"></a>1.4</h3><p>Pollard’s rho algorithm:<br>This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Pollard’s rho algorithm is if a find<br>the abs(x^2+1-x) mod N if not 1 then it is a factor of N.</p>\n<p>Running time: General case by the Birthday paradox in O(\\sqrt p)\\ &lt;= O(n^{1/4})<br>              but this is a heuristic claim, and rigorous analysis of the algorithm remains open.</p>\n<h2 id=\"2-A-description-of-what-sort-of-tests-I-have-included\"><a href=\"#2-A-description-of-what-sort-of-tests-I-have-included\" class=\"headerlink\" title=\"2 A description of what sort of tests I have included\"></a>2 A description of what sort of tests I have included</h2><h1 id=\"1-St-2-primes-production-each-prime-gt-million\"><a href=\"#1-St-2-primes-production-each-prime-gt-million\" class=\"headerlink\" title=\"1 St: 2 primes production (each prime &gt; million)\"></a>1 St: 2 primes production (each prime &gt; million)</h1><p>When the prime is very big, test the speed of each methods.</p>\n<p>###1.1<br>The testcases: 15485867*32452867 = 502560782130689</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">31.0308229923</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">32.3473279476+3.276534002</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">2.5645339489</td>\n<td style=\"text-align:right\">3.19916701317</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0414018630981</td>\n<td style=\"text-align:right\">0.0358607769012</td>\n</tr>\n</tbody>\n</table>\n<p>###1.2<br>The testcases: 15487019*15487469 = 239854726664911</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">22.700715065</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">21.358424902+3.0871624554</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">3.99884200096</td>\n<td style=\"text-align:right\">2.206387043</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0165410041809</td>\n<td style=\"text-align:right\">0.0110921859741</td>\n</tr>\n</tbody>\n</table>\n<p>###1.3<br>The testcases: 15490781*67869427 = 1051350430252487</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">53.9460468292</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">47.7404336929+8.194948196</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">8.05504488945</td>\n<td style=\"text-align:right\">9.30907988548</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0999021530151</td>\n<td style=\"text-align:right\">0.0918011665344</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"2-nd-4-primes-production-each-prime-between-1000-9999\"><a href=\"#2-nd-4-primes-production-each-prime-between-1000-9999\" class=\"headerlink\" title=\"2 nd: 4 primes production (each prime between[1000,9999])\"></a>2 nd: 4 primes production (each prime between[1000,9999])</h1><p>When the prime is big and more factors, test the speed of each methods.</p>\n<p>###2.1<br>The testcases: 8147 <em> 8369 </em> 8623 * 7127 = 4190216175859403</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">109.196480989</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">105.0888099666+5.89233399</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">2.5645339489</td>\n<td style=\"text-align:right\">3.19916701317</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0414018630981</td>\n<td style=\"text-align:right\">0.0358607769012</td>\n</tr>\n</tbody>\n</table>\n<p>###2.2<br>The testcases: 1259 <em> 1451 </em> 1613 * 1811 = 5336370322687</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">2.96865296364</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">3.3992729187+0.5768110752</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">5.54617881775</td>\n<td style=\"text-align:right\">3.116948843</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.00448799133301</td>\n<td style=\"text-align:right\">0.00167012214661</td>\n</tr>\n</tbody>\n</table>\n<p>###2.3<br>The testcases: 6277 <em> 5351 </em> 8831 * 9733 = 2886979418455921</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">77.4180650711</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">72.7111520767+13.2122049</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">2.87196207047</td>\n<td style=\"text-align:right\">3.24289989471</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.479516983032</td>\n<td style=\"text-align:right\">0.00454497337341</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"3-rd-over-6-small-prime-product\"><a href=\"#3-rd-over-6-small-prime-product\" class=\"headerlink\" title=\"3 rd: over 6 small prime product\"></a>3 rd: over 6 small prime product</h1><p>When the prime is not that big, but we have more factors for the number which need to be factorization.</p>\n<p>###3.1<br>The testcases: 13 <em> 127 </em> 263 <em> 419 </em> 17 <em> 131 </em> 269 = 108990674873561</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">12.9187300205</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">12.9247641563+2.445067882</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">3.29201412201</td>\n<td style=\"text-align:right\">2.98240017891</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.00494313240051</td>\n<td style=\"text-align:right\">0.00559782981873</td>\n</tr>\n</tbody>\n</table>\n<p>###3.2<br>The testcases: 13 <em> 17 </em> 2 <em> 1123 </em> 1426499 * 5 = 3540328013170</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">4.11624193192</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">4.38335967064+0.452334165</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">3.39200806618</td>\n<td style=\"text-align:right\">2.18938994408</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.00471496582031</td>\n<td style=\"text-align:right\">0.00225687026978</td>\n</tr>\n</tbody>\n</table>\n<p>###3.3<br>The testcases: 547 <em> 701 </em> 29 <em> 149 </em> 5 * 2 = 16568744870</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">3.92766094208</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">2.42819094658+0.031641006</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">3.00680112839</td>\n<td style=\"text-align:right\">4.95704817772</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0637471675873</td>\n<td style=\"text-align:right\">0.000675916671753</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"3-How-to-run-the-code\"><a href=\"#3-How-to-run-the-code\" class=\"headerlink\" title=\"3 How to run the code:\"></a>3 How to run the code:</h2><p>There is a easy way to run by<br><figure class=\"highlight python\"><figcaption><span>runner.py```</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Then following the introduction. </span><br><span class=\"line\"></span><br><span class=\"line\">Firstly, you will see this introduction format.</span><br></pre></td></tr></table></figure></p>\n<p>integer factorization algorithm<br>type the number you want to run the algorithm<br>eg: 1<br>then will run the Trial division<br>1.Trial division<br>2.Euler’s factorization method<br>3.Fermat’s factorization method<br>4.Pollard’s rho algorithm<br>input the number:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Then input the id of factorization method.</span><br><span class=\"line\"></span><br><span class=\"line\">for the 3rd and 4th, you also need to choose the id of the prime generate method.</span><br></pre></td></tr></table></figure></p>\n<p>runner.py<br>  -+—-1 “Trial division”—-+—“type number of testcases”<br>   |                          +—“input the number which needs factorization”<br>   |<br>   |<br>   +—-2 “Euler’s factorization method”—+—“type number of testcases”<br>   |                                       +—“input the number which needs factorization”<br>   |<br>   |<br>   +—-3 “Fermat’s factorization method”—+—“type the prime generate method”<br>   |                                        +—1 “trivial prime”<br>   |                                          +       +—“type number of testcases”<br>   |                                        |    +—“input the number which needs factorization”<br>   |                                        |<br>   |                                        +—2 “Miller-Rabin primality test”<br>   |                                             +—“type number of testcases”<br>   |                                             +—“input the number which needs factorization”<br>   |<br>   |<br>   +—-4 “Pollard’s rho algorithm”—+—“type the prime generate method”<br>                                      +—1 “trivial prime”<br>                                      |        +—“type number of testcases”<br>                                      |     +—“input the number which needs factorization”<br>                                     |<br>                                      +—2 “Miller-Rabin primality test”<br>                                               +—“type number of testcases”<br>                                            +—“input the number which needs factorization”<br>```<br>Or </p>\n<p>You can runner each single file<br>which has been listed:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">trial_division.py</td>\n<td style=\"text-align:right\">note0</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">euler_trivial.py note2</td>\n<td style=\"text-align:right\">note1</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">fermatfactor_trivial.py</td>\n<td style=\"text-align:right\">fermatfactor_improved_prime.py</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">Pollards_rho_trivial.py</td>\n<td style=\"text-align:right\">Pollards_rho_improved_prime.py</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Anything-else-you-deem-relevant\"><a href=\"#Anything-else-you-deem-relevant\" class=\"headerlink\" title=\"Anything else you deem relevant.\"></a>Anything else you deem relevant.</h2><p>1 I used the trivial prime:<br>    which judge if a number is a prime.</p>\n<p>2 I used another prime test called Miller-Rabin primality test:<br>    which is much quicker and I have referenced in all my test cases. Prime test is correct.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"http://www.bigprimes.net/archive/prime/10/\" target=\"_blank\" rel=\"noopener\">http://www.bigprimes.net/archive/prime/10/</a></p>\n<p><a href=\"https://rosettacode.org/wiki/Miller–Rabin_primality_test#Python:_Probably_correct_answers\" target=\"_blank\" rel=\"noopener\">https://rosettacode.org/wiki/Miller–Rabin_primality_test#Python:_Probably_correct_answers</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Integer_factorization\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Integer_factorization</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Trial_division\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Trial_division</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Fermat%27s_factorization_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Fermat%27s_factorization_method</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Euler%27s_factorization_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Euler%27s_factorization_method</a></p>\n<p><a href=\"http://mathworld.wolfram.com/PollardRhoFactorizationMethod.html\" target=\"_blank\" rel=\"noopener\">http://mathworld.wolfram.com/PollardRhoFactorizationMethod.html</a></p>\n<p><a href=\"http://kadinumberprops.blogspot.ca/2012/11/fermats-factorization-running-time.html\" target=\"_blank\" rel=\"noopener\">http://kadinumberprops.blogspot.ca/2012/11/fermats-factorization-running-time.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"integer-factorization-algorithm\"><a href=\"#integer-factorization-algorithm\" class=\"headerlink\" title=\"integer factorization algorithm\"></a>integer factorization algorithm</h1><h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 Introduction\"></a>1 Introduction</h2><table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">trial_division.py</td>\n<td style=\"text-align:right\">note0</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">euler_trivial.py note2</td>\n<td style=\"text-align:right\">note1</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">fermatfactor_trivial.py</td>\n<td style=\"text-align:right\">fermatfactor_improved_prime.py</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">Pollards_rho_trivial.py</td>\n<td style=\"text-align:right\">Pollards_rho_improved_prime.py</td>\n<td>note3</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>note0 : Trivial Division use the trivial prime, which cannot use Miller-Rabin primality test</li>\n<li>note1 : Euler’s factorization method stops the algorithm<pre><code>when a number cannot found number = a^2 + b^2 = c^2 + d^2. \nMiller-Rabin is only judge the prime not find the factors.\n</code></pre></li>\n<li>note2 : Since I have not find a better way to find a number = a^2 + b^2 = c^2 + d^2. <pre><code>So Euler&apos;s spend on the this function I will list after that \n</code></pre></li>\n<li>note3 : In Pollards_rho Miller-Rabin primality test, inorder to handle some base cases. I use the <pre><code>trial_division to handle some special cases.\n</code></pre></li>\n</ul>\n<p>#Integer factorization:</p>\n<h3 id=\"1-1\"><a href=\"#1-1\" class=\"headerlink\" title=\"1.1\"></a>1.1</h3><p>Trial division: (Baseline)<br>This is a base line in my integer factorization algorithm project. It is using<br>a prime sieve for prime number generation which can judge a number is a prime or<br>continue do factorization.</p>\n<p>Running time: in the worst case it is O(2^(n/2)/((n/2)*ln2)) </p>\n<ul>\n<li>n is base-2 n digit number.</li>\n</ul>\n<h3 id=\"1-2\"><a href=\"#1-2\" class=\"headerlink\" title=\"1.2\"></a>1.2</h3><p>Euler’s factorization method:<br>This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Euler’s factorization is if<br>a number = a^2 + b^2 = c^2 + d^2. There is a quickly way to seperate into 2 factors.</p>\n<p>Running time: It is very slow, worst case is greater than trial division,<br>only quick in some special cases and has potient quick.</p>\n<h3 id=\"1-3\"><a href=\"#1-3\" class=\"headerlink\" title=\"1.3\"></a>1.3</h3><p>Fermat’s factorization method:<br>This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Fermat’s factorization method:<br> is if a number = a^2 - b^2 There is a quickly way to seperate into 2 factors.</p>\n<p>Running time: Worst case is O(N^{1/2})<br>              General case is O(N^{1/3}) time.</p>\n<h3 id=\"1-4\"><a href=\"#1-4\" class=\"headerlink\" title=\"1.4\"></a>1.4</h3><p>Pollard’s rho algorithm:<br>This is an implementation factorization algorithm, which solves the following problem. Given an number, then find all prime factors. The base knowledge of Pollard’s rho algorithm is if a find<br>the abs(x^2+1-x) mod N if not 1 then it is a factor of N.</p>\n<p>Running time: General case by the Birthday paradox in O(\\sqrt p)\\ &lt;= O(n^{1/4})<br>              but this is a heuristic claim, and rigorous analysis of the algorithm remains open.</p>\n<h2 id=\"2-A-description-of-what-sort-of-tests-I-have-included\"><a href=\"#2-A-description-of-what-sort-of-tests-I-have-included\" class=\"headerlink\" title=\"2 A description of what sort of tests I have included\"></a>2 A description of what sort of tests I have included</h2><h1 id=\"1-St-2-primes-production-each-prime-gt-million\"><a href=\"#1-St-2-primes-production-each-prime-gt-million\" class=\"headerlink\" title=\"1 St: 2 primes production (each prime &gt; million)\"></a>1 St: 2 primes production (each prime &gt; million)</h1><p>When the prime is very big, test the speed of each methods.</p>\n<p>###1.1<br>The testcases: 15485867*32452867 = 502560782130689</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">31.0308229923</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">32.3473279476+3.276534002</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">2.5645339489</td>\n<td style=\"text-align:right\">3.19916701317</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0414018630981</td>\n<td style=\"text-align:right\">0.0358607769012</td>\n</tr>\n</tbody>\n</table>\n<p>###1.2<br>The testcases: 15487019*15487469 = 239854726664911</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">22.700715065</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">21.358424902+3.0871624554</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">3.99884200096</td>\n<td style=\"text-align:right\">2.206387043</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0165410041809</td>\n<td style=\"text-align:right\">0.0110921859741</td>\n</tr>\n</tbody>\n</table>\n<p>###1.3<br>The testcases: 15490781*67869427 = 1051350430252487</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">53.9460468292</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">47.7404336929+8.194948196</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">8.05504488945</td>\n<td style=\"text-align:right\">9.30907988548</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0999021530151</td>\n<td style=\"text-align:right\">0.0918011665344</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"2-nd-4-primes-production-each-prime-between-1000-9999\"><a href=\"#2-nd-4-primes-production-each-prime-between-1000-9999\" class=\"headerlink\" title=\"2 nd: 4 primes production (each prime between[1000,9999])\"></a>2 nd: 4 primes production (each prime between[1000,9999])</h1><p>When the prime is big and more factors, test the speed of each methods.</p>\n<p>###2.1<br>The testcases: 8147 <em> 8369 </em> 8623 * 7127 = 4190216175859403</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">109.196480989</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">105.0888099666+5.89233399</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">2.5645339489</td>\n<td style=\"text-align:right\">3.19916701317</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0414018630981</td>\n<td style=\"text-align:right\">0.0358607769012</td>\n</tr>\n</tbody>\n</table>\n<p>###2.2<br>The testcases: 1259 <em> 1451 </em> 1613 * 1811 = 5336370322687</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">2.96865296364</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">3.3992729187+0.5768110752</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">5.54617881775</td>\n<td style=\"text-align:right\">3.116948843</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.00448799133301</td>\n<td style=\"text-align:right\">0.00167012214661</td>\n</tr>\n</tbody>\n</table>\n<p>###2.3<br>The testcases: 6277 <em> 5351 </em> 8831 * 9733 = 2886979418455921</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">77.4180650711</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">72.7111520767+13.2122049</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">2.87196207047</td>\n<td style=\"text-align:right\">3.24289989471</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.479516983032</td>\n<td style=\"text-align:right\">0.00454497337341</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"3-rd-over-6-small-prime-product\"><a href=\"#3-rd-over-6-small-prime-product\" class=\"headerlink\" title=\"3 rd: over 6 small prime product\"></a>3 rd: over 6 small prime product</h1><p>When the prime is not that big, but we have more factors for the number which need to be factorization.</p>\n<p>###3.1<br>The testcases: 13 <em> 127 </em> 263 <em> 419 </em> 17 <em> 131 </em> 269 = 108990674873561</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">12.9187300205</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">12.9247641563+2.445067882</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">3.29201412201</td>\n<td style=\"text-align:right\">2.98240017891</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.00494313240051</td>\n<td style=\"text-align:right\">0.00559782981873</td>\n</tr>\n</tbody>\n</table>\n<p>###3.2<br>The testcases: 13 <em> 17 </em> 2 <em> 1123 </em> 1426499 * 5 = 3540328013170</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">4.11624193192</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">4.38335967064+0.452334165</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">3.39200806618</td>\n<td style=\"text-align:right\">2.18938994408</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.00471496582031</td>\n<td style=\"text-align:right\">0.00225687026978</td>\n</tr>\n</tbody>\n</table>\n<p>###3.3<br>The testcases: 547 <em> 701 </em> 29 <em> 149 </em> 5 * 2 = 16568744870</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">3.92766094208</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">2.42819094658+0.031641006</td>\n<td style=\"text-align:right\">null</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">3.00680112839</td>\n<td style=\"text-align:right\">4.95704817772</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">0.0637471675873</td>\n<td style=\"text-align:right\">0.000675916671753</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"3-How-to-run-the-code\"><a href=\"#3-How-to-run-the-code\" class=\"headerlink\" title=\"3 How to run the code:\"></a>3 How to run the code:</h2><p>There is a easy way to run by<br><figure class=\"highlight python\"><figcaption><span>runner.py```</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Then following the introduction. </span><br><span class=\"line\"></span><br><span class=\"line\">Firstly, you will see this introduction format.</span><br></pre></td></tr></table></figure></p>\n<p>integer factorization algorithm<br>type the number you want to run the algorithm<br>eg: 1<br>then will run the Trial division<br>1.Trial division<br>2.Euler’s factorization method<br>3.Fermat’s factorization method<br>4.Pollard’s rho algorithm<br>input the number:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Then input the id of factorization method.</span><br><span class=\"line\"></span><br><span class=\"line\">for the 3rd and 4th, you also need to choose the id of the prime generate method.</span><br></pre></td></tr></table></figure></p>\n<p>runner.py<br>  -+—-1 “Trial division”—-+—“type number of testcases”<br>   |                          +—“input the number which needs factorization”<br>   |<br>   |<br>   +—-2 “Euler’s factorization method”—+—“type number of testcases”<br>   |                                       +—“input the number which needs factorization”<br>   |<br>   |<br>   +—-3 “Fermat’s factorization method”—+—“type the prime generate method”<br>   |                                        +—1 “trivial prime”<br>   |                                          +       +—“type number of testcases”<br>   |                                        |    +—“input the number which needs factorization”<br>   |                                        |<br>   |                                        +—2 “Miller-Rabin primality test”<br>   |                                             +—“type number of testcases”<br>   |                                             +—“input the number which needs factorization”<br>   |<br>   |<br>   +—-4 “Pollard’s rho algorithm”—+—“type the prime generate method”<br>                                      +—1 “trivial prime”<br>                                      |        +—“type number of testcases”<br>                                      |     +—“input the number which needs factorization”<br>                                     |<br>                                      +—2 “Miller-Rabin primality test”<br>                                               +—“type number of testcases”<br>                                            +—“input the number which needs factorization”<br>```<br>Or </p>\n<p>You can runner each single file<br>which has been listed:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th style=\"text-align:center\">trivial prime</th>\n<th style=\"text-align:right\">Miller-Rabin primality test</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trial division(baseline)</td>\n<td style=\"text-align:center\">trial_division.py</td>\n<td style=\"text-align:right\">note0</td>\n</tr>\n<tr>\n<td>Euler’s factorization method</td>\n<td style=\"text-align:center\">euler_trivial.py note2</td>\n<td style=\"text-align:right\">note1</td>\n</tr>\n<tr>\n<td>Fermat’s factorization method</td>\n<td style=\"text-align:center\">fermatfactor_trivial.py</td>\n<td style=\"text-align:right\">fermatfactor_improved_prime.py</td>\n</tr>\n<tr>\n<td>Pollard’s rho algorithm</td>\n<td style=\"text-align:center\">Pollards_rho_trivial.py</td>\n<td style=\"text-align:right\">Pollards_rho_improved_prime.py</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Anything-else-you-deem-relevant\"><a href=\"#Anything-else-you-deem-relevant\" class=\"headerlink\" title=\"Anything else you deem relevant.\"></a>Anything else you deem relevant.</h2><p>1 I used the trivial prime:<br>    which judge if a number is a prime.</p>\n<p>2 I used another prime test called Miller-Rabin primality test:<br>    which is much quicker and I have referenced in all my test cases. Prime test is correct.</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"http://www.bigprimes.net/archive/prime/10/\" target=\"_blank\" rel=\"noopener\">http://www.bigprimes.net/archive/prime/10/</a></p>\n<p><a href=\"https://rosettacode.org/wiki/Miller–Rabin_primality_test#Python:_Probably_correct_answers\" target=\"_blank\" rel=\"noopener\">https://rosettacode.org/wiki/Miller–Rabin_primality_test#Python:_Probably_correct_answers</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Integer_factorization\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Integer_factorization</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Trial_division\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Trial_division</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Fermat%27s_factorization_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Fermat%27s_factorization_method</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm</a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Euler%27s_factorization_method\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Euler%27s_factorization_method</a></p>\n<p><a href=\"http://mathworld.wolfram.com/PollardRhoFactorizationMethod.html\" target=\"_blank\" rel=\"noopener\">http://mathworld.wolfram.com/PollardRhoFactorizationMethod.html</a></p>\n<p><a href=\"http://kadinumberprops.blogspot.ca/2012/11/fermats-factorization-running-time.html\" target=\"_blank\" rel=\"noopener\">http://kadinumberprops.blogspot.ca/2012/11/fermats-factorization-running-time.html</a></p>\n"},{"title":"Machine Learning Notes - 1","date":"2018-05-20T22:15:11.000Z","_content":"\nReference: [https://zhuanlan.zhihu.com/p/36287950](https://zhuanlan.zhihu.com/p/36287950)\n\n# Todo List\n- [X] - 01 - Overview\nP1-15 Machine learning background and flow\n\nP16-42  Machine learning flow\n\n- [ ] - 02 - Business Understanding \n\n- [X] - 03 - Data Understanding p56 - 62\n\n To learn about data analysis, it is right that each of us try many things that do not work – that we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed an opportunity to learn more\n\n- [ ] - 04 - Data Preparation -p63\n\nPrepare data is time consuming \n\nseveral methods to deal with missing data and outliers\n\nnormalize Data\n\nData binning 分级\n\nReduce Data, Clean Data\n\nFeature Engineering: 从raw data 提取出 feature\n\n### Feature Selection: Filter/Wrapper/Embedded method\n\nTraditional approaches 传统方法\n\n#### Forward selection \n\n一开始模型里面没有variable， 然后往里面加入variable，直到accuracy 没有任何的增长\n\n#### Backward elimination\n\n和前一种相反，一开始有所有的variable, 然后去除，直到accuracy 有明显的下降\n\n#### Stepwise regression\n\n这种是用在选k-best feature， 一开始有k个，然后加入更好的，并且去除最差的，直到经历过所有的feature\n\t\nP100","source":"_posts/Machine-Learning-Notes-1.md","raw":"---\ntitle: Machine Learning Notes - 1\ndate: 2018-05-20 16:15:11\ntags: Machine_Learning\n---\n\nReference: [https://zhuanlan.zhihu.com/p/36287950](https://zhuanlan.zhihu.com/p/36287950)\n\n# Todo List\n- [X] - 01 - Overview\nP1-15 Machine learning background and flow\n\nP16-42  Machine learning flow\n\n- [ ] - 02 - Business Understanding \n\n- [X] - 03 - Data Understanding p56 - 62\n\n To learn about data analysis, it is right that each of us try many things that do not work – that we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed an opportunity to learn more\n\n- [ ] - 04 - Data Preparation -p63\n\nPrepare data is time consuming \n\nseveral methods to deal with missing data and outliers\n\nnormalize Data\n\nData binning 分级\n\nReduce Data, Clean Data\n\nFeature Engineering: 从raw data 提取出 feature\n\n### Feature Selection: Filter/Wrapper/Embedded method\n\nTraditional approaches 传统方法\n\n#### Forward selection \n\n一开始模型里面没有variable， 然后往里面加入variable，直到accuracy 没有任何的增长\n\n#### Backward elimination\n\n和前一种相反，一开始有所有的variable, 然后去除，直到accuracy 有明显的下降\n\n#### Stepwise regression\n\n这种是用在选k-best feature， 一开始有k个，然后加入更好的，并且去除最差的，直到经历过所有的feature\n\t\nP100","slug":"Machine-Learning-Notes-1","published":1,"updated":"2018-05-24T07:07:22.969Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekzh0008im0rqmsb2yqj","content":"<p>Reference: <a href=\"https://zhuanlan.zhihu.com/p/36287950\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/36287950</a></p>\n<h1 id=\"Todo-List\"><a href=\"#Todo-List\" class=\"headerlink\" title=\"Todo List\"></a>Todo List</h1><ul>\n<li>[X] - 01 - Overview<br>P1-15 Machine learning background and flow</li>\n</ul>\n<p>P16-42  Machine learning flow</p>\n<ul>\n<li><p>[ ] - 02 - Business Understanding </p>\n</li>\n<li><p>[X] - 03 - Data Understanding p56 - 62</p>\n<p>To learn about data analysis, it is right that each of us try many things that do not work – that we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed an opportunity to learn more</p>\n</li>\n<li><p>[ ] - 04 - Data Preparation -p63</p>\n</li>\n</ul>\n<p>Prepare data is time consuming </p>\n<p>several methods to deal with missing data and outliers</p>\n<p>normalize Data</p>\n<p>Data binning 分级</p>\n<p>Reduce Data, Clean Data</p>\n<p>Feature Engineering: 从raw data 提取出 feature</p>\n<h3 id=\"Feature-Selection-Filter-Wrapper-Embedded-method\"><a href=\"#Feature-Selection-Filter-Wrapper-Embedded-method\" class=\"headerlink\" title=\"Feature Selection: Filter/Wrapper/Embedded method\"></a>Feature Selection: Filter/Wrapper/Embedded method</h3><p>Traditional approaches 传统方法</p>\n<h4 id=\"Forward-selection\"><a href=\"#Forward-selection\" class=\"headerlink\" title=\"Forward selection\"></a>Forward selection</h4><p>一开始模型里面没有variable， 然后往里面加入variable，直到accuracy 没有任何的增长</p>\n<h4 id=\"Backward-elimination\"><a href=\"#Backward-elimination\" class=\"headerlink\" title=\"Backward elimination\"></a>Backward elimination</h4><p>和前一种相反，一开始有所有的variable, 然后去除，直到accuracy 有明显的下降</p>\n<h4 id=\"Stepwise-regression\"><a href=\"#Stepwise-regression\" class=\"headerlink\" title=\"Stepwise regression\"></a>Stepwise regression</h4><p>这种是用在选k-best feature， 一开始有k个，然后加入更好的，并且去除最差的，直到经历过所有的feature</p>\n<p>P100</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Reference: <a href=\"https://zhuanlan.zhihu.com/p/36287950\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/36287950</a></p>\n<h1 id=\"Todo-List\"><a href=\"#Todo-List\" class=\"headerlink\" title=\"Todo List\"></a>Todo List</h1><ul>\n<li>[X] - 01 - Overview<br>P1-15 Machine learning background and flow</li>\n</ul>\n<p>P16-42  Machine learning flow</p>\n<ul>\n<li><p>[ ] - 02 - Business Understanding </p>\n</li>\n<li><p>[X] - 03 - Data Understanding p56 - 62</p>\n<p>To learn about data analysis, it is right that each of us try many things that do not work – that we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed an opportunity to learn more</p>\n</li>\n<li><p>[ ] - 04 - Data Preparation -p63</p>\n</li>\n</ul>\n<p>Prepare data is time consuming </p>\n<p>several methods to deal with missing data and outliers</p>\n<p>normalize Data</p>\n<p>Data binning 分级</p>\n<p>Reduce Data, Clean Data</p>\n<p>Feature Engineering: 从raw data 提取出 feature</p>\n<h3 id=\"Feature-Selection-Filter-Wrapper-Embedded-method\"><a href=\"#Feature-Selection-Filter-Wrapper-Embedded-method\" class=\"headerlink\" title=\"Feature Selection: Filter/Wrapper/Embedded method\"></a>Feature Selection: Filter/Wrapper/Embedded method</h3><p>Traditional approaches 传统方法</p>\n<h4 id=\"Forward-selection\"><a href=\"#Forward-selection\" class=\"headerlink\" title=\"Forward selection\"></a>Forward selection</h4><p>一开始模型里面没有variable， 然后往里面加入variable，直到accuracy 没有任何的增长</p>\n<h4 id=\"Backward-elimination\"><a href=\"#Backward-elimination\" class=\"headerlink\" title=\"Backward elimination\"></a>Backward elimination</h4><p>和前一种相反，一开始有所有的variable, 然后去除，直到accuracy 有明显的下降</p>\n<h4 id=\"Stepwise-regression\"><a href=\"#Stepwise-regression\" class=\"headerlink\" title=\"Stepwise regression\"></a>Stepwise regression</h4><p>这种是用在选k-best feature， 一开始有k个，然后加入更好的，并且去除最差的，直到经历过所有的feature</p>\n<p>P100</p>\n"},{"title":"My Ionic-Hybrid Experience[1]","date":"2016-07-27T19:26:54.000Z","_content":"### Ionic Development experience\n\nWhen I was in 3rd year in Undergraduate, I has an opportunity to develop a Mobile Application. Since I have the experience of developing Android Mobile Application which is a CMPUT 301 course project. I build a team which from 2 different teams in 301 course.\n\nThe First Plan for us is build an Android Application by Java and an iOS Application by Object-C. The Customer asked us to finish in a half year. We don't think that is enough to develop two Applications and test two.\n\nWe were looking for some solutions. The first solutions is Cordova Phonegap. However we searched some Phonegap products. The User experience and User Interface is much worse than an Application developed by Java or Object-C.\n\nThen one of my team member ask his sister's boyfriend who works in Google. He recommand the Ionic + Parse for our mobile application. That is the first time I heard about the Ionic.\n\nThen I use Ionic + Parse to develop 3 Mobile Applications inculde one course project. It is not easy until we fully understand what the structure in the Ionic. Trust me! Ionic is a very clear Structure.\n\n### The Structure\n\nIn my Understanding the Ionic Stucture is like this form.\n\n\n![](/images/ionic-Structure.png)\n\nSee you in next time.","source":"_posts/My Ionic-Hybrid Experience[1].md","raw":"---\ntitle: My Ionic-Hybrid Experience[1]\ndate: 2016-07-27 13:26:54\ntags: Web, Mobile\n---\n### Ionic Development experience\n\nWhen I was in 3rd year in Undergraduate, I has an opportunity to develop a Mobile Application. Since I have the experience of developing Android Mobile Application which is a CMPUT 301 course project. I build a team which from 2 different teams in 301 course.\n\nThe First Plan for us is build an Android Application by Java and an iOS Application by Object-C. The Customer asked us to finish in a half year. We don't think that is enough to develop two Applications and test two.\n\nWe were looking for some solutions. The first solutions is Cordova Phonegap. However we searched some Phonegap products. The User experience and User Interface is much worse than an Application developed by Java or Object-C.\n\nThen one of my team member ask his sister's boyfriend who works in Google. He recommand the Ionic + Parse for our mobile application. That is the first time I heard about the Ionic.\n\nThen I use Ionic + Parse to develop 3 Mobile Applications inculde one course project. It is not easy until we fully understand what the structure in the Ionic. Trust me! Ionic is a very clear Structure.\n\n### The Structure\n\nIn my Understanding the Ionic Stucture is like this form.\n\n\n![](/images/ionic-Structure.png)\n\nSee you in next time.","slug":"My Ionic-Hybrid Experience[1]","published":1,"updated":"2016-07-28T19:04:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekzn000aim0rm4b55fm9","content":"<h3 id=\"Ionic-Development-experience\"><a href=\"#Ionic-Development-experience\" class=\"headerlink\" title=\"Ionic Development experience\"></a>Ionic Development experience</h3><p>When I was in 3rd year in Undergraduate, I has an opportunity to develop a Mobile Application. Since I have the experience of developing Android Mobile Application which is a CMPUT 301 course project. I build a team which from 2 different teams in 301 course.</p>\n<p>The First Plan for us is build an Android Application by Java and an iOS Application by Object-C. The Customer asked us to finish in a half year. We don’t think that is enough to develop two Applications and test two.</p>\n<p>We were looking for some solutions. The first solutions is Cordova Phonegap. However we searched some Phonegap products. The User experience and User Interface is much worse than an Application developed by Java or Object-C.</p>\n<p>Then one of my team member ask his sister’s boyfriend who works in Google. He recommand the Ionic + Parse for our mobile application. That is the first time I heard about the Ionic.</p>\n<p>Then I use Ionic + Parse to develop 3 Mobile Applications inculde one course project. It is not easy until we fully understand what the structure in the Ionic. Trust me! Ionic is a very clear Structure.</p>\n<h3 id=\"The-Structure\"><a href=\"#The-Structure\" class=\"headerlink\" title=\"The Structure\"></a>The Structure</h3><p>In my Understanding the Ionic Stucture is like this form.</p>\n<p><img src=\"/images/ionic-Structure.png\" alt=\"\"></p>\n<p>See you in next time.</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Ionic-Development-experience\"><a href=\"#Ionic-Development-experience\" class=\"headerlink\" title=\"Ionic Development experience\"></a>Ionic Development experience</h3><p>When I was in 3rd year in Undergraduate, I has an opportunity to develop a Mobile Application. Since I have the experience of developing Android Mobile Application which is a CMPUT 301 course project. I build a team which from 2 different teams in 301 course.</p>\n<p>The First Plan for us is build an Android Application by Java and an iOS Application by Object-C. The Customer asked us to finish in a half year. We don’t think that is enough to develop two Applications and test two.</p>\n<p>We were looking for some solutions. The first solutions is Cordova Phonegap. However we searched some Phonegap products. The User experience and User Interface is much worse than an Application developed by Java or Object-C.</p>\n<p>Then one of my team member ask his sister’s boyfriend who works in Google. He recommand the Ionic + Parse for our mobile application. That is the first time I heard about the Ionic.</p>\n<p>Then I use Ionic + Parse to develop 3 Mobile Applications inculde one course project. It is not easy until we fully understand what the structure in the Ionic. Trust me! Ionic is a very clear Structure.</p>\n<h3 id=\"The-Structure\"><a href=\"#The-Structure\" class=\"headerlink\" title=\"The Structure\"></a>The Structure</h3><p>In my Understanding the Ionic Stucture is like this form.</p>\n<p><img src=\"/images/ionic-Structure.png\" alt=\"\"></p>\n<p>See you in next time.</p>\n"},{"title":"Install python library","date":"2018-05-25T05:01:30.000Z","_content":"\n# Pyenv\n\n因为之前用[nvm](https://github.com/creationix/nvm) node 的版本管理器\n\n所以pyenv 应该也是python 的版本管理器\n\n运行 curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash\n\n然后打开 vim ~/.bash_profile \n\n添加了三行\n\n\texport PATH=\"~/.pyenv/bin:$PATH\"\n\teval \"$(pyenv init -)\"\n\teval \"$(pyenv virtualenv-init -)\"\n\n我原来的是\n\n\texport XAMPP_HOME=/Applications/xampp/xamppfiles\n\texport PATH=${XAMPP_HOME}/bin:${PATH}\n\n最后变成\n\n\texport XAMPP_HOME=/Applications/xampp/xamppfiles\n\texport PATH=/Users/weijiesun/.pyenv/bin:${PATH}:${XAMPP_HOME}/bin\n\n\teval \"$(pyenv init -)\"\n\teval \"$(pyenv virtualenv-init -)\"\n\n然后有个坑踩了好久，就是XAMPP_HOME=/Applications/xampp/xamppfiles 要放在path 的最后一位，不然就是疯狂在找http url 的端口\n\n然后运行 \n\n\tsource ~/.bash_profile \t\n\n就好了\n\nCheating sheet：[reference](https://github.com/eteplus/blog/issues/4)\n\n- 安装和查看pyenv 和 python\n\n\t#查看pyenv版本\n\tpyenv --version\n\n\t#查看可用的python version\n\tpyenv versions\n\n\t#用pyenv 安装python \n\tpyenv install 3.6.3\n\n- 设置 python version\n\n\t# 对所有的Shell全局有效，会把版本号写入到~/.pyenv/version文件中\n\tpyenv global 3.6.3\n\n\t# 只对当前目录有效，会在当前目录创建.python-version文件\n\tpyenv local 3.6.3\n\n\t# 只在当前会话有效\n\tpyenv shell 3.6.3\n\n我在安装的时候遇到了一下描述的问题\n\n\tLast 10 log lines:   File \"/private/var/folders/py/3006ng_x6x3g5c42jx_gznlc0000gn/T/python-build.20180618223416.3806/Python-3.6.3/Lib/ensurepip/__main__.py\", line 4, in <module>     ensurepip._main()\n\n[解决方法](http://www.cnblogs.com/mingaixin/p/6295799.html)\n\n\n# Kears\n\n\t# GPU 版本\n\t>>> pip install --upgrade tensorflow-gpu\n\n\t# CPU 版本\n\t>>> pip install --upgrade tensorflow\n\n\t# Keras 安装\n\t>>> pip install keras -U --pre\n\n\timport keras\n\t#然后就显示tensorflow work on backend\n\n# pyecharts\n\n[github pyecharts](https://github.com/pyecharts/pyecharts)\n\n```\n\t$ pip install pyecharts\n\n\n\t$ pip install echarts-countries-pypkg\n\t$ pip install echarts-china-provinces-pypkg\n\t$ pip install echarts-china-cities-pypkg\n\t$ pip install echarts-china-counties-pypkg\n\t$ pip install echarts-china-misc-pypkg\n```\n$ pip install echarts-united-kingdom-pypkg\n","source":"_posts/Install-python-library.md","raw":"---\ntitle: Install python library\ndate: 2018-05-24 23:01:30\ntags: python\n---\n\n# Pyenv\n\n因为之前用[nvm](https://github.com/creationix/nvm) node 的版本管理器\n\n所以pyenv 应该也是python 的版本管理器\n\n运行 curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash\n\n然后打开 vim ~/.bash_profile \n\n添加了三行\n\n\texport PATH=\"~/.pyenv/bin:$PATH\"\n\teval \"$(pyenv init -)\"\n\teval \"$(pyenv virtualenv-init -)\"\n\n我原来的是\n\n\texport XAMPP_HOME=/Applications/xampp/xamppfiles\n\texport PATH=${XAMPP_HOME}/bin:${PATH}\n\n最后变成\n\n\texport XAMPP_HOME=/Applications/xampp/xamppfiles\n\texport PATH=/Users/weijiesun/.pyenv/bin:${PATH}:${XAMPP_HOME}/bin\n\n\teval \"$(pyenv init -)\"\n\teval \"$(pyenv virtualenv-init -)\"\n\n然后有个坑踩了好久，就是XAMPP_HOME=/Applications/xampp/xamppfiles 要放在path 的最后一位，不然就是疯狂在找http url 的端口\n\n然后运行 \n\n\tsource ~/.bash_profile \t\n\n就好了\n\nCheating sheet：[reference](https://github.com/eteplus/blog/issues/4)\n\n- 安装和查看pyenv 和 python\n\n\t#查看pyenv版本\n\tpyenv --version\n\n\t#查看可用的python version\n\tpyenv versions\n\n\t#用pyenv 安装python \n\tpyenv install 3.6.3\n\n- 设置 python version\n\n\t# 对所有的Shell全局有效，会把版本号写入到~/.pyenv/version文件中\n\tpyenv global 3.6.3\n\n\t# 只对当前目录有效，会在当前目录创建.python-version文件\n\tpyenv local 3.6.3\n\n\t# 只在当前会话有效\n\tpyenv shell 3.6.3\n\n我在安装的时候遇到了一下描述的问题\n\n\tLast 10 log lines:   File \"/private/var/folders/py/3006ng_x6x3g5c42jx_gznlc0000gn/T/python-build.20180618223416.3806/Python-3.6.3/Lib/ensurepip/__main__.py\", line 4, in <module>     ensurepip._main()\n\n[解决方法](http://www.cnblogs.com/mingaixin/p/6295799.html)\n\n\n# Kears\n\n\t# GPU 版本\n\t>>> pip install --upgrade tensorflow-gpu\n\n\t# CPU 版本\n\t>>> pip install --upgrade tensorflow\n\n\t# Keras 安装\n\t>>> pip install keras -U --pre\n\n\timport keras\n\t#然后就显示tensorflow work on backend\n\n# pyecharts\n\n[github pyecharts](https://github.com/pyecharts/pyecharts)\n\n```\n\t$ pip install pyecharts\n\n\n\t$ pip install echarts-countries-pypkg\n\t$ pip install echarts-china-provinces-pypkg\n\t$ pip install echarts-china-cities-pypkg\n\t$ pip install echarts-china-counties-pypkg\n\t$ pip install echarts-china-misc-pypkg\n```\n$ pip install echarts-united-kingdom-pypkg\n","slug":"Install-python-library","published":1,"updated":"2018-07-29T22:34:14.302Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekzr000cim0ricobeir6","content":"<h1 id=\"Pyenv\"><a href=\"#Pyenv\" class=\"headerlink\" title=\"Pyenv\"></a>Pyenv</h1><p>因为之前用<a href=\"https://github.com/creationix/nvm\" target=\"_blank\" rel=\"noopener\">nvm</a> node 的版本管理器</p>\n<p>所以pyenv 应该也是python 的版本管理器</p>\n<p>运行 curl -L <a href=\"https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer\" target=\"_blank\" rel=\"noopener\">https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer</a> | bash</p>\n<p>然后打开 vim ~/.bash_profile </p>\n<p>添加了三行</p>\n<pre><code>export PATH=&quot;~/.pyenv/bin:$PATH&quot;\neval &quot;$(pyenv init -)&quot;\neval &quot;$(pyenv virtualenv-init -)&quot;\n</code></pre><p>我原来的是</p>\n<pre><code>export XAMPP_HOME=/Applications/xampp/xamppfiles\nexport PATH=${XAMPP_HOME}/bin:${PATH}\n</code></pre><p>最后变成</p>\n<pre><code>export XAMPP_HOME=/Applications/xampp/xamppfiles\nexport PATH=/Users/weijiesun/.pyenv/bin:${PATH}:${XAMPP_HOME}/bin\n\neval &quot;$(pyenv init -)&quot;\neval &quot;$(pyenv virtualenv-init -)&quot;\n</code></pre><p>然后有个坑踩了好久，就是XAMPP_HOME=/Applications/xampp/xamppfiles 要放在path 的最后一位，不然就是疯狂在找http url 的端口</p>\n<p>然后运行 </p>\n<pre><code>source ~/.bash_profile     \n</code></pre><p>就好了</p>\n<p>Cheating sheet：<a href=\"https://github.com/eteplus/blog/issues/4\" target=\"_blank\" rel=\"noopener\">reference</a></p>\n<ul>\n<li><p>安装和查看pyenv 和 python</p>\n<p>  #查看pyenv版本<br>  pyenv –version</p>\n<p>  #查看可用的python version<br>  pyenv versions</p>\n<p>  #用pyenv 安装python<br>  pyenv install 3.6.3</p>\n</li>\n<li><p>设置 python version</p>\n<h1 id=\"对所有的Shell全局有效，会把版本号写入到-pyenv-version文件中\"><a href=\"#对所有的Shell全局有效，会把版本号写入到-pyenv-version文件中\" class=\"headerlink\" title=\"对所有的Shell全局有效，会把版本号写入到~/.pyenv/version文件中\"></a>对所有的Shell全局有效，会把版本号写入到~/.pyenv/version文件中</h1><p>  pyenv global 3.6.3</p>\n<h1 id=\"只对当前目录有效，会在当前目录创建-python-version文件\"><a href=\"#只对当前目录有效，会在当前目录创建-python-version文件\" class=\"headerlink\" title=\"只对当前目录有效，会在当前目录创建.python-version文件\"></a>只对当前目录有效，会在当前目录创建.python-version文件</h1><p>  pyenv local 3.6.3</p>\n<h1 id=\"只在当前会话有效\"><a href=\"#只在当前会话有效\" class=\"headerlink\" title=\"只在当前会话有效\"></a>只在当前会话有效</h1><p>  pyenv shell 3.6.3</p>\n</li>\n</ul>\n<p>我在安装的时候遇到了一下描述的问题</p>\n<pre><code>Last 10 log lines:   File &quot;/private/var/folders/py/3006ng_x6x3g5c42jx_gznlc0000gn/T/python-build.20180618223416.3806/Python-3.6.3/Lib/ensurepip/__main__.py&quot;, line 4, in &lt;module&gt;     ensurepip._main()\n</code></pre><p><a href=\"http://www.cnblogs.com/mingaixin/p/6295799.html\" target=\"_blank\" rel=\"noopener\">解决方法</a></p>\n<h1 id=\"Kears\"><a href=\"#Kears\" class=\"headerlink\" title=\"Kears\"></a>Kears</h1><pre><code># GPU 版本\n&gt;&gt;&gt; pip install --upgrade tensorflow-gpu\n\n# CPU 版本\n&gt;&gt;&gt; pip install --upgrade tensorflow\n\n# Keras 安装\n&gt;&gt;&gt; pip install keras -U --pre\n\nimport keras\n#然后就显示tensorflow work on backend\n</code></pre><h1 id=\"pyecharts\"><a href=\"#pyecharts\" class=\"headerlink\" title=\"pyecharts\"></a>pyecharts</h1><p><a href=\"https://github.com/pyecharts/pyecharts\" target=\"_blank\" rel=\"noopener\">github pyecharts</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ pip install pyecharts</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ pip install echarts-countries-pypkg</span><br><span class=\"line\">$ pip install echarts-china-provinces-pypkg</span><br><span class=\"line\">$ pip install echarts-china-cities-pypkg</span><br><span class=\"line\">$ pip install echarts-china-counties-pypkg</span><br><span class=\"line\">$ pip install echarts-china-misc-pypkg</span><br></pre></td></tr></table></figure>\n<p>$ pip install echarts-united-kingdom-pypkg</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Pyenv\"><a href=\"#Pyenv\" class=\"headerlink\" title=\"Pyenv\"></a>Pyenv</h1><p>因为之前用<a href=\"https://github.com/creationix/nvm\" target=\"_blank\" rel=\"noopener\">nvm</a> node 的版本管理器</p>\n<p>所以pyenv 应该也是python 的版本管理器</p>\n<p>运行 curl -L <a href=\"https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer\" target=\"_blank\" rel=\"noopener\">https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer</a> | bash</p>\n<p>然后打开 vim ~/.bash_profile </p>\n<p>添加了三行</p>\n<pre><code>export PATH=&quot;~/.pyenv/bin:$PATH&quot;\neval &quot;$(pyenv init -)&quot;\neval &quot;$(pyenv virtualenv-init -)&quot;\n</code></pre><p>我原来的是</p>\n<pre><code>export XAMPP_HOME=/Applications/xampp/xamppfiles\nexport PATH=${XAMPP_HOME}/bin:${PATH}\n</code></pre><p>最后变成</p>\n<pre><code>export XAMPP_HOME=/Applications/xampp/xamppfiles\nexport PATH=/Users/weijiesun/.pyenv/bin:${PATH}:${XAMPP_HOME}/bin\n\neval &quot;$(pyenv init -)&quot;\neval &quot;$(pyenv virtualenv-init -)&quot;\n</code></pre><p>然后有个坑踩了好久，就是XAMPP_HOME=/Applications/xampp/xamppfiles 要放在path 的最后一位，不然就是疯狂在找http url 的端口</p>\n<p>然后运行 </p>\n<pre><code>source ~/.bash_profile     \n</code></pre><p>就好了</p>\n<p>Cheating sheet：<a href=\"https://github.com/eteplus/blog/issues/4\" target=\"_blank\" rel=\"noopener\">reference</a></p>\n<ul>\n<li><p>安装和查看pyenv 和 python</p>\n<p>  #查看pyenv版本<br>  pyenv –version</p>\n<p>  #查看可用的python version<br>  pyenv versions</p>\n<p>  #用pyenv 安装python<br>  pyenv install 3.6.3</p>\n</li>\n<li><p>设置 python version</p>\n<h1 id=\"对所有的Shell全局有效，会把版本号写入到-pyenv-version文件中\"><a href=\"#对所有的Shell全局有效，会把版本号写入到-pyenv-version文件中\" class=\"headerlink\" title=\"对所有的Shell全局有效，会把版本号写入到~/.pyenv/version文件中\"></a>对所有的Shell全局有效，会把版本号写入到~/.pyenv/version文件中</h1><p>  pyenv global 3.6.3</p>\n<h1 id=\"只对当前目录有效，会在当前目录创建-python-version文件\"><a href=\"#只对当前目录有效，会在当前目录创建-python-version文件\" class=\"headerlink\" title=\"只对当前目录有效，会在当前目录创建.python-version文件\"></a>只对当前目录有效，会在当前目录创建.python-version文件</h1><p>  pyenv local 3.6.3</p>\n<h1 id=\"只在当前会话有效\"><a href=\"#只在当前会话有效\" class=\"headerlink\" title=\"只在当前会话有效\"></a>只在当前会话有效</h1><p>  pyenv shell 3.6.3</p>\n</li>\n</ul>\n<p>我在安装的时候遇到了一下描述的问题</p>\n<pre><code>Last 10 log lines:   File &quot;/private/var/folders/py/3006ng_x6x3g5c42jx_gznlc0000gn/T/python-build.20180618223416.3806/Python-3.6.3/Lib/ensurepip/__main__.py&quot;, line 4, in &lt;module&gt;     ensurepip._main()\n</code></pre><p><a href=\"http://www.cnblogs.com/mingaixin/p/6295799.html\" target=\"_blank\" rel=\"noopener\">解决方法</a></p>\n<h1 id=\"Kears\"><a href=\"#Kears\" class=\"headerlink\" title=\"Kears\"></a>Kears</h1><pre><code># GPU 版本\n&gt;&gt;&gt; pip install --upgrade tensorflow-gpu\n\n# CPU 版本\n&gt;&gt;&gt; pip install --upgrade tensorflow\n\n# Keras 安装\n&gt;&gt;&gt; pip install keras -U --pre\n\nimport keras\n#然后就显示tensorflow work on backend\n</code></pre><h1 id=\"pyecharts\"><a href=\"#pyecharts\" class=\"headerlink\" title=\"pyecharts\"></a>pyecharts</h1><p><a href=\"https://github.com/pyecharts/pyecharts\" target=\"_blank\" rel=\"noopener\">github pyecharts</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ pip install pyecharts</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ pip install echarts-countries-pypkg</span><br><span class=\"line\">$ pip install echarts-china-provinces-pypkg</span><br><span class=\"line\">$ pip install echarts-china-cities-pypkg</span><br><span class=\"line\">$ pip install echarts-china-counties-pypkg</span><br><span class=\"line\">$ pip install echarts-china-misc-pypkg</span><br></pre></td></tr></table></figure>\n<p>$ pip install echarts-united-kingdom-pypkg</p>\n"},{"title":"Machine Learning Project[1]-Weijie Sun","date":"2016-08-02T21:37:01.000Z","_content":"## Introduction\n\nI have taken a Machine Learning Course at the University of Alberta. Our professor is [Russell Greiner](https://en.wikipedia.org/wiki/Russell_Greiner). We need to make a project and do some researches by Machine Learning method with our coach [Koosha Golmohammadi](https://sites.ualberta.ca/~golmoham/). Since this is my first Machine Learning project. Soooo, this is not as good as some projects made by Machine Learning experts. However, we achieve a result is 90% accuracy which is closed to some the U.S agency in Boston.\n\n# 1 Scraping Data.\n\nWe scraped some real estate data from real estate data website from Edmonton and marked in Edmonton Map.\n\n![](/images/MachineLearning-data2.png)\n\n# 2 Matching Data\n\nThen we tried to match the real estate data with over 110 features in [Open City Edmonton Data](https://data.edmonton.ca/)\n\nIt is a tough work there are different names for an address which is easier for the human to understand than a machine. After a little of the algorithm in nature language process(very basic). \n\nwe got the following useful data.\n![](/images/MachineLearning-data1.png)\n\nThis is our first step for collating data and scraping data.\n\nI will keep updating for Machine learning. \nSee you next time.\n\n[Our code resource](https://github.com/koosha/property_values)","source":"_posts/Machine-Learning-Project-1-Weijie-Sun.md","raw":"---\ntitle: 'Machine Learning Project[1]-Weijie Sun'\ndate: 2016-08-02 15:37:01\ntags: \n\t- Algorithm\n\t- Machine-Learning\n---\n## Introduction\n\nI have taken a Machine Learning Course at the University of Alberta. Our professor is [Russell Greiner](https://en.wikipedia.org/wiki/Russell_Greiner). We need to make a project and do some researches by Machine Learning method with our coach [Koosha Golmohammadi](https://sites.ualberta.ca/~golmoham/). Since this is my first Machine Learning project. Soooo, this is not as good as some projects made by Machine Learning experts. However, we achieve a result is 90% accuracy which is closed to some the U.S agency in Boston.\n\n# 1 Scraping Data.\n\nWe scraped some real estate data from real estate data website from Edmonton and marked in Edmonton Map.\n\n![](/images/MachineLearning-data2.png)\n\n# 2 Matching Data\n\nThen we tried to match the real estate data with over 110 features in [Open City Edmonton Data](https://data.edmonton.ca/)\n\nIt is a tough work there are different names for an address which is easier for the human to understand than a machine. After a little of the algorithm in nature language process(very basic). \n\nwe got the following useful data.\n![](/images/MachineLearning-data1.png)\n\nThis is our first step for collating data and scraping data.\n\nI will keep updating for Machine learning. \nSee you next time.\n\n[Our code resource](https://github.com/koosha/property_values)","slug":"Machine-Learning-Project-1-Weijie-Sun","published":1,"updated":"2016-08-02T22:14:45.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekzu000eim0r28san2f3","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>I have taken a Machine Learning Course at the University of Alberta. Our professor is <a href=\"https://en.wikipedia.org/wiki/Russell_Greiner\" target=\"_blank\" rel=\"noopener\">Russell Greiner</a>. We need to make a project and do some researches by Machine Learning method with our coach <a href=\"https://sites.ualberta.ca/~golmoham/\" target=\"_blank\" rel=\"noopener\">Koosha Golmohammadi</a>. Since this is my first Machine Learning project. Soooo, this is not as good as some projects made by Machine Learning experts. However, we achieve a result is 90% accuracy which is closed to some the U.S agency in Boston.</p>\n<h1 id=\"1-Scraping-Data\"><a href=\"#1-Scraping-Data\" class=\"headerlink\" title=\"1 Scraping Data.\"></a>1 Scraping Data.</h1><p>We scraped some real estate data from real estate data website from Edmonton and marked in Edmonton Map.</p>\n<p><img src=\"/images/MachineLearning-data2.png\" alt=\"\"></p>\n<h1 id=\"2-Matching-Data\"><a href=\"#2-Matching-Data\" class=\"headerlink\" title=\"2 Matching Data\"></a>2 Matching Data</h1><p>Then we tried to match the real estate data with over 110 features in <a href=\"https://data.edmonton.ca/\" target=\"_blank\" rel=\"noopener\">Open City Edmonton Data</a></p>\n<p>It is a tough work there are different names for an address which is easier for the human to understand than a machine. After a little of the algorithm in nature language process(very basic). </p>\n<p>we got the following useful data.<br><img src=\"/images/MachineLearning-data1.png\" alt=\"\"></p>\n<p>This is our first step for collating data and scraping data.</p>\n<p>I will keep updating for Machine learning.<br>See you next time.</p>\n<p><a href=\"https://github.com/koosha/property_values\" target=\"_blank\" rel=\"noopener\">Our code resource</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>I have taken a Machine Learning Course at the University of Alberta. Our professor is <a href=\"https://en.wikipedia.org/wiki/Russell_Greiner\" target=\"_blank\" rel=\"noopener\">Russell Greiner</a>. We need to make a project and do some researches by Machine Learning method with our coach <a href=\"https://sites.ualberta.ca/~golmoham/\" target=\"_blank\" rel=\"noopener\">Koosha Golmohammadi</a>. Since this is my first Machine Learning project. Soooo, this is not as good as some projects made by Machine Learning experts. However, we achieve a result is 90% accuracy which is closed to some the U.S agency in Boston.</p>\n<h1 id=\"1-Scraping-Data\"><a href=\"#1-Scraping-Data\" class=\"headerlink\" title=\"1 Scraping Data.\"></a>1 Scraping Data.</h1><p>We scraped some real estate data from real estate data website from Edmonton and marked in Edmonton Map.</p>\n<p><img src=\"/images/MachineLearning-data2.png\" alt=\"\"></p>\n<h1 id=\"2-Matching-Data\"><a href=\"#2-Matching-Data\" class=\"headerlink\" title=\"2 Matching Data\"></a>2 Matching Data</h1><p>Then we tried to match the real estate data with over 110 features in <a href=\"https://data.edmonton.ca/\" target=\"_blank\" rel=\"noopener\">Open City Edmonton Data</a></p>\n<p>It is a tough work there are different names for an address which is easier for the human to understand than a machine. After a little of the algorithm in nature language process(very basic). </p>\n<p>we got the following useful data.<br><img src=\"/images/MachineLearning-data1.png\" alt=\"\"></p>\n<p>This is our first step for collating data and scraping data.</p>\n<p>I will keep updating for Machine learning.<br>See you next time.</p>\n<p><a href=\"https://github.com/koosha/property_values\" target=\"_blank\" rel=\"noopener\">Our code resource</a></p>\n"},{"title":"cs231n note - 1","date":"2018-05-20T23:26:44.000Z","_content":"\nReference：\n\n[https://zhuanlan.zhihu.com/p/20894041](https://zhuanlan.zhihu.com/p/20894041)\n\n[https://zhuanlan.zhihu.com/p/20900216](https://zhuanlan.zhihu.com/p/20900216) \n\n# 图像分类\n\n颜色channel 有三个 red green blue， RGB\n\n流程就是 输入 -> 学习 -> 评价\n\n# K Nearest Neighbor 分类器\n\nk 越高 generalization 能力越好\n\n## 用距离来分类\nL1范数 和 L2范数 \n\nL1 \n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_1%28I_1%2CI_2%29%3D%5Csum_p%7CI%5Ep_1-I%5Ep_2%7C)\n\nL2\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_2%28I_1%2CI_2%29%3D%5Csqrt%7B+%5Csum_p%28I%5Ep_1-I%5Ep_2%29%5E2%7D)\n\n``\ndistances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))\n``\n\nL1和L2比较。。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在p-norm常用的特殊形式。\n\n\n## hyperparameter: 超参数，需要去测试调整\n\n## validation set: 测试集 只能使用一次\n\n## Cross validation: train <-> test\n\n\n##k Nearest Neighbor 优缺点:\n\nAdvantange:易于理解，实现简单。\n\nDisadvantage: 算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率\n\n\n","source":"_posts/cs231n-note-1.md","raw":"---\ntitle: cs231n note - 1\ndate: 2018-05-20 17:26:44\ntags: Machine_Learning\n---\n\nReference：\n\n[https://zhuanlan.zhihu.com/p/20894041](https://zhuanlan.zhihu.com/p/20894041)\n\n[https://zhuanlan.zhihu.com/p/20900216](https://zhuanlan.zhihu.com/p/20900216) \n\n# 图像分类\n\n颜色channel 有三个 red green blue， RGB\n\n流程就是 输入 -> 学习 -> 评价\n\n# K Nearest Neighbor 分类器\n\nk 越高 generalization 能力越好\n\n## 用距离来分类\nL1范数 和 L2范数 \n\nL1 \n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_1%28I_1%2CI_2%29%3D%5Csum_p%7CI%5Ep_1-I%5Ep_2%7C)\n\nL2\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_2%28I_1%2CI_2%29%3D%5Csqrt%7B+%5Csum_p%28I%5Ep_1-I%5Ep_2%29%5E2%7D)\n\n``\ndistances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))\n``\n\nL1和L2比较。。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在p-norm常用的特殊形式。\n\n\n## hyperparameter: 超参数，需要去测试调整\n\n## validation set: 测试集 只能使用一次\n\n## Cross validation: train <-> test\n\n\n##k Nearest Neighbor 优缺点:\n\nAdvantange:易于理解，实现简单。\n\nDisadvantage: 算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率\n\n\n","slug":"cs231n-note-1","published":1,"updated":"2018-05-28T02:25:25.950Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekzw000gim0rc799wmka","content":"<p>Reference：</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/20894041\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/20894041</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/20900216\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/20900216</a> </p>\n<h1 id=\"图像分类\"><a href=\"#图像分类\" class=\"headerlink\" title=\"图像分类\"></a>图像分类</h1><p>颜色channel 有三个 red green blue， RGB</p>\n<p>流程就是 输入 -&gt; 学习 -&gt; 评价</p>\n<h1 id=\"K-Nearest-Neighbor-分类器\"><a href=\"#K-Nearest-Neighbor-分类器\" class=\"headerlink\" title=\"K Nearest Neighbor 分类器\"></a>K Nearest Neighbor 分类器</h1><p>k 越高 generalization 能力越好</p>\n<h2 id=\"用距离来分类\"><a href=\"#用距离来分类\" class=\"headerlink\" title=\"用距离来分类\"></a>用距离来分类</h2><p>L1范数 和 L2范数 </p>\n<p>L1 </p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_1%28I_1%2CI_2%29%3D%5Csum_p%7CI%5Ep_1-I%5Ep_2%7C\" alt=\"\"></p>\n<p>L2</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_2%28I_1%2CI_2%29%3D%5Csqrt%7B+%5Csum_p%28I%5Ep_1-I%5Ep_2%29%5E2%7D\" alt=\"\"></p>\n<p><code>distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))</code></p>\n<p>L1和L2比较。。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在p-norm常用的特殊形式。</p>\n<h2 id=\"hyperparameter-超参数，需要去测试调整\"><a href=\"#hyperparameter-超参数，需要去测试调整\" class=\"headerlink\" title=\"hyperparameter: 超参数，需要去测试调整\"></a>hyperparameter: 超参数，需要去测试调整</h2><h2 id=\"validation-set-测试集-只能使用一次\"><a href=\"#validation-set-测试集-只能使用一次\" class=\"headerlink\" title=\"validation set: 测试集 只能使用一次\"></a>validation set: 测试集 只能使用一次</h2><h2 id=\"Cross-validation-train-lt-gt-test\"><a href=\"#Cross-validation-train-lt-gt-test\" class=\"headerlink\" title=\"Cross validation: train &lt;-&gt; test\"></a>Cross validation: train &lt;-&gt; test</h2><p>##k Nearest Neighbor 优缺点:</p>\n<p>Advantange:易于理解，实现简单。</p>\n<p>Disadvantage: 算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Reference：</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/20894041\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/20894041</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/20900216\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/20900216</a> </p>\n<h1 id=\"图像分类\"><a href=\"#图像分类\" class=\"headerlink\" title=\"图像分类\"></a>图像分类</h1><p>颜色channel 有三个 red green blue， RGB</p>\n<p>流程就是 输入 -&gt; 学习 -&gt; 评价</p>\n<h1 id=\"K-Nearest-Neighbor-分类器\"><a href=\"#K-Nearest-Neighbor-分类器\" class=\"headerlink\" title=\"K Nearest Neighbor 分类器\"></a>K Nearest Neighbor 分类器</h1><p>k 越高 generalization 能力越好</p>\n<h2 id=\"用距离来分类\"><a href=\"#用距离来分类\" class=\"headerlink\" title=\"用距离来分类\"></a>用距离来分类</h2><p>L1范数 和 L2范数 </p>\n<p>L1 </p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_1%28I_1%2CI_2%29%3D%5Csum_p%7CI%5Ep_1-I%5Ep_2%7C\" alt=\"\"></p>\n<p>L2</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+d_2%28I_1%2CI_2%29%3D%5Csqrt%7B+%5Csum_p%28I%5Ep_1-I%5Ep_2%29%5E2%7D\" alt=\"\"></p>\n<p><code>distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))</code></p>\n<p>L1和L2比较。。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在p-norm常用的特殊形式。</p>\n<h2 id=\"hyperparameter-超参数，需要去测试调整\"><a href=\"#hyperparameter-超参数，需要去测试调整\" class=\"headerlink\" title=\"hyperparameter: 超参数，需要去测试调整\"></a>hyperparameter: 超参数，需要去测试调整</h2><h2 id=\"validation-set-测试集-只能使用一次\"><a href=\"#validation-set-测试集-只能使用一次\" class=\"headerlink\" title=\"validation set: 测试集 只能使用一次\"></a>validation set: 测试集 只能使用一次</h2><h2 id=\"Cross-validation-train-lt-gt-test\"><a href=\"#Cross-validation-train-lt-gt-test\" class=\"headerlink\" title=\"Cross validation: train &lt;-&gt; test\"></a>Cross validation: train &lt;-&gt; test</h2><p>##k Nearest Neighbor 优缺点:</p>\n<p>Advantange:易于理解，实现简单。</p>\n<p>Disadvantage: 算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率</p>\n"},{"title":"cs231n note - 2","date":"2018-05-21T20:56:48.000Z","mathjax":true,"_content":"\nReference:\n\n[https://zhuanlan.zhihu.com/p/20918580](https://zhuanlan.zhihu.com/p/20918580)\n\n[https://zhuanlan.zhihu.com/p/20945670](https://zhuanlan.zhihu.com/p/20945670)\n\n[https://zhuanlan.zhihu.com/p/21102293](https://zhuanlan.zhihu.com/p/21102293)\n\n\n# Linear Classification \n\n## score function (评分函数)\n\n原始图片到各个类别的评分情况， 分越高越接近\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb)\n\n每个图像都是 [D x 1] D = pixel X pixel X 3(RGB)\n\n### 参数\n\n权重\n\nW (weight) = [k X D] k是样本数 \n\n在维度空间里 做旋转变换\n\n偏差向量\n\nb (bias vector) = [k x 1]\n\n在维度空间里 做平移变化\n\n### 图像数据预处理\n\n最常见的就是 normalization\n\n零均值的中心化 把 [0-255] -> [-127-127] -> [-1,1]\n\n## Loss Function (损失函数)\n\n量化分类label 与真实label 之间的一致性 也叫 代价函数Cost Function或目标函数Objective\n\n当 Score Function 与真实结果相差越大，Cost Function输出越大 \n\n### 多类支持向量机损失 Multiclass Support Vector Machine Loss\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cs_j-s_%7By_i%7D%2B%5CDelta%29)\n\neg: 有三个分类 score 是 s = [13, -7, 11] \\delta = 10, 第一个label 是真实正确的\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3Dmax%280%2C-7-13%2B10%29%2Bmax%280%2C11-13%2B10%29)\n\n因为 -20 大于 \\delta 边界值， 所以最后的损失值为8\n\n线性评分函数 的损失函数公式：\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%29)\n\n### 折叶损失（hinge loss）: max (0, -) \n\n### 平方折叶损失SVM（即L2-SVM）： max (0, -) ^2\n\n![](https://pic4.zhimg.com/80/f254bd8d072128f1088c8cc47c3dff58_hd.jpg)\n\n目的是想要正确类别进入红色区域， 如果其他类别进入红色区域甚至更高的时候，计算loss， 我们的目的是找权重W\n\n## Regularization 正则化\n\n假设有一个数据集和一个权重集W能够正确地分类每个数据， 可能有很多相似的W都能正确地分类所有的数据\n\n比如有一个权重W 调整系数可以改变Loss score。 我们希望給W添加一些偏好\n\n向损失函数增加一个正则化惩罚\n\n### egularization penalty 正则化惩罚 R(W)\n\n![](https://www.zhihu.com/equation?tex=R%28W%29%3D%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D)\n\n多类SVM损失函数 L = 数据损失（data loss），即所有样例的的平均损失L_i + 正则化损失（regularization loss）\n\n![](https://www.zhihu.com/equation?tex=L%3D%5Cdisplaystyle+%5Cunderbrace%7B+%5Cfrac%7B1%7D%7BN%7D%5Csum_i+L_i%7D_%7Bdata+%5C++loss%7D%2B%5Cunderbrace%7B%5Clambda+R%28W%29%7D_%7Bregularization+%5C+loss%7D)\n\n展开公式\n\n![](https://www.zhihu.com/equation?tex=L%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_i%5Csum_%7Bj%5Cnot%3Dy_i%7D%5Bmax%280%2Cf%28x_i%3BW%29_j-f%28x_i%3BW%29_%7By_i%7D%2B%5CDelta%29%5D%2B%5Clambda+%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D)\n\n\nCode:\n\n\tdef L_i(x, y, W):\n\t  \"\"\"\n\t  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n\t  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n\t    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n\t  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n\t  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n\t  \"\"\"\n\t  delta = 1.0 # see notes about delta later in this section\n\t  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n\t  correct_class_score = scores[y]\n\t  D = W.shape[0] # number of classes, e.g. 10\n\t  loss_i = 0.0\n\t  for j in xrange(D): # iterate over all wrong classes\n\t    if j == y:\n\t      # skip for the true class to only loop over incorrect classes\n\t      continue\n\t    # accumulate loss for the i-th example\n\t    loss_i += max(0, scores[j] - correct_class_score + delta)\n\t  return loss_i\n\n\tdef L_i_vectorized(x, y, W):\n\t  \"\"\"\n\t  A faster half-vectorized implementation. half-vectorized\n\t  refers to the fact that for a single example the implementation contains\n\t  no for loops, but there is still one loop over the examples (outside this function)\n\t  \"\"\"\n\t  delta = 1.0\n\t  scores = W.dot(x)\n\t  # compute the margins for all classes in one vector operation\n\t  margins = np.maximum(0, scores - scores[y] + delta)\n\t  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n\t  # to ignore the y-th position and only consider margin on max wrong class\n\t  margins[y] = 0\n\t  loss_i = np.sum(margins)\n\t  return loss_i\n\n\tdef L(X, y, W):\n\t  \"\"\"\n\t  fully-vectorized implementation :\n\t  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n\t  - y is array of integers specifying correct class (e.g. 50,000-D array)\n\t  - W are weights (e.g. 10 x 3073)\n\t  \"\"\"\n\t  # evaluate loss over all examples in X without using any for loops\n\t  # left as exercise to reader in the assignment\n  \t  delta = 1.0\n  \t  score_matrix = W.dot(X)\n  \t  correct_score = score_matrix[y]\n  \t  #Todo, correct_score_matrix = correct_core * 50\n  \t  #Todo, delta_matrx repeat delta\n  \t  loss_matrix = score_matrix - correct_score_matrix + delta_matrx\n  \t  result = np.sum(loss_matrix, axis=1)\n\t  return loss_matrix\n\n\n### Softmax分类器 \n\n逻辑回归分类器面对多个分类的一般化归纳\n\n公式：\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29) \n\n或者\n\n![](https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29)\n\n所有的函数转换成 $$e^z$$\n\nscore function => ![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)\n\nsoftmax 函数, 每个元素都在0-1之间并且和为1\n\n\n概率解释:\n\n![](https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D)\n\n我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE）\n\n实现softmax函数计算的时候技巧可以用常数C， 通常$$\\log C = -maxf_j$$ \n\n![](https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D)\n\n\tf = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大\n\tp = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸\n\n\t# 那么将f中的值平移到最大值为0：\n\tf -= np.max(f) # f becomes [-666, -333, 0]\n\tp = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果\n\n\n折叶损失（hinge loss）替换为交叉熵损失（cross-entropy loss）\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)\n\n\n### SVM和Softmax的比较\n\nSoftmax \n\n\n### Summary\n\n- SVM 和 Softmax 基于 weight W 和 bias b\n\n- define Loss Function (损失函数) 用来更好的定义更好的预测模型\n \n\n----sad----\nTest Mathjax, 但是公式实在太复杂。\n\n$$\nL_{i}=f_{y_i}+\\log (\\sum_{j} e^{f_j})\n$$\n\n$L_{i} = -\\log \\frac {e^{f_y}} {e^{f_j}}$\n\n$\\sum_{j=0}$\n\n$$\na+b=c\n$$\n\nTODO","source":"_posts/cs231n-note-2.md","raw":"---\ntitle: cs231n note - 2\ndate: 2018-05-21 14:56:48\nmathjax: true\ntags: Machine_Learning\n---\n\nReference:\n\n[https://zhuanlan.zhihu.com/p/20918580](https://zhuanlan.zhihu.com/p/20918580)\n\n[https://zhuanlan.zhihu.com/p/20945670](https://zhuanlan.zhihu.com/p/20945670)\n\n[https://zhuanlan.zhihu.com/p/21102293](https://zhuanlan.zhihu.com/p/21102293)\n\n\n# Linear Classification \n\n## score function (评分函数)\n\n原始图片到各个类别的评分情况， 分越高越接近\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb)\n\n每个图像都是 [D x 1] D = pixel X pixel X 3(RGB)\n\n### 参数\n\n权重\n\nW (weight) = [k X D] k是样本数 \n\n在维度空间里 做旋转变换\n\n偏差向量\n\nb (bias vector) = [k x 1]\n\n在维度空间里 做平移变化\n\n### 图像数据预处理\n\n最常见的就是 normalization\n\n零均值的中心化 把 [0-255] -> [-127-127] -> [-1,1]\n\n## Loss Function (损失函数)\n\n量化分类label 与真实label 之间的一致性 也叫 代价函数Cost Function或目标函数Objective\n\n当 Score Function 与真实结果相差越大，Cost Function输出越大 \n\n### 多类支持向量机损失 Multiclass Support Vector Machine Loss\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cs_j-s_%7By_i%7D%2B%5CDelta%29)\n\neg: 有三个分类 score 是 s = [13, -7, 11] \\delta = 10, 第一个label 是真实正确的\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3Dmax%280%2C-7-13%2B10%29%2Bmax%280%2C11-13%2B10%29)\n\n因为 -20 大于 \\delta 边界值， 所以最后的损失值为8\n\n线性评分函数 的损失函数公式：\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%29)\n\n### 折叶损失（hinge loss）: max (0, -) \n\n### 平方折叶损失SVM（即L2-SVM）： max (0, -) ^2\n\n![](https://pic4.zhimg.com/80/f254bd8d072128f1088c8cc47c3dff58_hd.jpg)\n\n目的是想要正确类别进入红色区域， 如果其他类别进入红色区域甚至更高的时候，计算loss， 我们的目的是找权重W\n\n## Regularization 正则化\n\n假设有一个数据集和一个权重集W能够正确地分类每个数据， 可能有很多相似的W都能正确地分类所有的数据\n\n比如有一个权重W 调整系数可以改变Loss score。 我们希望給W添加一些偏好\n\n向损失函数增加一个正则化惩罚\n\n### egularization penalty 正则化惩罚 R(W)\n\n![](https://www.zhihu.com/equation?tex=R%28W%29%3D%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D)\n\n多类SVM损失函数 L = 数据损失（data loss），即所有样例的的平均损失L_i + 正则化损失（regularization loss）\n\n![](https://www.zhihu.com/equation?tex=L%3D%5Cdisplaystyle+%5Cunderbrace%7B+%5Cfrac%7B1%7D%7BN%7D%5Csum_i+L_i%7D_%7Bdata+%5C++loss%7D%2B%5Cunderbrace%7B%5Clambda+R%28W%29%7D_%7Bregularization+%5C+loss%7D)\n\n展开公式\n\n![](https://www.zhihu.com/equation?tex=L%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_i%5Csum_%7Bj%5Cnot%3Dy_i%7D%5Bmax%280%2Cf%28x_i%3BW%29_j-f%28x_i%3BW%29_%7By_i%7D%2B%5CDelta%29%5D%2B%5Clambda+%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D)\n\n\nCode:\n\n\tdef L_i(x, y, W):\n\t  \"\"\"\n\t  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n\t  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n\t    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n\t  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n\t  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n\t  \"\"\"\n\t  delta = 1.0 # see notes about delta later in this section\n\t  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n\t  correct_class_score = scores[y]\n\t  D = W.shape[0] # number of classes, e.g. 10\n\t  loss_i = 0.0\n\t  for j in xrange(D): # iterate over all wrong classes\n\t    if j == y:\n\t      # skip for the true class to only loop over incorrect classes\n\t      continue\n\t    # accumulate loss for the i-th example\n\t    loss_i += max(0, scores[j] - correct_class_score + delta)\n\t  return loss_i\n\n\tdef L_i_vectorized(x, y, W):\n\t  \"\"\"\n\t  A faster half-vectorized implementation. half-vectorized\n\t  refers to the fact that for a single example the implementation contains\n\t  no for loops, but there is still one loop over the examples (outside this function)\n\t  \"\"\"\n\t  delta = 1.0\n\t  scores = W.dot(x)\n\t  # compute the margins for all classes in one vector operation\n\t  margins = np.maximum(0, scores - scores[y] + delta)\n\t  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n\t  # to ignore the y-th position and only consider margin on max wrong class\n\t  margins[y] = 0\n\t  loss_i = np.sum(margins)\n\t  return loss_i\n\n\tdef L(X, y, W):\n\t  \"\"\"\n\t  fully-vectorized implementation :\n\t  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n\t  - y is array of integers specifying correct class (e.g. 50,000-D array)\n\t  - W are weights (e.g. 10 x 3073)\n\t  \"\"\"\n\t  # evaluate loss over all examples in X without using any for loops\n\t  # left as exercise to reader in the assignment\n  \t  delta = 1.0\n  \t  score_matrix = W.dot(X)\n  \t  correct_score = score_matrix[y]\n  \t  #Todo, correct_score_matrix = correct_core * 50\n  \t  #Todo, delta_matrx repeat delta\n  \t  loss_matrix = score_matrix - correct_score_matrix + delta_matrx\n  \t  result = np.sum(loss_matrix, axis=1)\n\t  return loss_matrix\n\n\n### Softmax分类器 \n\n逻辑回归分类器面对多个分类的一般化归纳\n\n公式：\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29) \n\n或者\n\n![](https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29)\n\n所有的函数转换成 $$e^z$$\n\nscore function => ![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)\n\nsoftmax 函数, 每个元素都在0-1之间并且和为1\n\n\n概率解释:\n\n![](https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D)\n\n我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE）\n\n实现softmax函数计算的时候技巧可以用常数C， 通常$$\\log C = -maxf_j$$ \n\n![](https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D)\n\n\tf = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大\n\tp = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸\n\n\t# 那么将f中的值平移到最大值为0：\n\tf -= np.max(f) # f becomes [-666, -333, 0]\n\tp = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果\n\n\n折叶损失（hinge loss）替换为交叉熵损失（cross-entropy loss）\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)\n\n\n### SVM和Softmax的比较\n\nSoftmax \n\n\n### Summary\n\n- SVM 和 Softmax 基于 weight W 和 bias b\n\n- define Loss Function (损失函数) 用来更好的定义更好的预测模型\n \n\n----sad----\nTest Mathjax, 但是公式实在太复杂。\n\n$$\nL_{i}=f_{y_i}+\\log (\\sum_{j} e^{f_j})\n$$\n\n$L_{i} = -\\log \\frac {e^{f_y}} {e^{f_j}}$\n\n$\\sum_{j=0}$\n\n$$\na+b=c\n$$\n\nTODO","slug":"cs231n-note-2","published":1,"updated":"2018-06-04T06:43:39.190Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekzy000him0r1l9bwtje","content":"<p>Reference:</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/20918580\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/20918580</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/20945670\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/20945670</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/21102293\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/21102293</a></p>\n<h1 id=\"Linear-Classification\"><a href=\"#Linear-Classification\" class=\"headerlink\" title=\"Linear Classification\"></a>Linear Classification</h1><h2 id=\"score-function-评分函数\"><a href=\"#score-function-评分函数\" class=\"headerlink\" title=\"score function (评分函数)\"></a>score function (评分函数)</h2><p>原始图片到各个类别的评分情况， 分越高越接近</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb\" alt=\"\"></p>\n<p>每个图像都是 [D x 1] D = pixel X pixel X 3(RGB)</p>\n<h3 id=\"参数\"><a href=\"#参数\" class=\"headerlink\" title=\"参数\"></a>参数</h3><p>权重</p>\n<p>W (weight) = [k X D] k是样本数 </p>\n<p>在维度空间里 做旋转变换</p>\n<p>偏差向量</p>\n<p>b (bias vector) = [k x 1]</p>\n<p>在维度空间里 做平移变化</p>\n<h3 id=\"图像数据预处理\"><a href=\"#图像数据预处理\" class=\"headerlink\" title=\"图像数据预处理\"></a>图像数据预处理</h3><p>最常见的就是 normalization</p>\n<p>零均值的中心化 把 [0-255] -&gt; [-127-127] -&gt; [-1,1]</p>\n<h2 id=\"Loss-Function-损失函数\"><a href=\"#Loss-Function-损失函数\" class=\"headerlink\" title=\"Loss Function (损失函数)\"></a>Loss Function (损失函数)</h2><p>量化分类label 与真实label 之间的一致性 也叫 代价函数Cost Function或目标函数Objective</p>\n<p>当 Score Function 与真实结果相差越大，Cost Function输出越大 </p>\n<h3 id=\"多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss\"><a href=\"#多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss\" class=\"headerlink\" title=\"多类支持向量机损失 Multiclass Support Vector Machine Loss\"></a>多类支持向量机损失 Multiclass Support Vector Machine Loss</h3><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cs_j-s_%7By_i%7D%2B%5CDelta%29\" alt=\"\"></p>\n<p>eg: 有三个分类 score 是 s = [13, -7, 11] \\delta = 10, 第一个label 是真实正确的</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3Dmax%280%2C-7-13%2B10%29%2Bmax%280%2C11-13%2B10%29\" alt=\"\"></p>\n<p>因为 -20 大于 \\delta 边界值， 所以最后的损失值为8</p>\n<p>线性评分函数 的损失函数公式：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%29\" alt=\"\"></p>\n<h3 id=\"折叶损失（hinge-loss）-max-0\"><a href=\"#折叶损失（hinge-loss）-max-0\" class=\"headerlink\" title=\"折叶损失（hinge loss）: max (0, -)\"></a>折叶损失（hinge loss）: max (0, -)</h3><h3 id=\"平方折叶损失SVM（即L2-SVM）：-max-0-2\"><a href=\"#平方折叶损失SVM（即L2-SVM）：-max-0-2\" class=\"headerlink\" title=\"平方折叶损失SVM（即L2-SVM）： max (0, -) ^2\"></a>平方折叶损失SVM（即L2-SVM）： max (0, -) ^2</h3><p><img src=\"https://pic4.zhimg.com/80/f254bd8d072128f1088c8cc47c3dff58_hd.jpg\" alt=\"\"></p>\n<p>目的是想要正确类别进入红色区域， 如果其他类别进入红色区域甚至更高的时候，计算loss， 我们的目的是找权重W</p>\n<h2 id=\"Regularization-正则化\"><a href=\"#Regularization-正则化\" class=\"headerlink\" title=\"Regularization 正则化\"></a>Regularization 正则化</h2><p>假设有一个数据集和一个权重集W能够正确地分类每个数据， 可能有很多相似的W都能正确地分类所有的数据</p>\n<p>比如有一个权重W 调整系数可以改变Loss score。 我们希望給W添加一些偏好</p>\n<p>向损失函数增加一个正则化惩罚</p>\n<h3 id=\"egularization-penalty-正则化惩罚-R-W\"><a href=\"#egularization-penalty-正则化惩罚-R-W\" class=\"headerlink\" title=\"egularization penalty 正则化惩罚 R(W)\"></a>egularization penalty 正则化惩罚 R(W)</h3><p><img src=\"https://www.zhihu.com/equation?tex=R%28W%29%3D%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D\" alt=\"\"></p>\n<p>多类SVM损失函数 L = 数据损失（data loss），即所有样例的的平均损失L_i + 正则化损失（regularization loss）</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=L%3D%5Cdisplaystyle+%5Cunderbrace%7B+%5Cfrac%7B1%7D%7BN%7D%5Csum_i+L_i%7D_%7Bdata+%5C++loss%7D%2B%5Cunderbrace%7B%5Clambda+R%28W%29%7D_%7Bregularization+%5C+loss%7D\" alt=\"\"></p>\n<p>展开公式</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=L%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_i%5Csum_%7Bj%5Cnot%3Dy_i%7D%5Bmax%280%2Cf%28x_i%3BW%29_j-f%28x_i%3BW%29_%7By_i%7D%2B%5CDelta%29%5D%2B%5Clambda+%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D\" alt=\"\"></p>\n<p>Code:</p>\n<pre><code>def L_i(x, y, W):\n  &quot;&quot;&quot;\n  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n  &quot;&quot;&quot;\n  delta = 1.0 # see notes about delta later in this section\n  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n  correct_class_score = scores[y]\n  D = W.shape[0] # number of classes, e.g. 10\n  loss_i = 0.0\n  for j in xrange(D): # iterate over all wrong classes\n    if j == y:\n      # skip for the true class to only loop over incorrect classes\n      continue\n    # accumulate loss for the i-th example\n    loss_i += max(0, scores[j] - correct_class_score + delta)\n  return loss_i\n\ndef L_i_vectorized(x, y, W):\n  &quot;&quot;&quot;\n  A faster half-vectorized implementation. half-vectorized\n  refers to the fact that for a single example the implementation contains\n  no for loops, but there is still one loop over the examples (outside this function)\n  &quot;&quot;&quot;\n  delta = 1.0\n  scores = W.dot(x)\n  # compute the margins for all classes in one vector operation\n  margins = np.maximum(0, scores - scores[y] + delta)\n  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n  # to ignore the y-th position and only consider margin on max wrong class\n  margins[y] = 0\n  loss_i = np.sum(margins)\n  return loss_i\n\ndef L(X, y, W):\n  &quot;&quot;&quot;\n  fully-vectorized implementation :\n  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n  - y is array of integers specifying correct class (e.g. 50,000-D array)\n  - W are weights (e.g. 10 x 3073)\n  &quot;&quot;&quot;\n  # evaluate loss over all examples in X without using any for loops\n  # left as exercise to reader in the assignment\n    delta = 1.0\n    score_matrix = W.dot(X)\n    correct_score = score_matrix[y]\n    #Todo, correct_score_matrix = correct_core * 50\n    #Todo, delta_matrx repeat delta\n    loss_matrix = score_matrix - correct_score_matrix + delta_matrx\n    result = np.sum(loss_matrix, axis=1)\n  return loss_matrix\n</code></pre><h3 id=\"Softmax分类器\"><a href=\"#Softmax分类器\" class=\"headerlink\" title=\"Softmax分类器\"></a>Softmax分类器</h3><p>逻辑回归分类器面对多个分类的一般化归纳</p>\n<p>公式：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29\" alt=\"\"> </p>\n<p>或者</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29\" alt=\"\"></p>\n<p>所有的函数转换成 $$e^z$$</p>\n<p>score function =&gt; <img src=\"https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D\" alt=\"\"></p>\n<p>softmax 函数, 每个元素都在0-1之间并且和为1</p>\n<p>概率解释:</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D\" alt=\"\"></p>\n<p>我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE）</p>\n<p>实现softmax函数计算的时候技巧可以用常数C， 通常$$\\log C = -maxf_j$$ </p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D\" alt=\"\"></p>\n<pre><code>f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大\np = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸\n\n# 那么将f中的值平移到最大值为0：\nf -= np.max(f) # f becomes [-666, -333, 0]\np = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果\n</code></pre><p>折叶损失（hinge loss）替换为交叉熵损失（cross-entropy loss）</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29\" alt=\"\"></p>\n<h3 id=\"SVM和Softmax的比较\"><a href=\"#SVM和Softmax的比较\" class=\"headerlink\" title=\"SVM和Softmax的比较\"></a>SVM和Softmax的比较</h3><p>Softmax </p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><ul>\n<li><p>SVM 和 Softmax 基于 weight W 和 bias b</p>\n</li>\n<li><p>define Loss Function (损失函数) 用来更好的定义更好的预测模型</p>\n</li>\n</ul>\n<p>—-sad—-<br>Test Mathjax, 但是公式实在太复杂。</p>\n<p>$$<br>L<em>{i}=f</em>{y<em>i}+\\log (\\sum</em>{j} e^{f_j})<br>$$</p>\n<p>$L_{i} = -\\log \\frac {e^{f_y}} {e^{f_j}}$</p>\n<p>$\\sum_{j=0}$</p>\n<p>$$<br>a+b=c<br>$$</p>\n<p>TODO</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Reference:</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/20918580\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/20918580</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/20945670\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/20945670</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/21102293\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/21102293</a></p>\n<h1 id=\"Linear-Classification\"><a href=\"#Linear-Classification\" class=\"headerlink\" title=\"Linear Classification\"></a>Linear Classification</h1><h2 id=\"score-function-评分函数\"><a href=\"#score-function-评分函数\" class=\"headerlink\" title=\"score function (评分函数)\"></a>score function (评分函数)</h2><p>原始图片到各个类别的评分情况， 分越高越接近</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb\" alt=\"\"></p>\n<p>每个图像都是 [D x 1] D = pixel X pixel X 3(RGB)</p>\n<h3 id=\"参数\"><a href=\"#参数\" class=\"headerlink\" title=\"参数\"></a>参数</h3><p>权重</p>\n<p>W (weight) = [k X D] k是样本数 </p>\n<p>在维度空间里 做旋转变换</p>\n<p>偏差向量</p>\n<p>b (bias vector) = [k x 1]</p>\n<p>在维度空间里 做平移变化</p>\n<h3 id=\"图像数据预处理\"><a href=\"#图像数据预处理\" class=\"headerlink\" title=\"图像数据预处理\"></a>图像数据预处理</h3><p>最常见的就是 normalization</p>\n<p>零均值的中心化 把 [0-255] -&gt; [-127-127] -&gt; [-1,1]</p>\n<h2 id=\"Loss-Function-损失函数\"><a href=\"#Loss-Function-损失函数\" class=\"headerlink\" title=\"Loss Function (损失函数)\"></a>Loss Function (损失函数)</h2><p>量化分类label 与真实label 之间的一致性 也叫 代价函数Cost Function或目标函数Objective</p>\n<p>当 Score Function 与真实结果相差越大，Cost Function输出越大 </p>\n<h3 id=\"多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss\"><a href=\"#多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss\" class=\"headerlink\" title=\"多类支持向量机损失 Multiclass Support Vector Machine Loss\"></a>多类支持向量机损失 Multiclass Support Vector Machine Loss</h3><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cs_j-s_%7By_i%7D%2B%5CDelta%29\" alt=\"\"></p>\n<p>eg: 有三个分类 score 是 s = [13, -7, 11] \\delta = 10, 第一个label 是真实正确的</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3Dmax%280%2C-7-13%2B10%29%2Bmax%280%2C11-13%2B10%29\" alt=\"\"></p>\n<p>因为 -20 大于 \\delta 边界值， 所以最后的损失值为8</p>\n<p>线性评分函数 的损失函数公式：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%29\" alt=\"\"></p>\n<h3 id=\"折叶损失（hinge-loss）-max-0\"><a href=\"#折叶损失（hinge-loss）-max-0\" class=\"headerlink\" title=\"折叶损失（hinge loss）: max (0, -)\"></a>折叶损失（hinge loss）: max (0, -)</h3><h3 id=\"平方折叶损失SVM（即L2-SVM）：-max-0-2\"><a href=\"#平方折叶损失SVM（即L2-SVM）：-max-0-2\" class=\"headerlink\" title=\"平方折叶损失SVM（即L2-SVM）： max (0, -) ^2\"></a>平方折叶损失SVM（即L2-SVM）： max (0, -) ^2</h3><p><img src=\"https://pic4.zhimg.com/80/f254bd8d072128f1088c8cc47c3dff58_hd.jpg\" alt=\"\"></p>\n<p>目的是想要正确类别进入红色区域， 如果其他类别进入红色区域甚至更高的时候，计算loss， 我们的目的是找权重W</p>\n<h2 id=\"Regularization-正则化\"><a href=\"#Regularization-正则化\" class=\"headerlink\" title=\"Regularization 正则化\"></a>Regularization 正则化</h2><p>假设有一个数据集和一个权重集W能够正确地分类每个数据， 可能有很多相似的W都能正确地分类所有的数据</p>\n<p>比如有一个权重W 调整系数可以改变Loss score。 我们希望給W添加一些偏好</p>\n<p>向损失函数增加一个正则化惩罚</p>\n<h3 id=\"egularization-penalty-正则化惩罚-R-W\"><a href=\"#egularization-penalty-正则化惩罚-R-W\" class=\"headerlink\" title=\"egularization penalty 正则化惩罚 R(W)\"></a>egularization penalty 正则化惩罚 R(W)</h3><p><img src=\"https://www.zhihu.com/equation?tex=R%28W%29%3D%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D\" alt=\"\"></p>\n<p>多类SVM损失函数 L = 数据损失（data loss），即所有样例的的平均损失L_i + 正则化损失（regularization loss）</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=L%3D%5Cdisplaystyle+%5Cunderbrace%7B+%5Cfrac%7B1%7D%7BN%7D%5Csum_i+L_i%7D_%7Bdata+%5C++loss%7D%2B%5Cunderbrace%7B%5Clambda+R%28W%29%7D_%7Bregularization+%5C+loss%7D\" alt=\"\"></p>\n<p>展开公式</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=L%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_i%5Csum_%7Bj%5Cnot%3Dy_i%7D%5Bmax%280%2Cf%28x_i%3BW%29_j-f%28x_i%3BW%29_%7By_i%7D%2B%5CDelta%29%5D%2B%5Clambda+%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D\" alt=\"\"></p>\n<p>Code:</p>\n<pre><code>def L_i(x, y, W):\n  &quot;&quot;&quot;\n  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n  &quot;&quot;&quot;\n  delta = 1.0 # see notes about delta later in this section\n  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n  correct_class_score = scores[y]\n  D = W.shape[0] # number of classes, e.g. 10\n  loss_i = 0.0\n  for j in xrange(D): # iterate over all wrong classes\n    if j == y:\n      # skip for the true class to only loop over incorrect classes\n      continue\n    # accumulate loss for the i-th example\n    loss_i += max(0, scores[j] - correct_class_score + delta)\n  return loss_i\n\ndef L_i_vectorized(x, y, W):\n  &quot;&quot;&quot;\n  A faster half-vectorized implementation. half-vectorized\n  refers to the fact that for a single example the implementation contains\n  no for loops, but there is still one loop over the examples (outside this function)\n  &quot;&quot;&quot;\n  delta = 1.0\n  scores = W.dot(x)\n  # compute the margins for all classes in one vector operation\n  margins = np.maximum(0, scores - scores[y] + delta)\n  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n  # to ignore the y-th position and only consider margin on max wrong class\n  margins[y] = 0\n  loss_i = np.sum(margins)\n  return loss_i\n\ndef L(X, y, W):\n  &quot;&quot;&quot;\n  fully-vectorized implementation :\n  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n  - y is array of integers specifying correct class (e.g. 50,000-D array)\n  - W are weights (e.g. 10 x 3073)\n  &quot;&quot;&quot;\n  # evaluate loss over all examples in X without using any for loops\n  # left as exercise to reader in the assignment\n    delta = 1.0\n    score_matrix = W.dot(X)\n    correct_score = score_matrix[y]\n    #Todo, correct_score_matrix = correct_core * 50\n    #Todo, delta_matrx repeat delta\n    loss_matrix = score_matrix - correct_score_matrix + delta_matrx\n    result = np.sum(loss_matrix, axis=1)\n  return loss_matrix\n</code></pre><h3 id=\"Softmax分类器\"><a href=\"#Softmax分类器\" class=\"headerlink\" title=\"Softmax分类器\"></a>Softmax分类器</h3><p>逻辑回归分类器面对多个分类的一般化归纳</p>\n<p>公式：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29\" alt=\"\"> </p>\n<p>或者</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29\" alt=\"\"></p>\n<p>所有的函数转换成 $$e^z$$</p>\n<p>score function =&gt; <img src=\"https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D\" alt=\"\"></p>\n<p>softmax 函数, 每个元素都在0-1之间并且和为1</p>\n<p>概率解释:</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D\" alt=\"\"></p>\n<p>我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE）</p>\n<p>实现softmax函数计算的时候技巧可以用常数C， 通常$$\\log C = -maxf_j$$ </p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D\" alt=\"\"></p>\n<pre><code>f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大\np = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸\n\n# 那么将f中的值平移到最大值为0：\nf -= np.max(f) # f becomes [-666, -333, 0]\np = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果\n</code></pre><p>折叶损失（hinge loss）替换为交叉熵损失（cross-entropy loss）</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29\" alt=\"\"></p>\n<h3 id=\"SVM和Softmax的比较\"><a href=\"#SVM和Softmax的比较\" class=\"headerlink\" title=\"SVM和Softmax的比较\"></a>SVM和Softmax的比较</h3><p>Softmax </p>\n<h3 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h3><ul>\n<li><p>SVM 和 Softmax 基于 weight W 和 bias b</p>\n</li>\n<li><p>define Loss Function (损失函数) 用来更好的定义更好的预测模型</p>\n</li>\n</ul>\n<p>—-sad—-<br>Test Mathjax, 但是公式实在太复杂。</p>\n<p>$$<br>L<em>{i}=f</em>{y<em>i}+\\log (\\sum</em>{j} e^{f_j})<br>$$</p>\n<p>$L_{i} = -\\log \\frac {e^{f_y}} {e^{f_j}}$</p>\n<p>$\\sum_{j=0}$</p>\n<p>$$<br>a+b=c<br>$$</p>\n<p>TODO</p>\n"},{"title":"cs231n-note-3","date":"2018-06-04T06:38:22.000Z","mathjax":true,"_content":"\n\n# Review \n\n- 基于参数的评分函数。该函数将原始图像像素映射为分类评分值。\n\n- 损失函数。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。 *Softmax 和 SVM 不应该包括了score function 和 lost function*\n\n- 最优化Optimization。最优化是寻找能使得损失函数值最小化的参数W的过程。\n\n\n\n# 损失函数可视化\n\n- 在一维尺度上 $$L(W+aW_1)$$ x轴是a， y轴是loss function\n\n- 在二维尺度上 $$L(W+aW_1+bW_2)$$, a,b 表达x轴, y轴， loss function 用颜色表示\n\n单独的损失函数表示：\n\n![](https://www.zhihu.com/equation?tex=Li%3D%5Csum_%7Bj%5Cnot%3Dy_i%7D%5Bmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B1%29%5D)\n\n$$w_j$$ 如果对应分类正确即是负号，错误即是正号 $$L = \\sumL/n$$\n\n![](https://pic3.zhimg.com/80/3f6fbcd487b1c214e8fea1ea66eb413e_hd.jpg)\n\nSVM的损失函数是一种凸函数，可以学习一下如何高效最小化凸函数，在这种损失函数会有一些不可导的点（kinks）\n\n两个新概念： \n- 梯度？ 次梯度？未来学习点\n\n# 最优化 Optimization\n\n对于神经网络的最优化策略有：\n\n## 策略#1：随即搜索 最差劲的搜索方案（base line）\n\n\t# 假设X_train的每一列都是一个数据样本（比如3073 x 50000）\n\t# 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）\n\t# 假设函数L对损失函数进行评价\n\n\tbestloss = float(\"inf\") # Python assigns the highest possible float value\n\tfor num in xrange(1000):\n\t  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n\t  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n\t  if loss < bestloss: # keep track of the best solution\n\t    bestloss = loss\n\t    bestW = W\n\t  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)\n\n感觉跟那个monkey sort 差不多 随机生成W weight。\n\n\t# 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]\n\tscores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples\n\t# 找到在每列中评分值最大的索引（即预测的分类）\n\tYte_predict = np.argmax(scores, axis = 0)\n\t# 以及计算准确率\n\tnp.mean(Yte_predict == Yte)\n\t# 返回 0.1555\n\n策略是：随机权重开始，然后迭代取优，从而获得更低的损失值。\n\n## 策略#2：随机本地搜索\n\n生成一个随机的扰动 $$ \\delta W $$\n\n$$ Wtry = W + \\delta W $$ \n\n当 Wtry 的loss 变小的时候， 才决定移动\n\n\tW = np.random.randn(10, 3073) * 0.001 # 生成随机初始W\n\tbestloss = float(\"inf\")\n\tfor i in xrange(1000):\n\t  step_size = 0.0001\n\t  Wtry = W + np.random.randn(10, 3073) * step_size\n\t  loss = L(Xtr_cols, Ytr, Wtry)\n\t  if loss < bestloss:\n\t    W = Wtry\n\t    bestloss = loss\n\t  print 'iter %d loss is %f' % (i, bestloss)\n\n\n## 策略#3：跟随梯度\n\n策略1 和 策略2 都是尝试好几个方向来找减少loss的方向，其实可以用梯度（gradient）来找到最陡峭的方向减少loss，\n\n一维求导公式： d(fx)/dx\n\n当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。\n\n\n# 梯度计算\n\n有两种方法计算梯度：\n\n## 数值梯度法 （实现简单 但是缓慢）\n\n\n\tdef eval_numerical_gradient(f, x):\n\t  \"\"\"  \n\t  一个f在x处的数值梯度法的简单实现\n\t  - f是只有一个参数的函数\n\t  - x是计算梯度的点\n\t  \"\"\" \n\n\t  fx = f(x) # 在原点计算函数值\n\t  grad = np.zeros(x.shape)\n\t  h = 0.00001\n\n\t  # 对x中所有的item索引进行迭代\n\t  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n\t  while not it.finished:\n\n\t    # 计算x+h处的函数值\n\t    ix = it.multi_index\n\t    old_value = x[ix]\n\t    x[ix] = old_value + h # 增加h\n\t    fxh = f(x) # 计算f(x + h)\n\t    x[ix] = old_value # 存到前一个值中 (非常重要)\n\n\t    # 计算偏导数\n\t    grad[ix] = (fxh - fx) / h # 坡度\n\t    it.iternext() # 到下个维度\n\n\t  return grad\n\n实际中用中心差值公式（centered difference formula）[f(x+h)-f(x-h)]/2h 效果较好 [Numerical_differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation)\n\n\t# 要使用上面的代码我们需要一个只有一个参数的函数\n\t# (在这里参数就是权重)所以也包含了X_train和Y_train\n\tdef CIFAR10_loss_fun(W):\n\t  return L(X_train, Y_train, W)\n\n\tW = np.random.rand(10, 3073) * 0.001 # 随机权重向量\n\tdf = eval_numerical_gradient(CIFAR10_loss_fun, W) # 得到梯度\n\nloss_original = CIFAR10_loss_fun(W) # 初始损失值\nprint 'original loss: %f' % (loss_original, )\n\n\t# 查看不同步长的效果\n\tfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n\t  step_size = 10 ** step_size_log\n\t  W_new = W - step_size * df # 权重空间中的新位置\n\t  loss_new = CIFAR10_loss_fun(W_new)\n\t  print 'for step size %f new loss: %f' % (step_size, loss_new)\n\n\t# 输出:\n\t# original loss: 2.200718\n\t# for step size 1.000000e-10 new loss: 2.200652\n\t# for step size 1.000000e-09 new loss: 2.200057\n\t# for step size 1.000000e-08 new loss: 2.194116\n\t# for step size 1.000000e-07 new loss: 2.135493\n\t# for step size 1.000000e-06 new loss: 1.647802\n\t# for step size 1.000000e-05 new loss: 2.844355\n\t# for step size 1.000000e-04 new loss: 25.558142\n\t# for step size 1.000000e-03 new loss: 254.086573\n\t# for step size 1.000000e-02 new loss: 2539.370888\n\t# for step size 1.000000e-01 new loss: 25392.214036\n\n步长的影响：梯度指明了函数在哪个方向?是变化率最大的 步长(也叫作学习率)\n\n小步长下降稳定但进度慢 <-> 大步长进展快但是风险更大\n\n在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度, 效率太低\n\n## 分析梯度法 （计算迅速，结果精确） 微分分析计算梯度\n\n用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错. 于是我们需要将分析梯度法的结果于数值梯度法作比较， 这个步骤叫做梯度检查。\n\neg: SVM lossfunction\n\n![](https://www.zhihu.com/equation?tex=L_i%3D%5Cdisplaystyle%5Csum_%7Bj%5Cnot+%3Dy_i%7D%5Bmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%29%5D)\n\n对W_yi 进行微分\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cnabla_%7Bw_%7By_i%7D%7DL_i%3D-%28%5Csum_%7Bj%5Cnot%3Dy_i%7D1%28w%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%3E0%29%29x_i)\n\n### 梯度下降\n\n#### 普通版本\n\n\t# 普通的梯度下降\n\twhile True:\n\t  weights_grad = evaluate_gradient(loss_fun, data, weights)\n\t  weights += - step_size * weights_grad # 进行梯度更新\n\n#### 小批量数据梯度下降（Mini-batch gradient descent）\n\n每次小子集往下减少，小批量数据的梯度就是对整个数据集梯度的一个近似， 需要data的数量远大于小批数量\n\n\t# 普通的小批量数据梯度下降\n\twhile True:\n\t  data_batch = sample_training_data(data, 256) # 256个数据\n\t  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n\t  weights += - step_size * weights_grad # 参数更新\n\n#### 随机梯度下降（Stochastic Gradient Descent 简称SGD）\n\n如果小批量数据中每个批量只有1个数据样本\n\n# Summary:\n\n![](https://pic2.zhimg.com/80/03b3eccf18ee3760e219f9f95ec14305_hd.jpg)\n\nx,y 是给定的，weight 从一个随机开始，可以随时改变。 损失函数包含两个部分：数据损失和正则化损失，在梯度下降中，计算权重的维度实现参数的更新。\n\n# Reference：\n\n[最优化上](https://zhuanlan.zhihu.com/p/21360434)\n\n[最优化下](https://zhuanlan.zhihu.com/p/21387326)\n","source":"_posts/cs231n-note-3.md","raw":"---\ntitle: cs231n-note-3\ndate: 2018-06-04 00:38:22\nmathjax: true\ntags: Machine_Learning\n---\n\n\n# Review \n\n- 基于参数的评分函数。该函数将原始图像像素映射为分类评分值。\n\n- 损失函数。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。 *Softmax 和 SVM 不应该包括了score function 和 lost function*\n\n- 最优化Optimization。最优化是寻找能使得损失函数值最小化的参数W的过程。\n\n\n\n# 损失函数可视化\n\n- 在一维尺度上 $$L(W+aW_1)$$ x轴是a， y轴是loss function\n\n- 在二维尺度上 $$L(W+aW_1+bW_2)$$, a,b 表达x轴, y轴， loss function 用颜色表示\n\n单独的损失函数表示：\n\n![](https://www.zhihu.com/equation?tex=Li%3D%5Csum_%7Bj%5Cnot%3Dy_i%7D%5Bmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B1%29%5D)\n\n$$w_j$$ 如果对应分类正确即是负号，错误即是正号 $$L = \\sumL/n$$\n\n![](https://pic3.zhimg.com/80/3f6fbcd487b1c214e8fea1ea66eb413e_hd.jpg)\n\nSVM的损失函数是一种凸函数，可以学习一下如何高效最小化凸函数，在这种损失函数会有一些不可导的点（kinks）\n\n两个新概念： \n- 梯度？ 次梯度？未来学习点\n\n# 最优化 Optimization\n\n对于神经网络的最优化策略有：\n\n## 策略#1：随即搜索 最差劲的搜索方案（base line）\n\n\t# 假设X_train的每一列都是一个数据样本（比如3073 x 50000）\n\t# 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）\n\t# 假设函数L对损失函数进行评价\n\n\tbestloss = float(\"inf\") # Python assigns the highest possible float value\n\tfor num in xrange(1000):\n\t  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n\t  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n\t  if loss < bestloss: # keep track of the best solution\n\t    bestloss = loss\n\t    bestW = W\n\t  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)\n\n感觉跟那个monkey sort 差不多 随机生成W weight。\n\n\t# 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]\n\tscores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples\n\t# 找到在每列中评分值最大的索引（即预测的分类）\n\tYte_predict = np.argmax(scores, axis = 0)\n\t# 以及计算准确率\n\tnp.mean(Yte_predict == Yte)\n\t# 返回 0.1555\n\n策略是：随机权重开始，然后迭代取优，从而获得更低的损失值。\n\n## 策略#2：随机本地搜索\n\n生成一个随机的扰动 $$ \\delta W $$\n\n$$ Wtry = W + \\delta W $$ \n\n当 Wtry 的loss 变小的时候， 才决定移动\n\n\tW = np.random.randn(10, 3073) * 0.001 # 生成随机初始W\n\tbestloss = float(\"inf\")\n\tfor i in xrange(1000):\n\t  step_size = 0.0001\n\t  Wtry = W + np.random.randn(10, 3073) * step_size\n\t  loss = L(Xtr_cols, Ytr, Wtry)\n\t  if loss < bestloss:\n\t    W = Wtry\n\t    bestloss = loss\n\t  print 'iter %d loss is %f' % (i, bestloss)\n\n\n## 策略#3：跟随梯度\n\n策略1 和 策略2 都是尝试好几个方向来找减少loss的方向，其实可以用梯度（gradient）来找到最陡峭的方向减少loss，\n\n一维求导公式： d(fx)/dx\n\n当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。\n\n\n# 梯度计算\n\n有两种方法计算梯度：\n\n## 数值梯度法 （实现简单 但是缓慢）\n\n\n\tdef eval_numerical_gradient(f, x):\n\t  \"\"\"  \n\t  一个f在x处的数值梯度法的简单实现\n\t  - f是只有一个参数的函数\n\t  - x是计算梯度的点\n\t  \"\"\" \n\n\t  fx = f(x) # 在原点计算函数值\n\t  grad = np.zeros(x.shape)\n\t  h = 0.00001\n\n\t  # 对x中所有的item索引进行迭代\n\t  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n\t  while not it.finished:\n\n\t    # 计算x+h处的函数值\n\t    ix = it.multi_index\n\t    old_value = x[ix]\n\t    x[ix] = old_value + h # 增加h\n\t    fxh = f(x) # 计算f(x + h)\n\t    x[ix] = old_value # 存到前一个值中 (非常重要)\n\n\t    # 计算偏导数\n\t    grad[ix] = (fxh - fx) / h # 坡度\n\t    it.iternext() # 到下个维度\n\n\t  return grad\n\n实际中用中心差值公式（centered difference formula）[f(x+h)-f(x-h)]/2h 效果较好 [Numerical_differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation)\n\n\t# 要使用上面的代码我们需要一个只有一个参数的函数\n\t# (在这里参数就是权重)所以也包含了X_train和Y_train\n\tdef CIFAR10_loss_fun(W):\n\t  return L(X_train, Y_train, W)\n\n\tW = np.random.rand(10, 3073) * 0.001 # 随机权重向量\n\tdf = eval_numerical_gradient(CIFAR10_loss_fun, W) # 得到梯度\n\nloss_original = CIFAR10_loss_fun(W) # 初始损失值\nprint 'original loss: %f' % (loss_original, )\n\n\t# 查看不同步长的效果\n\tfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n\t  step_size = 10 ** step_size_log\n\t  W_new = W - step_size * df # 权重空间中的新位置\n\t  loss_new = CIFAR10_loss_fun(W_new)\n\t  print 'for step size %f new loss: %f' % (step_size, loss_new)\n\n\t# 输出:\n\t# original loss: 2.200718\n\t# for step size 1.000000e-10 new loss: 2.200652\n\t# for step size 1.000000e-09 new loss: 2.200057\n\t# for step size 1.000000e-08 new loss: 2.194116\n\t# for step size 1.000000e-07 new loss: 2.135493\n\t# for step size 1.000000e-06 new loss: 1.647802\n\t# for step size 1.000000e-05 new loss: 2.844355\n\t# for step size 1.000000e-04 new loss: 25.558142\n\t# for step size 1.000000e-03 new loss: 254.086573\n\t# for step size 1.000000e-02 new loss: 2539.370888\n\t# for step size 1.000000e-01 new loss: 25392.214036\n\n步长的影响：梯度指明了函数在哪个方向?是变化率最大的 步长(也叫作学习率)\n\n小步长下降稳定但进度慢 <-> 大步长进展快但是风险更大\n\n在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度, 效率太低\n\n## 分析梯度法 （计算迅速，结果精确） 微分分析计算梯度\n\n用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错. 于是我们需要将分析梯度法的结果于数值梯度法作比较， 这个步骤叫做梯度检查。\n\neg: SVM lossfunction\n\n![](https://www.zhihu.com/equation?tex=L_i%3D%5Cdisplaystyle%5Csum_%7Bj%5Cnot+%3Dy_i%7D%5Bmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%29%5D)\n\n对W_yi 进行微分\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cnabla_%7Bw_%7By_i%7D%7DL_i%3D-%28%5Csum_%7Bj%5Cnot%3Dy_i%7D1%28w%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%3E0%29%29x_i)\n\n### 梯度下降\n\n#### 普通版本\n\n\t# 普通的梯度下降\n\twhile True:\n\t  weights_grad = evaluate_gradient(loss_fun, data, weights)\n\t  weights += - step_size * weights_grad # 进行梯度更新\n\n#### 小批量数据梯度下降（Mini-batch gradient descent）\n\n每次小子集往下减少，小批量数据的梯度就是对整个数据集梯度的一个近似， 需要data的数量远大于小批数量\n\n\t# 普通的小批量数据梯度下降\n\twhile True:\n\t  data_batch = sample_training_data(data, 256) # 256个数据\n\t  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n\t  weights += - step_size * weights_grad # 参数更新\n\n#### 随机梯度下降（Stochastic Gradient Descent 简称SGD）\n\n如果小批量数据中每个批量只有1个数据样本\n\n# Summary:\n\n![](https://pic2.zhimg.com/80/03b3eccf18ee3760e219f9f95ec14305_hd.jpg)\n\nx,y 是给定的，weight 从一个随机开始，可以随时改变。 损失函数包含两个部分：数据损失和正则化损失，在梯度下降中，计算权重的维度实现参数的更新。\n\n# Reference：\n\n[最优化上](https://zhuanlan.zhihu.com/p/21360434)\n\n[最优化下](https://zhuanlan.zhihu.com/p/21387326)\n","slug":"cs231n-note-3","published":1,"updated":"2018-06-19T04:21:53.413Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiekzz000jim0raky08wqw","content":"<h1 id=\"Review\"><a href=\"#Review\" class=\"headerlink\" title=\"Review\"></a>Review</h1><ul>\n<li><p>基于参数的评分函数。该函数将原始图像像素映射为分类评分值。</p>\n</li>\n<li><p>损失函数。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。 <em>Softmax 和 SVM 不应该包括了score function 和 lost function</em></p>\n</li>\n<li><p>最优化Optimization。最优化是寻找能使得损失函数值最小化的参数W的过程。</p>\n</li>\n</ul>\n<h1 id=\"损失函数可视化\"><a href=\"#损失函数可视化\" class=\"headerlink\" title=\"损失函数可视化\"></a>损失函数可视化</h1><ul>\n<li><p>在一维尺度上 $$L(W+aW_1)$$ x轴是a， y轴是loss function</p>\n</li>\n<li><p>在二维尺度上 $$L(W+aW_1+bW_2)$$, a,b 表达x轴, y轴， loss function 用颜色表示</p>\n</li>\n</ul>\n<p>单独的损失函数表示：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=Li%3D%5Csum_%7Bj%5Cnot%3Dy_i%7D%5Bmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B1%29%5D\" alt=\"\"></p>\n<p>$$w_j$$ 如果对应分类正确即是负号，错误即是正号 $$L = \\sumL/n$$</p>\n<p><img src=\"https://pic3.zhimg.com/80/3f6fbcd487b1c214e8fea1ea66eb413e_hd.jpg\" alt=\"\"></p>\n<p>SVM的损失函数是一种凸函数，可以学习一下如何高效最小化凸函数，在这种损失函数会有一些不可导的点（kinks）</p>\n<p>两个新概念： </p>\n<ul>\n<li>梯度？ 次梯度？未来学习点</li>\n</ul>\n<h1 id=\"最优化-Optimization\"><a href=\"#最优化-Optimization\" class=\"headerlink\" title=\"最优化 Optimization\"></a>最优化 Optimization</h1><p>对于神经网络的最优化策略有：</p>\n<h2 id=\"策略-1：随即搜索-最差劲的搜索方案（base-line）\"><a href=\"#策略-1：随即搜索-最差劲的搜索方案（base-line）\" class=\"headerlink\" title=\"策略#1：随即搜索 最差劲的搜索方案（base line）\"></a>策略#1：随即搜索 最差劲的搜索方案（base line）</h2><pre><code># 假设X_train的每一列都是一个数据样本（比如3073 x 50000）\n# 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）\n# 假设函数L对损失函数进行评价\n\nbestloss = float(&quot;inf&quot;) # Python assigns the highest possible float value\nfor num in xrange(1000):\n  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n  if loss &lt; bestloss: # keep track of the best solution\n    bestloss = loss\n    bestW = W\n  print &apos;in attempt %d the loss was %f, best %f&apos; % (num, loss, bestloss)\n</code></pre><p>感觉跟那个monkey sort 差不多 随机生成W weight。</p>\n<pre><code># 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]\nscores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples\n# 找到在每列中评分值最大的索引（即预测的分类）\nYte_predict = np.argmax(scores, axis = 0)\n# 以及计算准确率\nnp.mean(Yte_predict == Yte)\n# 返回 0.1555\n</code></pre><p>策略是：随机权重开始，然后迭代取优，从而获得更低的损失值。</p>\n<h2 id=\"策略-2：随机本地搜索\"><a href=\"#策略-2：随机本地搜索\" class=\"headerlink\" title=\"策略#2：随机本地搜索\"></a>策略#2：随机本地搜索</h2><p>生成一个随机的扰动 $$ \\delta W $$</p>\n<p>$$ Wtry = W + \\delta W $$ </p>\n<p>当 Wtry 的loss 变小的时候， 才决定移动</p>\n<pre><code>W = np.random.randn(10, 3073) * 0.001 # 生成随机初始W\nbestloss = float(&quot;inf&quot;)\nfor i in xrange(1000):\n  step_size = 0.0001\n  Wtry = W + np.random.randn(10, 3073) * step_size\n  loss = L(Xtr_cols, Ytr, Wtry)\n  if loss &lt; bestloss:\n    W = Wtry\n    bestloss = loss\n  print &apos;iter %d loss is %f&apos; % (i, bestloss)\n</code></pre><h2 id=\"策略-3：跟随梯度\"><a href=\"#策略-3：跟随梯度\" class=\"headerlink\" title=\"策略#3：跟随梯度\"></a>策略#3：跟随梯度</h2><p>策略1 和 策略2 都是尝试好几个方向来找减少loss的方向，其实可以用梯度（gradient）来找到最陡峭的方向减少loss，</p>\n<p>一维求导公式： d(fx)/dx</p>\n<p>当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。</p>\n<h1 id=\"梯度计算\"><a href=\"#梯度计算\" class=\"headerlink\" title=\"梯度计算\"></a>梯度计算</h1><p>有两种方法计算梯度：</p>\n<h2 id=\"数值梯度法-（实现简单-但是缓慢）\"><a href=\"#数值梯度法-（实现简单-但是缓慢）\" class=\"headerlink\" title=\"数值梯度法 （实现简单 但是缓慢）\"></a>数值梯度法 （实现简单 但是缓慢）</h2><pre><code>def eval_numerical_gradient(f, x):\n  &quot;&quot;&quot;  \n  一个f在x处的数值梯度法的简单实现\n  - f是只有一个参数的函数\n  - x是计算梯度的点\n  &quot;&quot;&quot; \n\n  fx = f(x) # 在原点计算函数值\n  grad = np.zeros(x.shape)\n  h = 0.00001\n\n  # 对x中所有的item索引进行迭代\n  it = np.nditer(x, flags=[&apos;multi_index&apos;], op_flags=[&apos;readwrite&apos;])\n  while not it.finished:\n\n    # 计算x+h处的函数值\n    ix = it.multi_index\n    old_value = x[ix]\n    x[ix] = old_value + h # 增加h\n    fxh = f(x) # 计算f(x + h)\n    x[ix] = old_value # 存到前一个值中 (非常重要)\n\n    # 计算偏导数\n    grad[ix] = (fxh - fx) / h # 坡度\n    it.iternext() # 到下个维度\n\n  return grad\n</code></pre><p>实际中用中心差值公式（centered difference formula）[f(x+h)-f(x-h)]/2h 效果较好 <a href=\"https://en.wikipedia.org/wiki/Numerical_differentiation\" target=\"_blank\" rel=\"noopener\">Numerical_differentiation</a></p>\n<pre><code># 要使用上面的代码我们需要一个只有一个参数的函数\n# (在这里参数就是权重)所以也包含了X_train和Y_train\ndef CIFAR10_loss_fun(W):\n  return L(X_train, Y_train, W)\n\nW = np.random.rand(10, 3073) * 0.001 # 随机权重向量\ndf = eval_numerical_gradient(CIFAR10_loss_fun, W) # 得到梯度\n</code></pre><p>loss_original = CIFAR10_loss_fun(W) # 初始损失值<br>print ‘original loss: %f’ % (loss_original, )</p>\n<pre><code># 查看不同步长的效果\nfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n  step_size = 10 ** step_size_log\n  W_new = W - step_size * df # 权重空间中的新位置\n  loss_new = CIFAR10_loss_fun(W_new)\n  print &apos;for step size %f new loss: %f&apos; % (step_size, loss_new)\n\n# 输出:\n# original loss: 2.200718\n# for step size 1.000000e-10 new loss: 2.200652\n# for step size 1.000000e-09 new loss: 2.200057\n# for step size 1.000000e-08 new loss: 2.194116\n# for step size 1.000000e-07 new loss: 2.135493\n# for step size 1.000000e-06 new loss: 1.647802\n# for step size 1.000000e-05 new loss: 2.844355\n# for step size 1.000000e-04 new loss: 25.558142\n# for step size 1.000000e-03 new loss: 254.086573\n# for step size 1.000000e-02 new loss: 2539.370888\n# for step size 1.000000e-01 new loss: 25392.214036\n</code></pre><p>步长的影响：梯度指明了函数在哪个方向?是变化率最大的 步长(也叫作学习率)</p>\n<p>小步长下降稳定但进度慢 &lt;-&gt; 大步长进展快但是风险更大</p>\n<p>在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度, 效率太低</p>\n<h2 id=\"分析梯度法-（计算迅速，结果精确）-微分分析计算梯度\"><a href=\"#分析梯度法-（计算迅速，结果精确）-微分分析计算梯度\" class=\"headerlink\" title=\"分析梯度法 （计算迅速，结果精确） 微分分析计算梯度\"></a>分析梯度法 （计算迅速，结果精确） 微分分析计算梯度</h2><p>用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错. 于是我们需要将分析梯度法的结果于数值梯度法作比较， 这个步骤叫做梯度检查。</p>\n<p>eg: SVM lossfunction</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=L_i%3D%5Cdisplaystyle%5Csum_%7Bj%5Cnot+%3Dy_i%7D%5Bmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%29%5D\" alt=\"\"></p>\n<p>对W_yi 进行微分</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cnabla_%7Bw_%7By_i%7D%7DL_i%3D-%28%5Csum_%7Bj%5Cnot%3Dy_i%7D1%28w%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%3E0%29%29x_i\" alt=\"\"></p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><h4 id=\"普通版本\"><a href=\"#普通版本\" class=\"headerlink\" title=\"普通版本\"></a>普通版本</h4><pre><code># 普通的梯度下降\nwhile True:\n  weights_grad = evaluate_gradient(loss_fun, data, weights)\n  weights += - step_size * weights_grad # 进行梯度更新\n</code></pre><h4 id=\"小批量数据梯度下降（Mini-batch-gradient-descent）\"><a href=\"#小批量数据梯度下降（Mini-batch-gradient-descent）\" class=\"headerlink\" title=\"小批量数据梯度下降（Mini-batch gradient descent）\"></a>小批量数据梯度下降（Mini-batch gradient descent）</h4><p>每次小子集往下减少，小批量数据的梯度就是对整个数据集梯度的一个近似， 需要data的数量远大于小批数量</p>\n<pre><code># 普通的小批量数据梯度下降\nwhile True:\n  data_batch = sample_training_data(data, 256) # 256个数据\n  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n  weights += - step_size * weights_grad # 参数更新\n</code></pre><h4 id=\"随机梯度下降（Stochastic-Gradient-Descent-简称SGD）\"><a href=\"#随机梯度下降（Stochastic-Gradient-Descent-简称SGD）\" class=\"headerlink\" title=\"随机梯度下降（Stochastic Gradient Descent 简称SGD）\"></a>随机梯度下降（Stochastic Gradient Descent 简称SGD）</h4><p>如果小批量数据中每个批量只有1个数据样本</p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary:\"></a>Summary:</h1><p><img src=\"https://pic2.zhimg.com/80/03b3eccf18ee3760e219f9f95ec14305_hd.jpg\" alt=\"\"></p>\n<p>x,y 是给定的，weight 从一个随机开始，可以随时改变。 损失函数包含两个部分：数据损失和正则化损失，在梯度下降中，计算权重的维度实现参数的更新。</p>\n<h1 id=\"Reference：\"><a href=\"#Reference：\" class=\"headerlink\" title=\"Reference：\"></a>Reference：</h1><p><a href=\"https://zhuanlan.zhihu.com/p/21360434\" target=\"_blank\" rel=\"noopener\">最优化上</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/21387326\" target=\"_blank\" rel=\"noopener\">最优化下</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Review\"><a href=\"#Review\" class=\"headerlink\" title=\"Review\"></a>Review</h1><ul>\n<li><p>基于参数的评分函数。该函数将原始图像像素映射为分类评分值。</p>\n</li>\n<li><p>损失函数。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。 <em>Softmax 和 SVM 不应该包括了score function 和 lost function</em></p>\n</li>\n<li><p>最优化Optimization。最优化是寻找能使得损失函数值最小化的参数W的过程。</p>\n</li>\n</ul>\n<h1 id=\"损失函数可视化\"><a href=\"#损失函数可视化\" class=\"headerlink\" title=\"损失函数可视化\"></a>损失函数可视化</h1><ul>\n<li><p>在一维尺度上 $$L(W+aW_1)$$ x轴是a， y轴是loss function</p>\n</li>\n<li><p>在二维尺度上 $$L(W+aW_1+bW_2)$$, a,b 表达x轴, y轴， loss function 用颜色表示</p>\n</li>\n</ul>\n<p>单独的损失函数表示：</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=Li%3D%5Csum_%7Bj%5Cnot%3Dy_i%7D%5Bmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B1%29%5D\" alt=\"\"></p>\n<p>$$w_j$$ 如果对应分类正确即是负号，错误即是正号 $$L = \\sumL/n$$</p>\n<p><img src=\"https://pic3.zhimg.com/80/3f6fbcd487b1c214e8fea1ea66eb413e_hd.jpg\" alt=\"\"></p>\n<p>SVM的损失函数是一种凸函数，可以学习一下如何高效最小化凸函数，在这种损失函数会有一些不可导的点（kinks）</p>\n<p>两个新概念： </p>\n<ul>\n<li>梯度？ 次梯度？未来学习点</li>\n</ul>\n<h1 id=\"最优化-Optimization\"><a href=\"#最优化-Optimization\" class=\"headerlink\" title=\"最优化 Optimization\"></a>最优化 Optimization</h1><p>对于神经网络的最优化策略有：</p>\n<h2 id=\"策略-1：随即搜索-最差劲的搜索方案（base-line）\"><a href=\"#策略-1：随即搜索-最差劲的搜索方案（base-line）\" class=\"headerlink\" title=\"策略#1：随即搜索 最差劲的搜索方案（base line）\"></a>策略#1：随即搜索 最差劲的搜索方案（base line）</h2><pre><code># 假设X_train的每一列都是一个数据样本（比如3073 x 50000）\n# 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）\n# 假设函数L对损失函数进行评价\n\nbestloss = float(&quot;inf&quot;) # Python assigns the highest possible float value\nfor num in xrange(1000):\n  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n  if loss &lt; bestloss: # keep track of the best solution\n    bestloss = loss\n    bestW = W\n  print &apos;in attempt %d the loss was %f, best %f&apos; % (num, loss, bestloss)\n</code></pre><p>感觉跟那个monkey sort 差不多 随机生成W weight。</p>\n<pre><code># 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]\nscores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples\n# 找到在每列中评分值最大的索引（即预测的分类）\nYte_predict = np.argmax(scores, axis = 0)\n# 以及计算准确率\nnp.mean(Yte_predict == Yte)\n# 返回 0.1555\n</code></pre><p>策略是：随机权重开始，然后迭代取优，从而获得更低的损失值。</p>\n<h2 id=\"策略-2：随机本地搜索\"><a href=\"#策略-2：随机本地搜索\" class=\"headerlink\" title=\"策略#2：随机本地搜索\"></a>策略#2：随机本地搜索</h2><p>生成一个随机的扰动 $$ \\delta W $$</p>\n<p>$$ Wtry = W + \\delta W $$ </p>\n<p>当 Wtry 的loss 变小的时候， 才决定移动</p>\n<pre><code>W = np.random.randn(10, 3073) * 0.001 # 生成随机初始W\nbestloss = float(&quot;inf&quot;)\nfor i in xrange(1000):\n  step_size = 0.0001\n  Wtry = W + np.random.randn(10, 3073) * step_size\n  loss = L(Xtr_cols, Ytr, Wtry)\n  if loss &lt; bestloss:\n    W = Wtry\n    bestloss = loss\n  print &apos;iter %d loss is %f&apos; % (i, bestloss)\n</code></pre><h2 id=\"策略-3：跟随梯度\"><a href=\"#策略-3：跟随梯度\" class=\"headerlink\" title=\"策略#3：跟随梯度\"></a>策略#3：跟随梯度</h2><p>策略1 和 策略2 都是尝试好几个方向来找减少loss的方向，其实可以用梯度（gradient）来找到最陡峭的方向减少loss，</p>\n<p>一维求导公式： d(fx)/dx</p>\n<p>当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。</p>\n<h1 id=\"梯度计算\"><a href=\"#梯度计算\" class=\"headerlink\" title=\"梯度计算\"></a>梯度计算</h1><p>有两种方法计算梯度：</p>\n<h2 id=\"数值梯度法-（实现简单-但是缓慢）\"><a href=\"#数值梯度法-（实现简单-但是缓慢）\" class=\"headerlink\" title=\"数值梯度法 （实现简单 但是缓慢）\"></a>数值梯度法 （实现简单 但是缓慢）</h2><pre><code>def eval_numerical_gradient(f, x):\n  &quot;&quot;&quot;  \n  一个f在x处的数值梯度法的简单实现\n  - f是只有一个参数的函数\n  - x是计算梯度的点\n  &quot;&quot;&quot; \n\n  fx = f(x) # 在原点计算函数值\n  grad = np.zeros(x.shape)\n  h = 0.00001\n\n  # 对x中所有的item索引进行迭代\n  it = np.nditer(x, flags=[&apos;multi_index&apos;], op_flags=[&apos;readwrite&apos;])\n  while not it.finished:\n\n    # 计算x+h处的函数值\n    ix = it.multi_index\n    old_value = x[ix]\n    x[ix] = old_value + h # 增加h\n    fxh = f(x) # 计算f(x + h)\n    x[ix] = old_value # 存到前一个值中 (非常重要)\n\n    # 计算偏导数\n    grad[ix] = (fxh - fx) / h # 坡度\n    it.iternext() # 到下个维度\n\n  return grad\n</code></pre><p>实际中用中心差值公式（centered difference formula）[f(x+h)-f(x-h)]/2h 效果较好 <a href=\"https://en.wikipedia.org/wiki/Numerical_differentiation\" target=\"_blank\" rel=\"noopener\">Numerical_differentiation</a></p>\n<pre><code># 要使用上面的代码我们需要一个只有一个参数的函数\n# (在这里参数就是权重)所以也包含了X_train和Y_train\ndef CIFAR10_loss_fun(W):\n  return L(X_train, Y_train, W)\n\nW = np.random.rand(10, 3073) * 0.001 # 随机权重向量\ndf = eval_numerical_gradient(CIFAR10_loss_fun, W) # 得到梯度\n</code></pre><p>loss_original = CIFAR10_loss_fun(W) # 初始损失值<br>print ‘original loss: %f’ % (loss_original, )</p>\n<pre><code># 查看不同步长的效果\nfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n  step_size = 10 ** step_size_log\n  W_new = W - step_size * df # 权重空间中的新位置\n  loss_new = CIFAR10_loss_fun(W_new)\n  print &apos;for step size %f new loss: %f&apos; % (step_size, loss_new)\n\n# 输出:\n# original loss: 2.200718\n# for step size 1.000000e-10 new loss: 2.200652\n# for step size 1.000000e-09 new loss: 2.200057\n# for step size 1.000000e-08 new loss: 2.194116\n# for step size 1.000000e-07 new loss: 2.135493\n# for step size 1.000000e-06 new loss: 1.647802\n# for step size 1.000000e-05 new loss: 2.844355\n# for step size 1.000000e-04 new loss: 25.558142\n# for step size 1.000000e-03 new loss: 254.086573\n# for step size 1.000000e-02 new loss: 2539.370888\n# for step size 1.000000e-01 new loss: 25392.214036\n</code></pre><p>步长的影响：梯度指明了函数在哪个方向?是变化率最大的 步长(也叫作学习率)</p>\n<p>小步长下降稳定但进度慢 &lt;-&gt; 大步长进展快但是风险更大</p>\n<p>在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度, 效率太低</p>\n<h2 id=\"分析梯度法-（计算迅速，结果精确）-微分分析计算梯度\"><a href=\"#分析梯度法-（计算迅速，结果精确）-微分分析计算梯度\" class=\"headerlink\" title=\"分析梯度法 （计算迅速，结果精确） 微分分析计算梯度\"></a>分析梯度法 （计算迅速，结果精确） 微分分析计算梯度</h2><p>用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错. 于是我们需要将分析梯度法的结果于数值梯度法作比较， 这个步骤叫做梯度检查。</p>\n<p>eg: SVM lossfunction</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=L_i%3D%5Cdisplaystyle%5Csum_%7Bj%5Cnot+%3Dy_i%7D%5Bmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%29%5D\" alt=\"\"></p>\n<p>对W_yi 进行微分</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cnabla_%7Bw_%7By_i%7D%7DL_i%3D-%28%5Csum_%7Bj%5Cnot%3Dy_i%7D1%28w%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%3E0%29%29x_i\" alt=\"\"></p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><h4 id=\"普通版本\"><a href=\"#普通版本\" class=\"headerlink\" title=\"普通版本\"></a>普通版本</h4><pre><code># 普通的梯度下降\nwhile True:\n  weights_grad = evaluate_gradient(loss_fun, data, weights)\n  weights += - step_size * weights_grad # 进行梯度更新\n</code></pre><h4 id=\"小批量数据梯度下降（Mini-batch-gradient-descent）\"><a href=\"#小批量数据梯度下降（Mini-batch-gradient-descent）\" class=\"headerlink\" title=\"小批量数据梯度下降（Mini-batch gradient descent）\"></a>小批量数据梯度下降（Mini-batch gradient descent）</h4><p>每次小子集往下减少，小批量数据的梯度就是对整个数据集梯度的一个近似， 需要data的数量远大于小批数量</p>\n<pre><code># 普通的小批量数据梯度下降\nwhile True:\n  data_batch = sample_training_data(data, 256) # 256个数据\n  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n  weights += - step_size * weights_grad # 参数更新\n</code></pre><h4 id=\"随机梯度下降（Stochastic-Gradient-Descent-简称SGD）\"><a href=\"#随机梯度下降（Stochastic-Gradient-Descent-简称SGD）\" class=\"headerlink\" title=\"随机梯度下降（Stochastic Gradient Descent 简称SGD）\"></a>随机梯度下降（Stochastic Gradient Descent 简称SGD）</h4><p>如果小批量数据中每个批量只有1个数据样本</p>\n<h1 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary:\"></a>Summary:</h1><p><img src=\"https://pic2.zhimg.com/80/03b3eccf18ee3760e219f9f95ec14305_hd.jpg\" alt=\"\"></p>\n<p>x,y 是给定的，weight 从一个随机开始，可以随时改变。 损失函数包含两个部分：数据损失和正则化损失，在梯度下降中，计算权重的维度实现参数的更新。</p>\n<h1 id=\"Reference：\"><a href=\"#Reference：\" class=\"headerlink\" title=\"Reference：\"></a>Reference：</h1><p><a href=\"https://zhuanlan.zhihu.com/p/21360434\" target=\"_blank\" rel=\"noopener\">最优化上</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/21387326\" target=\"_blank\" rel=\"noopener\">最优化下</a></p>\n"},{"title":"cs231n-note-5","date":"2018-07-17T03:03:08.000Z","_content":"\n# 神经网络笔记1\n\n## 对比线性代数算法\n\n基础的线性代数算法 s = Wx, x 是一个输入矩阵 [n * 1], W 是一个权重矩阵 [种类 * n]，s 为一个评分矩阵。\n\n## 神经网络算法简介\n\n神经网络算法是 s = W_2 max(0, W_1x). 在这种情况下 W_1 广度增加，比如可能为 [100 * n], max 是通过设置熵值来过滤掉所有的小于0的score。其实有其他方法来过滤。\n\nW_2 是一个[种类 * 100] 的矩阵， 权重W 通过梯度下降来学习。\n\n## 单个神经元\t建模\n\n神经网络是从生物上得到的启发。 \n\n#### 生物动机与连接\n\n!()[https://pic4.zhimg.com/80/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg]\n\n当多个信号传进神经元，与神经元内权重相乘并且相加，如果超过某一个阈值，那么激活神经元。激活函数最早接触的sigmoid，最大优势是将数据控制在【0，1】之间。\n\n一个神经元前向传播的代码是：\n\n```\nclass Neuron(object):\n\t# ... \n\tdef forward(inputs):\n\t  \"\"\" 假设输入和权重是1-D的numpy数组，偏差是一个数字 \"\"\"\n\t  cell_body_sum = np.sum(inputs * self.weights) + self.bias\n\t  firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid激活函数\n\t  return firing_rate\n```\n\n---- Todo ----- \n\n## 作为线性分类器的单个神经元\n\n分类器我的理解还是吧score 转化为正确概率。然后进行一个loss 的计算, 前面有公式\n\n#### 二分类Softmax分类器\n\n#### 二分类SVM分类器\n\n|    分类器     | 公式  | 优点    | 缺点 |\n| ------------ | ---- | ---------- | --- |\n| Softmax分类器 |  softmax函数 | 和为1 |  较慢的学习曲线，所有的结果都有loss |\n| SVM分类器     |  在0和得分差中选最大值 | 更快的去除一些失误值 | 有时候学习会卡住 |\n\n## 常用激活函数\n\n我的理解，激活函数就是通过score 得到概率。\n\n![](https://pic3.zhimg.com/80/677187e96671a4cac9c95352743b3806_hd.jpg)\n\n![](https://pic1.zhimg.com/80/83682a138f6224230f5b0292d9c01bd2_hd.jpg)\n\n|    激活函数     | 优点    | 缺点 |\n| ------------ | ---------- | --- |\n| Sigmoid | 简单，易理解 | Sigmoid函数饱和使梯度消失，输出不是零中心的 |\n| Tanh  | 输出不是零中心的，易理解 |  有时候会 |\n| ReLU | 1. 线性，非饱和的公式，随机梯度下降的收敛有巨大的加速 2，耗费较少计算资源的操作 | 学习率太高的时候，这个ReLU单元在训练中将不可逆转的死亡 |\n| Leaky ReLU | 同上，并且解决了ReLu单元死亡的问题 |  然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰（不太懂） |\n| Maxout | 有以上所有的优点 |  参数过多 |\n\n## 神经网络结构\n\n最普通的层的类型是全连接层（fully-connected layer）\n\n- 命名规则 N层神经网络 = hidden layer + output layer\n\n- 输出层 大多用于表示分类评分值\n\n- 网络尺寸 标准主要有两个：一个是神经元的个数，另一个是参数的个数\n\n\n## 前向传播\n\n```\n\t# 一个3层神经网络的前向传播:\n\tf = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)\n\tx = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)\n\th1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)\n\th2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)\n\tout = np.dot(W3, h2) + b3 # 神经元输出(1x1)\n```\n\n神经网络最后一层通常是没有激活函数的, 得出一个实数值的评分\n\n## 表达能力\n\n至少拥有一个隐层(hidden)的神经网络是一个通用的近似器，神经网络可以近似任何连续函数。\n\n实践而言，构建更多层的神经网络所表达出来的函数不仅平滑，而且更容易学习（利用最优化）\n\n## 设置层的数量和尺寸\n\n![每层的神经元数目不同：只有一个隐层](https://pic4.zhimg.com/80/cf3fc543bf1dc81e2083530a4492b0ec_hd.jpg)\n\n更大的神经网络可以表达出更复杂的函数,但是缺点是过拟合(overfitting),只是重视数据在复杂情况中的分类，而忽略了潜在关系。\n\n这时候合适的layer可以在测试数据里获得更好的泛化(generalization)能力\n\n\n![不同正则化强度控制过拟合](https://pic3.zhimg.com/80/4f8af027d6059549d160199a1717df14_hd.jpg)\n\n然而, 防止神经网络过拟合的方法有很多(To learn), 选择其他过拟合的解决方法，而不应该去选择小的神经网络。\n\n这个是提供的测试的链接[convnetjs DEMO](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)\n\n## Reference\n\n[convnetjs DEMO](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)\n\n[神经网络笔记1（上）](https://zhuanlan.zhihu.com/p/21462488)\n\n[神经网络笔记1（下）](https://zhuanlan.zhihu.com/p/21513367)","source":"_posts/cs231n-note-5.md","raw":"---\ntitle: cs231n-note-5\ndate: 2018-07-16 21:03:08\ntags:\n---\n\n# 神经网络笔记1\n\n## 对比线性代数算法\n\n基础的线性代数算法 s = Wx, x 是一个输入矩阵 [n * 1], W 是一个权重矩阵 [种类 * n]，s 为一个评分矩阵。\n\n## 神经网络算法简介\n\n神经网络算法是 s = W_2 max(0, W_1x). 在这种情况下 W_1 广度增加，比如可能为 [100 * n], max 是通过设置熵值来过滤掉所有的小于0的score。其实有其他方法来过滤。\n\nW_2 是一个[种类 * 100] 的矩阵， 权重W 通过梯度下降来学习。\n\n## 单个神经元\t建模\n\n神经网络是从生物上得到的启发。 \n\n#### 生物动机与连接\n\n!()[https://pic4.zhimg.com/80/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg]\n\n当多个信号传进神经元，与神经元内权重相乘并且相加，如果超过某一个阈值，那么激活神经元。激活函数最早接触的sigmoid，最大优势是将数据控制在【0，1】之间。\n\n一个神经元前向传播的代码是：\n\n```\nclass Neuron(object):\n\t# ... \n\tdef forward(inputs):\n\t  \"\"\" 假设输入和权重是1-D的numpy数组，偏差是一个数字 \"\"\"\n\t  cell_body_sum = np.sum(inputs * self.weights) + self.bias\n\t  firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid激活函数\n\t  return firing_rate\n```\n\n---- Todo ----- \n\n## 作为线性分类器的单个神经元\n\n分类器我的理解还是吧score 转化为正确概率。然后进行一个loss 的计算, 前面有公式\n\n#### 二分类Softmax分类器\n\n#### 二分类SVM分类器\n\n|    分类器     | 公式  | 优点    | 缺点 |\n| ------------ | ---- | ---------- | --- |\n| Softmax分类器 |  softmax函数 | 和为1 |  较慢的学习曲线，所有的结果都有loss |\n| SVM分类器     |  在0和得分差中选最大值 | 更快的去除一些失误值 | 有时候学习会卡住 |\n\n## 常用激活函数\n\n我的理解，激活函数就是通过score 得到概率。\n\n![](https://pic3.zhimg.com/80/677187e96671a4cac9c95352743b3806_hd.jpg)\n\n![](https://pic1.zhimg.com/80/83682a138f6224230f5b0292d9c01bd2_hd.jpg)\n\n|    激活函数     | 优点    | 缺点 |\n| ------------ | ---------- | --- |\n| Sigmoid | 简单，易理解 | Sigmoid函数饱和使梯度消失，输出不是零中心的 |\n| Tanh  | 输出不是零中心的，易理解 |  有时候会 |\n| ReLU | 1. 线性，非饱和的公式，随机梯度下降的收敛有巨大的加速 2，耗费较少计算资源的操作 | 学习率太高的时候，这个ReLU单元在训练中将不可逆转的死亡 |\n| Leaky ReLU | 同上，并且解决了ReLu单元死亡的问题 |  然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰（不太懂） |\n| Maxout | 有以上所有的优点 |  参数过多 |\n\n## 神经网络结构\n\n最普通的层的类型是全连接层（fully-connected layer）\n\n- 命名规则 N层神经网络 = hidden layer + output layer\n\n- 输出层 大多用于表示分类评分值\n\n- 网络尺寸 标准主要有两个：一个是神经元的个数，另一个是参数的个数\n\n\n## 前向传播\n\n```\n\t# 一个3层神经网络的前向传播:\n\tf = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)\n\tx = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)\n\th1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)\n\th2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)\n\tout = np.dot(W3, h2) + b3 # 神经元输出(1x1)\n```\n\n神经网络最后一层通常是没有激活函数的, 得出一个实数值的评分\n\n## 表达能力\n\n至少拥有一个隐层(hidden)的神经网络是一个通用的近似器，神经网络可以近似任何连续函数。\n\n实践而言，构建更多层的神经网络所表达出来的函数不仅平滑，而且更容易学习（利用最优化）\n\n## 设置层的数量和尺寸\n\n![每层的神经元数目不同：只有一个隐层](https://pic4.zhimg.com/80/cf3fc543bf1dc81e2083530a4492b0ec_hd.jpg)\n\n更大的神经网络可以表达出更复杂的函数,但是缺点是过拟合(overfitting),只是重视数据在复杂情况中的分类，而忽略了潜在关系。\n\n这时候合适的layer可以在测试数据里获得更好的泛化(generalization)能力\n\n\n![不同正则化强度控制过拟合](https://pic3.zhimg.com/80/4f8af027d6059549d160199a1717df14_hd.jpg)\n\n然而, 防止神经网络过拟合的方法有很多(To learn), 选择其他过拟合的解决方法，而不应该去选择小的神经网络。\n\n这个是提供的测试的链接[convnetjs DEMO](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)\n\n## Reference\n\n[convnetjs DEMO](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)\n\n[神经网络笔记1（上）](https://zhuanlan.zhihu.com/p/21462488)\n\n[神经网络笔记1（下）](https://zhuanlan.zhihu.com/p/21513367)","slug":"cs231n-note-5","published":1,"updated":"2018-07-29T20:52:08.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiel01000mim0rtl565n0j","content":"<h1 id=\"神经网络笔记1\"><a href=\"#神经网络笔记1\" class=\"headerlink\" title=\"神经网络笔记1\"></a>神经网络笔记1</h1><h2 id=\"对比线性代数算法\"><a href=\"#对比线性代数算法\" class=\"headerlink\" title=\"对比线性代数算法\"></a>对比线性代数算法</h2><p>基础的线性代数算法 s = Wx, x 是一个输入矩阵 [n <em> 1], W 是一个权重矩阵 [种类 </em> n]，s 为一个评分矩阵。</p>\n<h2 id=\"神经网络算法简介\"><a href=\"#神经网络算法简介\" class=\"headerlink\" title=\"神经网络算法简介\"></a>神经网络算法简介</h2><p>神经网络算法是 s = W_2 max(0, W_1x). 在这种情况下 W_1 广度增加，比如可能为 [100 * n], max 是通过设置熵值来过滤掉所有的小于0的score。其实有其他方法来过滤。</p>\n<p>W_2 是一个[种类 * 100] 的矩阵， 权重W 通过梯度下降来学习。</p>\n<h2 id=\"单个神经元-建模\"><a href=\"#单个神经元-建模\" class=\"headerlink\" title=\"单个神经元    建模\"></a>单个神经元    建模</h2><p>神经网络是从生物上得到的启发。 </p>\n<h4 id=\"生物动机与连接\"><a href=\"#生物动机与连接\" class=\"headerlink\" title=\"生物动机与连接\"></a>生物动机与连接</h4><p>!()[<a href=\"https://pic4.zhimg.com/80/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg\" target=\"_blank\" rel=\"noopener\">https://pic4.zhimg.com/80/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg</a>]</p>\n<p>当多个信号传进神经元，与神经元内权重相乘并且相加，如果超过某一个阈值，那么激活神经元。激活函数最早接触的sigmoid，最大优势是将数据控制在【0，1】之间。</p>\n<p>一个神经元前向传播的代码是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Neuron(object):</span><br><span class=\"line\">\t# ... </span><br><span class=\"line\">\tdef forward(inputs):</span><br><span class=\"line\">\t  &quot;&quot;&quot; 假设输入和权重是1-D的numpy数组，偏差是一个数字 &quot;&quot;&quot;</span><br><span class=\"line\">\t  cell_body_sum = np.sum(inputs * self.weights) + self.bias</span><br><span class=\"line\">\t  firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid激活函数</span><br><span class=\"line\">\t  return firing_rate</span><br></pre></td></tr></table></figure>\n<p>—- Todo —– </p>\n<h2 id=\"作为线性分类器的单个神经元\"><a href=\"#作为线性分类器的单个神经元\" class=\"headerlink\" title=\"作为线性分类器的单个神经元\"></a>作为线性分类器的单个神经元</h2><p>分类器我的理解还是吧score 转化为正确概率。然后进行一个loss 的计算, 前面有公式</p>\n<h4 id=\"二分类Softmax分类器\"><a href=\"#二分类Softmax分类器\" class=\"headerlink\" title=\"二分类Softmax分类器\"></a>二分类Softmax分类器</h4><h4 id=\"二分类SVM分类器\"><a href=\"#二分类SVM分类器\" class=\"headerlink\" title=\"二分类SVM分类器\"></a>二分类SVM分类器</h4><table>\n<thead>\n<tr>\n<th>分类器</th>\n<th>公式</th>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Softmax分类器</td>\n<td>softmax函数</td>\n<td>和为1</td>\n<td>较慢的学习曲线，所有的结果都有loss</td>\n</tr>\n<tr>\n<td>SVM分类器</td>\n<td>在0和得分差中选最大值</td>\n<td>更快的去除一些失误值</td>\n<td>有时候学习会卡住</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"常用激活函数\"><a href=\"#常用激活函数\" class=\"headerlink\" title=\"常用激活函数\"></a>常用激活函数</h2><p>我的理解，激活函数就是通过score 得到概率。</p>\n<p><img src=\"https://pic3.zhimg.com/80/677187e96671a4cac9c95352743b3806_hd.jpg\" alt=\"\"></p>\n<p><img src=\"https://pic1.zhimg.com/80/83682a138f6224230f5b0292d9c01bd2_hd.jpg\" alt=\"\"></p>\n<table>\n<thead>\n<tr>\n<th>激活函数</th>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sigmoid</td>\n<td>简单，易理解</td>\n<td>Sigmoid函数饱和使梯度消失，输出不是零中心的</td>\n</tr>\n<tr>\n<td>Tanh</td>\n<td>输出不是零中心的，易理解</td>\n<td>有时候会</td>\n</tr>\n<tr>\n<td>ReLU</td>\n<td>1. 线性，非饱和的公式，随机梯度下降的收敛有巨大的加速 2，耗费较少计算资源的操作</td>\n<td>学习率太高的时候，这个ReLU单元在训练中将不可逆转的死亡</td>\n</tr>\n<tr>\n<td>Leaky ReLU</td>\n<td>同上，并且解决了ReLu单元死亡的问题</td>\n<td>然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰（不太懂）</td>\n</tr>\n<tr>\n<td>Maxout</td>\n<td>有以上所有的优点</td>\n<td>参数过多</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"神经网络结构\"><a href=\"#神经网络结构\" class=\"headerlink\" title=\"神经网络结构\"></a>神经网络结构</h2><p>最普通的层的类型是全连接层（fully-connected layer）</p>\n<ul>\n<li><p>命名规则 N层神经网络 = hidden layer + output layer</p>\n</li>\n<li><p>输出层 大多用于表示分类评分值</p>\n</li>\n<li><p>网络尺寸 标准主要有两个：一个是神经元的个数，另一个是参数的个数</p>\n</li>\n</ul>\n<h2 id=\"前向传播\"><a href=\"#前向传播\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 一个3层神经网络的前向传播:</span><br><span class=\"line\">f = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)</span><br><span class=\"line\">x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)</span><br><span class=\"line\">h1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)</span><br><span class=\"line\">h2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)</span><br><span class=\"line\">out = np.dot(W3, h2) + b3 # 神经元输出(1x1)</span><br></pre></td></tr></table></figure>\n<p>神经网络最后一层通常是没有激活函数的, 得出一个实数值的评分</p>\n<h2 id=\"表达能力\"><a href=\"#表达能力\" class=\"headerlink\" title=\"表达能力\"></a>表达能力</h2><p>至少拥有一个隐层(hidden)的神经网络是一个通用的近似器，神经网络可以近似任何连续函数。</p>\n<p>实践而言，构建更多层的神经网络所表达出来的函数不仅平滑，而且更容易学习（利用最优化）</p>\n<h2 id=\"设置层的数量和尺寸\"><a href=\"#设置层的数量和尺寸\" class=\"headerlink\" title=\"设置层的数量和尺寸\"></a>设置层的数量和尺寸</h2><p><img src=\"https://pic4.zhimg.com/80/cf3fc543bf1dc81e2083530a4492b0ec_hd.jpg\" alt=\"每层的神经元数目不同：只有一个隐层\"></p>\n<p>更大的神经网络可以表达出更复杂的函数,但是缺点是过拟合(overfitting),只是重视数据在复杂情况中的分类，而忽略了潜在关系。</p>\n<p>这时候合适的layer可以在测试数据里获得更好的泛化(generalization)能力</p>\n<p><img src=\"https://pic3.zhimg.com/80/4f8af027d6059549d160199a1717df14_hd.jpg\" alt=\"不同正则化强度控制过拟合\"></p>\n<p>然而, 防止神经网络过拟合的方法有很多(To learn), 选择其他过拟合的解决方法，而不应该去选择小的神经网络。</p>\n<p>这个是提供的测试的链接<a href=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html\" target=\"_blank\" rel=\"noopener\">convnetjs DEMO</a></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html\" target=\"_blank\" rel=\"noopener\">convnetjs DEMO</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/21462488\" target=\"_blank\" rel=\"noopener\">神经网络笔记1（上）</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/21513367\" target=\"_blank\" rel=\"noopener\">神经网络笔记1（下）</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"神经网络笔记1\"><a href=\"#神经网络笔记1\" class=\"headerlink\" title=\"神经网络笔记1\"></a>神经网络笔记1</h1><h2 id=\"对比线性代数算法\"><a href=\"#对比线性代数算法\" class=\"headerlink\" title=\"对比线性代数算法\"></a>对比线性代数算法</h2><p>基础的线性代数算法 s = Wx, x 是一个输入矩阵 [n <em> 1], W 是一个权重矩阵 [种类 </em> n]，s 为一个评分矩阵。</p>\n<h2 id=\"神经网络算法简介\"><a href=\"#神经网络算法简介\" class=\"headerlink\" title=\"神经网络算法简介\"></a>神经网络算法简介</h2><p>神经网络算法是 s = W_2 max(0, W_1x). 在这种情况下 W_1 广度增加，比如可能为 [100 * n], max 是通过设置熵值来过滤掉所有的小于0的score。其实有其他方法来过滤。</p>\n<p>W_2 是一个[种类 * 100] 的矩阵， 权重W 通过梯度下降来学习。</p>\n<h2 id=\"单个神经元-建模\"><a href=\"#单个神经元-建模\" class=\"headerlink\" title=\"单个神经元    建模\"></a>单个神经元    建模</h2><p>神经网络是从生物上得到的启发。 </p>\n<h4 id=\"生物动机与连接\"><a href=\"#生物动机与连接\" class=\"headerlink\" title=\"生物动机与连接\"></a>生物动机与连接</h4><p>!()[<a href=\"https://pic4.zhimg.com/80/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg\" target=\"_blank\" rel=\"noopener\">https://pic4.zhimg.com/80/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg</a>]</p>\n<p>当多个信号传进神经元，与神经元内权重相乘并且相加，如果超过某一个阈值，那么激活神经元。激活函数最早接触的sigmoid，最大优势是将数据控制在【0，1】之间。</p>\n<p>一个神经元前向传播的代码是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Neuron(object):</span><br><span class=\"line\">\t# ... </span><br><span class=\"line\">\tdef forward(inputs):</span><br><span class=\"line\">\t  &quot;&quot;&quot; 假设输入和权重是1-D的numpy数组，偏差是一个数字 &quot;&quot;&quot;</span><br><span class=\"line\">\t  cell_body_sum = np.sum(inputs * self.weights) + self.bias</span><br><span class=\"line\">\t  firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid激活函数</span><br><span class=\"line\">\t  return firing_rate</span><br></pre></td></tr></table></figure>\n<p>—- Todo —– </p>\n<h2 id=\"作为线性分类器的单个神经元\"><a href=\"#作为线性分类器的单个神经元\" class=\"headerlink\" title=\"作为线性分类器的单个神经元\"></a>作为线性分类器的单个神经元</h2><p>分类器我的理解还是吧score 转化为正确概率。然后进行一个loss 的计算, 前面有公式</p>\n<h4 id=\"二分类Softmax分类器\"><a href=\"#二分类Softmax分类器\" class=\"headerlink\" title=\"二分类Softmax分类器\"></a>二分类Softmax分类器</h4><h4 id=\"二分类SVM分类器\"><a href=\"#二分类SVM分类器\" class=\"headerlink\" title=\"二分类SVM分类器\"></a>二分类SVM分类器</h4><table>\n<thead>\n<tr>\n<th>分类器</th>\n<th>公式</th>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Softmax分类器</td>\n<td>softmax函数</td>\n<td>和为1</td>\n<td>较慢的学习曲线，所有的结果都有loss</td>\n</tr>\n<tr>\n<td>SVM分类器</td>\n<td>在0和得分差中选最大值</td>\n<td>更快的去除一些失误值</td>\n<td>有时候学习会卡住</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"常用激活函数\"><a href=\"#常用激活函数\" class=\"headerlink\" title=\"常用激活函数\"></a>常用激活函数</h2><p>我的理解，激活函数就是通过score 得到概率。</p>\n<p><img src=\"https://pic3.zhimg.com/80/677187e96671a4cac9c95352743b3806_hd.jpg\" alt=\"\"></p>\n<p><img src=\"https://pic1.zhimg.com/80/83682a138f6224230f5b0292d9c01bd2_hd.jpg\" alt=\"\"></p>\n<table>\n<thead>\n<tr>\n<th>激活函数</th>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Sigmoid</td>\n<td>简单，易理解</td>\n<td>Sigmoid函数饱和使梯度消失，输出不是零中心的</td>\n</tr>\n<tr>\n<td>Tanh</td>\n<td>输出不是零中心的，易理解</td>\n<td>有时候会</td>\n</tr>\n<tr>\n<td>ReLU</td>\n<td>1. 线性，非饱和的公式，随机梯度下降的收敛有巨大的加速 2，耗费较少计算资源的操作</td>\n<td>学习率太高的时候，这个ReLU单元在训练中将不可逆转的死亡</td>\n</tr>\n<tr>\n<td>Leaky ReLU</td>\n<td>同上，并且解决了ReLu单元死亡的问题</td>\n<td>然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰（不太懂）</td>\n</tr>\n<tr>\n<td>Maxout</td>\n<td>有以上所有的优点</td>\n<td>参数过多</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"神经网络结构\"><a href=\"#神经网络结构\" class=\"headerlink\" title=\"神经网络结构\"></a>神经网络结构</h2><p>最普通的层的类型是全连接层（fully-connected layer）</p>\n<ul>\n<li><p>命名规则 N层神经网络 = hidden layer + output layer</p>\n</li>\n<li><p>输出层 大多用于表示分类评分值</p>\n</li>\n<li><p>网络尺寸 标准主要有两个：一个是神经元的个数，另一个是参数的个数</p>\n</li>\n</ul>\n<h2 id=\"前向传播\"><a href=\"#前向传播\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 一个3层神经网络的前向传播:</span><br><span class=\"line\">f = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)</span><br><span class=\"line\">x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)</span><br><span class=\"line\">h1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)</span><br><span class=\"line\">h2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)</span><br><span class=\"line\">out = np.dot(W3, h2) + b3 # 神经元输出(1x1)</span><br></pre></td></tr></table></figure>\n<p>神经网络最后一层通常是没有激活函数的, 得出一个实数值的评分</p>\n<h2 id=\"表达能力\"><a href=\"#表达能力\" class=\"headerlink\" title=\"表达能力\"></a>表达能力</h2><p>至少拥有一个隐层(hidden)的神经网络是一个通用的近似器，神经网络可以近似任何连续函数。</p>\n<p>实践而言，构建更多层的神经网络所表达出来的函数不仅平滑，而且更容易学习（利用最优化）</p>\n<h2 id=\"设置层的数量和尺寸\"><a href=\"#设置层的数量和尺寸\" class=\"headerlink\" title=\"设置层的数量和尺寸\"></a>设置层的数量和尺寸</h2><p><img src=\"https://pic4.zhimg.com/80/cf3fc543bf1dc81e2083530a4492b0ec_hd.jpg\" alt=\"每层的神经元数目不同：只有一个隐层\"></p>\n<p>更大的神经网络可以表达出更复杂的函数,但是缺点是过拟合(overfitting),只是重视数据在复杂情况中的分类，而忽略了潜在关系。</p>\n<p>这时候合适的layer可以在测试数据里获得更好的泛化(generalization)能力</p>\n<p><img src=\"https://pic3.zhimg.com/80/4f8af027d6059549d160199a1717df14_hd.jpg\" alt=\"不同正则化强度控制过拟合\"></p>\n<p>然而, 防止神经网络过拟合的方法有很多(To learn), 选择其他过拟合的解决方法，而不应该去选择小的神经网络。</p>\n<p>这个是提供的测试的链接<a href=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html\" target=\"_blank\" rel=\"noopener\">convnetjs DEMO</a></p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html\" target=\"_blank\" rel=\"noopener\">convnetjs DEMO</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/21462488\" target=\"_blank\" rel=\"noopener\">神经网络笔记1（上）</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/21513367\" target=\"_blank\" rel=\"noopener\">神经网络笔记1（下）</a></p>\n"},{"title":"cs231n-note-4","date":"2018-06-19T04:23:21.000Z","_content":"\n# 反向传播\n\n# Introduction:\n\n反向传播是利用链式法则递归计算表达式的梯度的方法。 根据函数 f(x), 求关于f 关于 x的梯度 gradient\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c86221324a9d066ca28310ee772941748a5f370f)\n\n## 偏导数\n\n（我的理解就是当对其中一个param 求导的时候视其他的params为const， 含义为当这个param 增加的时候 对f 的增加量的比值）\n\n## 链式法则\n\nChain rule\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%7D%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+x%7D)\n\n这下面这个sample给的真的是清楚\n\nx = -2; y = 5; z = -4\n\nf(x,y,z) = (x+y)z\n\n![](https://pic4.zhimg.com/80/213da7f66594510b45989bd134fc2d8b_hd.jpg)\n\n绿色的从输入端到输出端 为前向传播\n\n红色的从输出端到输入端 为反向传播\n\n## 反向传播的理解\n\n反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：\n\n1.这个门的输出值，\n\n2.其输出值关于输入值的局部梯度。\n\n门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。\n\n因为在加法门中 是 y = a+b 所以slope 也就是梯度都是1， 在乘法门中 y = ab, 所以a 的梯度就是b， b的梯度就是a\n\n## Example \n\nw = [2,-3,-3] # 假设一些随机数据和权重\nx = [-1, -2]\n\n\t# 前向传播\n\tdot = w[0]*x[0] + w[1]*x[1] + w[2]\n\tf = 1.0 / (1 + math.exp(-dot)) # sigmoid函数\n\n\t# 对神经元反向传播\n\tddot = (1 - f) * f # 点积变量的梯度, 使用sigmoid函数求导\n\tdx = [w[0] * ddot, w[1] * ddot] # 回传到x\n\tdw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # 回传到w\n\t# 完成！得到输入的梯度\n\n\nTip:\n\n对前向传播变量进行缓存：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。\n\n在不同分支的梯度要相加：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。 比如 f（x,y） = sigmal(x) + x balabala\n\n\n## 回传流中的模式\n\n主要就是 \n\n加法门单元: 把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门把梯度2.00不变且相等地路由给了两个输入。\n\ny = 1.0 x a + 1.0 x b\n\n取最大值门单元: 对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了z变量，因为z的值比w高，于是w的梯度保持为0。\n\ny = max(high, low) = 1.0 x high + 0 x low\n\n乘法门单元: 相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.00x2.00=-8.00。\n\ny = ab, a的梯度是b\n\n\n## 用向量化操作计算梯度\n\n其实就是矩阵相乘之类，作者建议通过维度来推测计算是否正确\n\n## Summary：\n\n对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。(梯度如何计算)\n\n讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。（不难理解，但是不知道实际操作是否需要，我感觉每一层都会有自己计算梯度的方法）\n\n# Reference:\n\n[反向传播](https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit)\n\n","source":"_posts/cs231n-note-4.md","raw":"---\ntitle: cs231n-note-4\ndate: 2018-06-18 22:23:21\ntags: Machine_Learning\n---\n\n# 反向传播\n\n# Introduction:\n\n反向传播是利用链式法则递归计算表达式的梯度的方法。 根据函数 f(x), 求关于f 关于 x的梯度 gradient\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c86221324a9d066ca28310ee772941748a5f370f)\n\n## 偏导数\n\n（我的理解就是当对其中一个param 求导的时候视其他的params为const， 含义为当这个param 增加的时候 对f 的增加量的比值）\n\n## 链式法则\n\nChain rule\n\n![](https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%7D%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+x%7D)\n\n这下面这个sample给的真的是清楚\n\nx = -2; y = 5; z = -4\n\nf(x,y,z) = (x+y)z\n\n![](https://pic4.zhimg.com/80/213da7f66594510b45989bd134fc2d8b_hd.jpg)\n\n绿色的从输入端到输出端 为前向传播\n\n红色的从输出端到输入端 为反向传播\n\n## 反向传播的理解\n\n反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：\n\n1.这个门的输出值，\n\n2.其输出值关于输入值的局部梯度。\n\n门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。\n\n因为在加法门中 是 y = a+b 所以slope 也就是梯度都是1， 在乘法门中 y = ab, 所以a 的梯度就是b， b的梯度就是a\n\n## Example \n\nw = [2,-3,-3] # 假设一些随机数据和权重\nx = [-1, -2]\n\n\t# 前向传播\n\tdot = w[0]*x[0] + w[1]*x[1] + w[2]\n\tf = 1.0 / (1 + math.exp(-dot)) # sigmoid函数\n\n\t# 对神经元反向传播\n\tddot = (1 - f) * f # 点积变量的梯度, 使用sigmoid函数求导\n\tdx = [w[0] * ddot, w[1] * ddot] # 回传到x\n\tdw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # 回传到w\n\t# 完成！得到输入的梯度\n\n\nTip:\n\n对前向传播变量进行缓存：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。\n\n在不同分支的梯度要相加：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。 比如 f（x,y） = sigmal(x) + x balabala\n\n\n## 回传流中的模式\n\n主要就是 \n\n加法门单元: 把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门把梯度2.00不变且相等地路由给了两个输入。\n\ny = 1.0 x a + 1.0 x b\n\n取最大值门单元: 对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了z变量，因为z的值比w高，于是w的梯度保持为0。\n\ny = max(high, low) = 1.0 x high + 0 x low\n\n乘法门单元: 相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.00x2.00=-8.00。\n\ny = ab, a的梯度是b\n\n\n## 用向量化操作计算梯度\n\n其实就是矩阵相乘之类，作者建议通过维度来推测计算是否正确\n\n## Summary：\n\n对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。(梯度如何计算)\n\n讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。（不难理解，但是不知道实际操作是否需要，我感觉每一层都会有自己计算梯度的方法）\n\n# Reference:\n\n[反向传播](https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit)\n\n","slug":"cs231n-note-4","published":1,"updated":"2018-07-11T04:33:39.544Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiel03000oim0r76medl1q","content":"<h1 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h1><h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction:\"></a>Introduction:</h1><p>反向传播是利用链式法则递归计算表达式的梯度的方法。 根据函数 f(x), 求关于f 关于 x的梯度 gradient</p>\n<p><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c86221324a9d066ca28310ee772941748a5f370f\" alt=\"\"></p>\n<h2 id=\"偏导数\"><a href=\"#偏导数\" class=\"headerlink\" title=\"偏导数\"></a>偏导数</h2><p>（我的理解就是当对其中一个param 求导的时候视其他的params为const， 含义为当这个param 增加的时候 对f 的增加量的比值）</p>\n<h2 id=\"链式法则\"><a href=\"#链式法则\" class=\"headerlink\" title=\"链式法则\"></a>链式法则</h2><p>Chain rule</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%7D%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+x%7D\" alt=\"\"></p>\n<p>这下面这个sample给的真的是清楚</p>\n<p>x = -2; y = 5; z = -4</p>\n<p>f(x,y,z) = (x+y)z</p>\n<p><img src=\"https://pic4.zhimg.com/80/213da7f66594510b45989bd134fc2d8b_hd.jpg\" alt=\"\"></p>\n<p>绿色的从输入端到输出端 为前向传播</p>\n<p>红色的从输出端到输入端 为反向传播</p>\n<h2 id=\"反向传播的理解\"><a href=\"#反向传播的理解\" class=\"headerlink\" title=\"反向传播的理解\"></a>反向传播的理解</h2><p>反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：</p>\n<p>1.这个门的输出值，</p>\n<p>2.其输出值关于输入值的局部梯度。</p>\n<p>门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。</p>\n<p>因为在加法门中 是 y = a+b 所以slope 也就是梯度都是1， 在乘法门中 y = ab, 所以a 的梯度就是b， b的梯度就是a</p>\n<h2 id=\"Example\"><a href=\"#Example\" class=\"headerlink\" title=\"Example\"></a>Example</h2><p>w = [2,-3,-3] # 假设一些随机数据和权重<br>x = [-1, -2]</p>\n<pre><code># 前向传播\ndot = w[0]*x[0] + w[1]*x[1] + w[2]\nf = 1.0 / (1 + math.exp(-dot)) # sigmoid函数\n\n# 对神经元反向传播\nddot = (1 - f) * f # 点积变量的梯度, 使用sigmoid函数求导\ndx = [w[0] * ddot, w[1] * ddot] # 回传到x\ndw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # 回传到w\n# 完成！得到输入的梯度\n</code></pre><p>Tip:</p>\n<p>对前向传播变量进行缓存：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。</p>\n<p>在不同分支的梯度要相加：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。 比如 f（x,y） = sigmal(x) + x balabala</p>\n<h2 id=\"回传流中的模式\"><a href=\"#回传流中的模式\" class=\"headerlink\" title=\"回传流中的模式\"></a>回传流中的模式</h2><p>主要就是 </p>\n<p>加法门单元: 把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门把梯度2.00不变且相等地路由给了两个输入。</p>\n<p>y = 1.0 x a + 1.0 x b</p>\n<p>取最大值门单元: 对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了z变量，因为z的值比w高，于是w的梯度保持为0。</p>\n<p>y = max(high, low) = 1.0 x high + 0 x low</p>\n<p>乘法门单元: 相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.00x2.00=-8.00。</p>\n<p>y = ab, a的梯度是b</p>\n<h2 id=\"用向量化操作计算梯度\"><a href=\"#用向量化操作计算梯度\" class=\"headerlink\" title=\"用向量化操作计算梯度\"></a>用向量化操作计算梯度</h2><p>其实就是矩阵相乘之类，作者建议通过维度来推测计算是否正确</p>\n<h2 id=\"Summary：\"><a href=\"#Summary：\" class=\"headerlink\" title=\"Summary：\"></a>Summary：</h2><p>对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。(梯度如何计算)</p>\n<p>讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。（不难理解，但是不知道实际操作是否需要，我感觉每一层都会有自己计算梯度的方法）</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h1><p><a href=\"https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit\" target=\"_blank\" rel=\"noopener\">反向传播</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h1><h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction:\"></a>Introduction:</h1><p>反向传播是利用链式法则递归计算表达式的梯度的方法。 根据函数 f(x), 求关于f 关于 x的梯度 gradient</p>\n<p><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c86221324a9d066ca28310ee772941748a5f370f\" alt=\"\"></p>\n<h2 id=\"偏导数\"><a href=\"#偏导数\" class=\"headerlink\" title=\"偏导数\"></a>偏导数</h2><p>（我的理解就是当对其中一个param 求导的时候视其他的params为const， 含义为当这个param 增加的时候 对f 的增加量的比值）</p>\n<h2 id=\"链式法则\"><a href=\"#链式法则\" class=\"headerlink\" title=\"链式法则\"></a>链式法则</h2><p>Chain rule</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%7D%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+x%7D\" alt=\"\"></p>\n<p>这下面这个sample给的真的是清楚</p>\n<p>x = -2; y = 5; z = -4</p>\n<p>f(x,y,z) = (x+y)z</p>\n<p><img src=\"https://pic4.zhimg.com/80/213da7f66594510b45989bd134fc2d8b_hd.jpg\" alt=\"\"></p>\n<p>绿色的从输入端到输出端 为前向传播</p>\n<p>红色的从输出端到输入端 为反向传播</p>\n<h2 id=\"反向传播的理解\"><a href=\"#反向传播的理解\" class=\"headerlink\" title=\"反向传播的理解\"></a>反向传播的理解</h2><p>反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：</p>\n<p>1.这个门的输出值，</p>\n<p>2.其输出值关于输入值的局部梯度。</p>\n<p>门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。</p>\n<p>因为在加法门中 是 y = a+b 所以slope 也就是梯度都是1， 在乘法门中 y = ab, 所以a 的梯度就是b， b的梯度就是a</p>\n<h2 id=\"Example\"><a href=\"#Example\" class=\"headerlink\" title=\"Example\"></a>Example</h2><p>w = [2,-3,-3] # 假设一些随机数据和权重<br>x = [-1, -2]</p>\n<pre><code># 前向传播\ndot = w[0]*x[0] + w[1]*x[1] + w[2]\nf = 1.0 / (1 + math.exp(-dot)) # sigmoid函数\n\n# 对神经元反向传播\nddot = (1 - f) * f # 点积变量的梯度, 使用sigmoid函数求导\ndx = [w[0] * ddot, w[1] * ddot] # 回传到x\ndw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # 回传到w\n# 完成！得到输入的梯度\n</code></pre><p>Tip:</p>\n<p>对前向传播变量进行缓存：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。</p>\n<p>在不同分支的梯度要相加：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用+=而不是=来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的多元链式法则，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。 比如 f（x,y） = sigmal(x) + x balabala</p>\n<h2 id=\"回传流中的模式\"><a href=\"#回传流中的模式\" class=\"headerlink\" title=\"回传流中的模式\"></a>回传流中的模式</h2><p>主要就是 </p>\n<p>加法门单元: 把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门把梯度2.00不变且相等地路由给了两个输入。</p>\n<p>y = 1.0 x a + 1.0 x b</p>\n<p>取最大值门单元: 对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了z变量，因为z的值比w高，于是w的梯度保持为0。</p>\n<p>y = max(high, low) = 1.0 x high + 0 x low</p>\n<p>乘法门单元: 相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，x的梯度是-4.00x2.00=-8.00。</p>\n<p>y = ab, a的梯度是b</p>\n<h2 id=\"用向量化操作计算梯度\"><a href=\"#用向量化操作计算梯度\" class=\"headerlink\" title=\"用向量化操作计算梯度\"></a>用向量化操作计算梯度</h2><p>其实就是矩阵相乘之类，作者建议通过维度来推测计算是否正确</p>\n<h2 id=\"Summary：\"><a href=\"#Summary：\" class=\"headerlink\" title=\"Summary：\"></a>Summary：</h2><p>对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。(梯度如何计算)</p>\n<p>讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。（不难理解，但是不知道实际操作是否需要，我感觉每一层都会有自己计算梯度的方法）</p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h1><p><a href=\"https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit\" target=\"_blank\" rel=\"noopener\">反向传播</a></p>\n"},{"title":"cs231n-note-6","date":"2018-07-29T21:06:19.000Z","_content":"\n# 神经网络笔记2\n\n# 设置数据和模型\n\n神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算, 这些做法共同定义了评分函数(score function)\n\n## 数据预处理\n\n3个常用的符号，数据矩阵X，假设其尺寸是[N x D]（N是数据样本的数量，D是数据的维度）\n\n\n### 均值减法(Mean subtraction):\n\n每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点.\n\nnumpy代码：\n\n```\nX -= np.mean(X, axis=0)\n```\n\n### 归一化（Normalization）\n\n数据的所有维度都归一化，使其数值范围都近似相等.\n\n有两种方法：\n\n第一种是先对数据做零中心化,然后每个维度都除以其标准差\n\n![](https://pic2.zhimg.com/80/e743b6777775b1671c3b5503d7afbbc4_hd.jpg)\n\n\nnumpy 代码\n\n```\nX -= np.mean(X, axis=0)\nX /= np.std(X, axis=0)\n```\n\n第二种方法是对每个维度都做归一化，每个维度最大值1，最小值-1\n\n\n### PCA和白化（Whitening）To learning\n\n```\n# 假设输入数据矩阵X的尺寸为[N x D]\nX -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)\ncov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵\n```\n\n\n## Reference:\n\n[神经网络笔记 2](https://zhuanlan.zhihu.com/p/21560667)\n","source":"_posts/cs231n-note-6.md","raw":"---\ntitle: cs231n-note-6\ndate: 2018-07-29 15:06:19\ntags:\n---\n\n# 神经网络笔记2\n\n# 设置数据和模型\n\n神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算, 这些做法共同定义了评分函数(score function)\n\n## 数据预处理\n\n3个常用的符号，数据矩阵X，假设其尺寸是[N x D]（N是数据样本的数量，D是数据的维度）\n\n\n### 均值减法(Mean subtraction):\n\n每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点.\n\nnumpy代码：\n\n```\nX -= np.mean(X, axis=0)\n```\n\n### 归一化（Normalization）\n\n数据的所有维度都归一化，使其数值范围都近似相等.\n\n有两种方法：\n\n第一种是先对数据做零中心化,然后每个维度都除以其标准差\n\n![](https://pic2.zhimg.com/80/e743b6777775b1671c3b5503d7afbbc4_hd.jpg)\n\n\nnumpy 代码\n\n```\nX -= np.mean(X, axis=0)\nX /= np.std(X, axis=0)\n```\n\n第二种方法是对每个维度都做归一化，每个维度最大值1，最小值-1\n\n\n### PCA和白化（Whitening）To learning\n\n```\n# 假设输入数据矩阵X的尺寸为[N x D]\nX -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)\ncov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵\n```\n\n\n## Reference:\n\n[神经网络笔记 2](https://zhuanlan.zhihu.com/p/21560667)\n","slug":"cs231n-note-6","published":1,"updated":"2018-07-29T22:03:15.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiel05000qim0r5ws17h1f","content":"<h1 id=\"神经网络笔记2\"><a href=\"#神经网络笔记2\" class=\"headerlink\" title=\"神经网络笔记2\"></a>神经网络笔记2</h1><h1 id=\"设置数据和模型\"><a href=\"#设置数据和模型\" class=\"headerlink\" title=\"设置数据和模型\"></a>设置数据和模型</h1><p>神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算, 这些做法共同定义了评分函数(score function)</p>\n<h2 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h2><p>3个常用的符号，数据矩阵X，假设其尺寸是[N x D]（N是数据样本的数量，D是数据的维度）</p>\n<h3 id=\"均值减法-Mean-subtraction\"><a href=\"#均值减法-Mean-subtraction\" class=\"headerlink\" title=\"均值减法(Mean subtraction):\"></a>均值减法(Mean subtraction):</h3><p>每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点.</p>\n<p>numpy代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X -= np.mean(X, axis=0)</span><br></pre></td></tr></table></figure>\n<h3 id=\"归一化（Normalization）\"><a href=\"#归一化（Normalization）\" class=\"headerlink\" title=\"归一化（Normalization）\"></a>归一化（Normalization）</h3><p>数据的所有维度都归一化，使其数值范围都近似相等.</p>\n<p>有两种方法：</p>\n<p>第一种是先对数据做零中心化,然后每个维度都除以其标准差</p>\n<p><img src=\"https://pic2.zhimg.com/80/e743b6777775b1671c3b5503d7afbbc4_hd.jpg\" alt=\"\"></p>\n<p>numpy 代码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X -= np.mean(X, axis=0)</span><br><span class=\"line\">X /= np.std(X, axis=0)</span><br></pre></td></tr></table></figure>\n<p>第二种方法是对每个维度都做归一化，每个维度最大值1，最小值-1</p>\n<h3 id=\"PCA和白化（Whitening）To-learning\"><a href=\"#PCA和白化（Whitening）To-learning\" class=\"headerlink\" title=\"PCA和白化（Whitening）To learning\"></a>PCA和白化（Whitening）To learning</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 假设输入数据矩阵X的尺寸为[N x D]</span><br><span class=\"line\">X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)</span><br><span class=\"line\">cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵</span><br></pre></td></tr></table></figure>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h2><p><a href=\"https://zhuanlan.zhihu.com/p/21560667\" target=\"_blank\" rel=\"noopener\">神经网络笔记 2</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"神经网络笔记2\"><a href=\"#神经网络笔记2\" class=\"headerlink\" title=\"神经网络笔记2\"></a>神经网络笔记2</h1><h1 id=\"设置数据和模型\"><a href=\"#设置数据和模型\" class=\"headerlink\" title=\"设置数据和模型\"></a>设置数据和模型</h1><p>神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算, 这些做法共同定义了评分函数(score function)</p>\n<h2 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h2><p>3个常用的符号，数据矩阵X，假设其尺寸是[N x D]（N是数据样本的数量，D是数据的维度）</p>\n<h3 id=\"均值减法-Mean-subtraction\"><a href=\"#均值减法-Mean-subtraction\" class=\"headerlink\" title=\"均值减法(Mean subtraction):\"></a>均值减法(Mean subtraction):</h3><p>每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点.</p>\n<p>numpy代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X -= np.mean(X, axis=0)</span><br></pre></td></tr></table></figure>\n<h3 id=\"归一化（Normalization）\"><a href=\"#归一化（Normalization）\" class=\"headerlink\" title=\"归一化（Normalization）\"></a>归一化（Normalization）</h3><p>数据的所有维度都归一化，使其数值范围都近似相等.</p>\n<p>有两种方法：</p>\n<p>第一种是先对数据做零中心化,然后每个维度都除以其标准差</p>\n<p><img src=\"https://pic2.zhimg.com/80/e743b6777775b1671c3b5503d7afbbc4_hd.jpg\" alt=\"\"></p>\n<p>numpy 代码</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X -= np.mean(X, axis=0)</span><br><span class=\"line\">X /= np.std(X, axis=0)</span><br></pre></td></tr></table></figure>\n<p>第二种方法是对每个维度都做归一化，每个维度最大值1，最小值-1</p>\n<h3 id=\"PCA和白化（Whitening）To-learning\"><a href=\"#PCA和白化（Whitening）To-learning\" class=\"headerlink\" title=\"PCA和白化（Whitening）To learning\"></a>PCA和白化（Whitening）To learning</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 假设输入数据矩阵X的尺寸为[N x D]</span><br><span class=\"line\">X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)</span><br><span class=\"line\">cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵</span><br></pre></td></tr></table></figure>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h2><p><a href=\"https://zhuanlan.zhihu.com/p/21560667\" target=\"_blank\" rel=\"noopener\">神经网络笔记 2</a></p>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2016-07-15T22:43:52.000Z","updated":"2016-07-15T22:43:52.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiel07000sim0r1u5136ls","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n"},{"title":"数学手册","date":"2018-07-30T06:01:07.000Z","_content":"\n# PCA的数学原理\n\nPCA（Principal Component Analysis）是一种常用的数据分析方法(降维)\n\n## 1. 数据的向量表示及降维问题\n\n我们要在降维的同时让数据信息资源的损失尽可能降低.\n\n朴素降维思考，比如男女column， M F，可以去掉一列，因为非黑即白。\n\neg: 数据记录为\n\n(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额) => （500，240，25，13，2312.15）T\n\n比如浏览量和访客数是有关联的，可以简单降维\n\n## 2. 向量的表示及基变换\n\n## 3. 内积与投影\n\n[内积](https://zh.wikipedia.org/wiki/%E7%82%B9%E7%A7%AF), 高中学的叫点乘，或者还看见有人叫点积（Dot Product）\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d7de7b9aa6a9bbc6f6435c24173c0597464c8420)\n\n$ \\vec A \\vec B = a_1 x b_1 + ... + a_n x b_n $\n\n在几何定义为向量A 和 向量B的长度 乘以 cos（夹角）如果B的模长度为1，那么AB 的点乘就是A 投影在B上的长度\n\n$ \\vec A \\vec B = \\left\\lvert \\vec A \\vec B \\right\\rvert x cos(\\theta) $\n\n## 4. 基 [basis](https://zh.wikipedia.org/wiki/%E5%9F%BA_(%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8))\n\n![(3,2)](https://pic2.zhimg.com/80/df6a713c1b97cc55bd20afce46ace718_hd.jpg)\n\n原理上是在x轴上的投影为3，在y轴上的投影为2。 从代数原理上，（3，2） = 3x（1，0）+2x（0，1）\n\n（1，0） 和 （0，1） 就是二维空间中的一组基，基的模长度往往为1\n\n我们可以变换这组基的方向，比如（1，1）， （-1，1），然后把他转化为基，即为 (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})和(-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})。 \n\n### 基变换 （二维）\n\n顺时针 theta 角度\n\ncos(theta) -sin(theta)\nsin(theta) cos(theta)\n\n## 5. 基变换的矩阵表示\n\n原坐标 [1 0], [0 1]\n\n[1 0]  [3]  => [3]\n[0 1]  [2]     [2]\n\n新坐标\n$$ (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}) $$ \n\n$$ (-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}) $$\n\n![](https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+3+%5C%5C+2+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+5%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D)\n\n当把M个N维向量变换到R个N维向量表示的新空间中去，数学表达式为\n\n![](https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+p_1+%5C%5C+p_2+%5C%5C+%5Cvdots+%5C%5C+p_R+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+a_1+%26+a_2+%26+%5Ccdots+%26+a_M+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+p_1a_1+%26+p_1a_2+%26+%5Ccdots+%26+p_1a_M+%5C%5C+p_2a_1+%26+p_2a_2+%26+%5Ccdots+%26+p_2a_M+%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C+p_Ra_1+%26+p_Ra_2+%26+%5Ccdots+%26+p_Ra_M+%5Cend%7Bpmatrix%7D)\n\nR是可以小于N的，R是基的数量，可以讲N维数据变换到更低的维度中去。这种矩阵相乘表示为降维变换。\n\n## 6. 协方差矩阵及优化目标\n\n\n## Reference\n\n\n[知乎PCA的数学原理 貌似也是转的，找不到原link](https://zhuanlan.zhihu.com/p/21580949) \n","source":"_posts/数学手册.md","raw":"---\ntitle: 数学手册\ndate: 2018-07-30 00:01:07\ntags: Math\n---\n\n# PCA的数学原理\n\nPCA（Principal Component Analysis）是一种常用的数据分析方法(降维)\n\n## 1. 数据的向量表示及降维问题\n\n我们要在降维的同时让数据信息资源的损失尽可能降低.\n\n朴素降维思考，比如男女column， M F，可以去掉一列，因为非黑即白。\n\neg: 数据记录为\n\n(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额) => （500，240，25，13，2312.15）T\n\n比如浏览量和访客数是有关联的，可以简单降维\n\n## 2. 向量的表示及基变换\n\n## 3. 内积与投影\n\n[内积](https://zh.wikipedia.org/wiki/%E7%82%B9%E7%A7%AF), 高中学的叫点乘，或者还看见有人叫点积（Dot Product）\n\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d7de7b9aa6a9bbc6f6435c24173c0597464c8420)\n\n$ \\vec A \\vec B = a_1 x b_1 + ... + a_n x b_n $\n\n在几何定义为向量A 和 向量B的长度 乘以 cos（夹角）如果B的模长度为1，那么AB 的点乘就是A 投影在B上的长度\n\n$ \\vec A \\vec B = \\left\\lvert \\vec A \\vec B \\right\\rvert x cos(\\theta) $\n\n## 4. 基 [basis](https://zh.wikipedia.org/wiki/%E5%9F%BA_(%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8))\n\n![(3,2)](https://pic2.zhimg.com/80/df6a713c1b97cc55bd20afce46ace718_hd.jpg)\n\n原理上是在x轴上的投影为3，在y轴上的投影为2。 从代数原理上，（3，2） = 3x（1，0）+2x（0，1）\n\n（1，0） 和 （0，1） 就是二维空间中的一组基，基的模长度往往为1\n\n我们可以变换这组基的方向，比如（1，1）， （-1，1），然后把他转化为基，即为 (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})和(-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})。 \n\n### 基变换 （二维）\n\n顺时针 theta 角度\n\ncos(theta) -sin(theta)\nsin(theta) cos(theta)\n\n## 5. 基变换的矩阵表示\n\n原坐标 [1 0], [0 1]\n\n[1 0]  [3]  => [3]\n[0 1]  [2]     [2]\n\n新坐标\n$$ (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}) $$ \n\n$$ (-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}) $$\n\n![](https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+3+%5C%5C+2+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+5%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D)\n\n当把M个N维向量变换到R个N维向量表示的新空间中去，数学表达式为\n\n![](https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+p_1+%5C%5C+p_2+%5C%5C+%5Cvdots+%5C%5C+p_R+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+a_1+%26+a_2+%26+%5Ccdots+%26+a_M+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+p_1a_1+%26+p_1a_2+%26+%5Ccdots+%26+p_1a_M+%5C%5C+p_2a_1+%26+p_2a_2+%26+%5Ccdots+%26+p_2a_M+%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C+p_Ra_1+%26+p_Ra_2+%26+%5Ccdots+%26+p_Ra_M+%5Cend%7Bpmatrix%7D)\n\nR是可以小于N的，R是基的数量，可以讲N维数据变换到更低的维度中去。这种矩阵相乘表示为降维变换。\n\n## 6. 协方差矩阵及优化目标\n\n\n## Reference\n\n\n[知乎PCA的数学原理 貌似也是转的，找不到原link](https://zhuanlan.zhihu.com/p/21580949) \n","slug":"数学手册","published":1,"updated":"2018-08-08T04:25:23.091Z","_id":"cjkdiel09000vim0rv9utn3xw","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"PCA的数学原理\"><a href=\"#PCA的数学原理\" class=\"headerlink\" title=\"PCA的数学原理\"></a>PCA的数学原理</h1><p>PCA（Principal Component Analysis）是一种常用的数据分析方法(降维)</p>\n<h2 id=\"1-数据的向量表示及降维问题\"><a href=\"#1-数据的向量表示及降维问题\" class=\"headerlink\" title=\"1. 数据的向量表示及降维问题\"></a>1. 数据的向量表示及降维问题</h2><p>我们要在降维的同时让数据信息资源的损失尽可能降低.</p>\n<p>朴素降维思考，比如男女column， M F，可以去掉一列，因为非黑即白。</p>\n<p>eg: 数据记录为</p>\n<p>(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额) =&gt; （500，240，25，13，2312.15）T</p>\n<p>比如浏览量和访客数是有关联的，可以简单降维</p>\n<h2 id=\"2-向量的表示及基变换\"><a href=\"#2-向量的表示及基变换\" class=\"headerlink\" title=\"2. 向量的表示及基变换\"></a>2. 向量的表示及基变换</h2><h2 id=\"3-内积与投影\"><a href=\"#3-内积与投影\" class=\"headerlink\" title=\"3. 内积与投影\"></a>3. 内积与投影</h2><p><a href=\"https://zh.wikipedia.org/wiki/%E7%82%B9%E7%A7%AF\" target=\"_blank\" rel=\"noopener\">内积</a>, 高中学的叫点乘，或者还看见有人叫点积（Dot Product）</p>\n<p><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d7de7b9aa6a9bbc6f6435c24173c0597464c8420\" alt=\"\"></p>\n<p>$ \\vec A \\vec B = a_1 x b_1 + … + a_n x b_n $</p>\n<p>在几何定义为向量A 和 向量B的长度 乘以 cos（夹角）如果B的模长度为1，那么AB 的点乘就是A 投影在B上的长度</p>\n<p>$ \\vec A \\vec B = \\left\\lvert \\vec A \\vec B \\right\\rvert x cos(\\theta) $</p>\n<h2 id=\"4-基-basis\"><a href=\"#4-基-basis\" class=\"headerlink\" title=\"4. 基 basis)\"></a>4. 基 <a href=\"https://zh.wikipedia.org/wiki/%E5%9F%BA_(%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8\" target=\"_blank\" rel=\"noopener\">basis</a>)</h2><p><img src=\"https://pic2.zhimg.com/80/df6a713c1b97cc55bd20afce46ace718_hd.jpg\" alt=\"(3,2)\"></p>\n<p>原理上是在x轴上的投影为3，在y轴上的投影为2。 从代数原理上，（3，2） = 3x（1，0）+2x（0，1）</p>\n<p>（1，0） 和 （0，1） 就是二维空间中的一组基，基的模长度往往为1</p>\n<p>我们可以变换这组基的方向，比如（1，1）， （-1，1），然后把他转化为基，即为 (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})和(-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})。 </p>\n<h3 id=\"基变换-（二维）\"><a href=\"#基变换-（二维）\" class=\"headerlink\" title=\"基变换 （二维）\"></a>基变换 （二维）</h3><p>顺时针 theta 角度</p>\n<p>cos(theta) -sin(theta)<br>sin(theta) cos(theta)</p>\n<h2 id=\"5-基变换的矩阵表示\"><a href=\"#5-基变换的矩阵表示\" class=\"headerlink\" title=\"5. 基变换的矩阵表示\"></a>5. 基变换的矩阵表示</h2><p>原坐标 [1 0], [0 1]</p>\n<p>[1 0]  [3]  =&gt; [3]<br>[0 1]  [2]     [2]</p>\n<p>新坐标<br>$$ (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}) $$ </p>\n<p>$$ (-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}) $$</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+3+%5C%5C+2+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+5%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D\" alt=\"\"></p>\n<p>当把M个N维向量变换到R个N维向量表示的新空间中去，数学表达式为</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+p_1+%5C%5C+p_2+%5C%5C+%5Cvdots+%5C%5C+p_R+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+a_1+%26+a_2+%26+%5Ccdots+%26+a_M+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+p_1a_1+%26+p_1a_2+%26+%5Ccdots+%26+p_1a_M+%5C%5C+p_2a_1+%26+p_2a_2+%26+%5Ccdots+%26+p_2a_M+%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C+p_Ra_1+%26+p_Ra_2+%26+%5Ccdots+%26+p_Ra_M+%5Cend%7Bpmatrix%7D\" alt=\"\"></p>\n<p>R是可以小于N的，R是基的数量，可以讲N维数据变换到更低的维度中去。这种矩阵相乘表示为降维变换。</p>\n<h2 id=\"6-协方差矩阵及优化目标\"><a href=\"#6-协方差矩阵及优化目标\" class=\"headerlink\" title=\"6. 协方差矩阵及优化目标\"></a>6. 协方差矩阵及优化目标</h2><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"https://zhuanlan.zhihu.com/p/21580949\" target=\"_blank\" rel=\"noopener\">知乎PCA的数学原理 貌似也是转的，找不到原link</a> </p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"PCA的数学原理\"><a href=\"#PCA的数学原理\" class=\"headerlink\" title=\"PCA的数学原理\"></a>PCA的数学原理</h1><p>PCA（Principal Component Analysis）是一种常用的数据分析方法(降维)</p>\n<h2 id=\"1-数据的向量表示及降维问题\"><a href=\"#1-数据的向量表示及降维问题\" class=\"headerlink\" title=\"1. 数据的向量表示及降维问题\"></a>1. 数据的向量表示及降维问题</h2><p>我们要在降维的同时让数据信息资源的损失尽可能降低.</p>\n<p>朴素降维思考，比如男女column， M F，可以去掉一列，因为非黑即白。</p>\n<p>eg: 数据记录为</p>\n<p>(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额) =&gt; （500，240，25，13，2312.15）T</p>\n<p>比如浏览量和访客数是有关联的，可以简单降维</p>\n<h2 id=\"2-向量的表示及基变换\"><a href=\"#2-向量的表示及基变换\" class=\"headerlink\" title=\"2. 向量的表示及基变换\"></a>2. 向量的表示及基变换</h2><h2 id=\"3-内积与投影\"><a href=\"#3-内积与投影\" class=\"headerlink\" title=\"3. 内积与投影\"></a>3. 内积与投影</h2><p><a href=\"https://zh.wikipedia.org/wiki/%E7%82%B9%E7%A7%AF\" target=\"_blank\" rel=\"noopener\">内积</a>, 高中学的叫点乘，或者还看见有人叫点积（Dot Product）</p>\n<p><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d7de7b9aa6a9bbc6f6435c24173c0597464c8420\" alt=\"\"></p>\n<p>$ \\vec A \\vec B = a_1 x b_1 + … + a_n x b_n $</p>\n<p>在几何定义为向量A 和 向量B的长度 乘以 cos（夹角）如果B的模长度为1，那么AB 的点乘就是A 投影在B上的长度</p>\n<p>$ \\vec A \\vec B = \\left\\lvert \\vec A \\vec B \\right\\rvert x cos(\\theta) $</p>\n<h2 id=\"4-基-basis\"><a href=\"#4-基-basis\" class=\"headerlink\" title=\"4. 基 basis)\"></a>4. 基 <a href=\"https://zh.wikipedia.org/wiki/%E5%9F%BA_(%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8\" target=\"_blank\" rel=\"noopener\">basis</a>)</h2><p><img src=\"https://pic2.zhimg.com/80/df6a713c1b97cc55bd20afce46ace718_hd.jpg\" alt=\"(3,2)\"></p>\n<p>原理上是在x轴上的投影为3，在y轴上的投影为2。 从代数原理上，（3，2） = 3x（1，0）+2x（0，1）</p>\n<p>（1，0） 和 （0，1） 就是二维空间中的一组基，基的模长度往往为1</p>\n<p>我们可以变换这组基的方向，比如（1，1）， （-1，1），然后把他转化为基，即为 (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})和(-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})。 </p>\n<h3 id=\"基变换-（二维）\"><a href=\"#基变换-（二维）\" class=\"headerlink\" title=\"基变换 （二维）\"></a>基变换 （二维）</h3><p>顺时针 theta 角度</p>\n<p>cos(theta) -sin(theta)<br>sin(theta) cos(theta)</p>\n<h2 id=\"5-基变换的矩阵表示\"><a href=\"#5-基变换的矩阵表示\" class=\"headerlink\" title=\"5. 基变换的矩阵表示\"></a>5. 基变换的矩阵表示</h2><p>原坐标 [1 0], [0 1]</p>\n<p>[1 0]  [3]  =&gt; [3]<br>[0 1]  [2]     [2]</p>\n<p>新坐标<br>$$ (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}) $$ </p>\n<p>$$ (-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}) $$</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%26+1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+3+%5C%5C+2+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+5%2F%5Csqrt%7B2%7D+%5C%5C+-1%2F%5Csqrt%7B2%7D+%5Cend%7Bpmatrix%7D\" alt=\"\"></p>\n<p>当把M个N维向量变换到R个N维向量表示的新空间中去，数学表达式为</p>\n<p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+p_1+%5C%5C+p_2+%5C%5C+%5Cvdots+%5C%5C+p_R+%5Cend%7Bpmatrix%7D+%5Cbegin%7Bpmatrix%7D+a_1+%26+a_2+%26+%5Ccdots+%26+a_M+%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D+p_1a_1+%26+p_1a_2+%26+%5Ccdots+%26+p_1a_M+%5C%5C+p_2a_1+%26+p_2a_2+%26+%5Ccdots+%26+p_2a_M+%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C%5C+p_Ra_1+%26+p_Ra_2+%26+%5Ccdots+%26+p_Ra_M+%5Cend%7Bpmatrix%7D\" alt=\"\"></p>\n<p>R是可以小于N的，R是基的数量，可以讲N维数据变换到更低的维度中去。这种矩阵相乘表示为降维变换。</p>\n<h2 id=\"6-协方差矩阵及优化目标\"><a href=\"#6-协方差矩阵及优化目标\" class=\"headerlink\" title=\"6. 协方差矩阵及优化目标\"></a>6. 协方差矩阵及优化目标</h2><h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p><a href=\"https://zhuanlan.zhihu.com/p/21580949\" target=\"_blank\" rel=\"noopener\">知乎PCA的数学原理 貌似也是转的，找不到原link</a> </p>\n"},{"title":"sequelize cli NodeJs Learning Notes","date":"2018-06-04T06:52:04.000Z","_content":"\nSequlize cli 是一个后端对象控制的一个library\n\n## Introduce\n\n首先要install： npm install --save sequelize-cli\n\n然后init： node_modules/.bin/sequelize init\n\n如果你是用Mac 你可以重命名 sequelize:\talias sequelize='node_modules/.bin/sequelize'\n\n设置 config/config.json 基本上就是列出 database的schema username password 之类的\n\n## Migration\n\n一种是create migration by model \n\n\tnode_modules/.bin/sequelize model:generate --name User\n\n还有一种是直接create migration\n\n\tnode_modules/.bin/sequelize migration:create --name add-wechatid-in-user\n\n## Seed\n\n在数据库里生成一些demo 或者 测试dat啊\n\n生成 demo-user 的 seed \n\n\tnode_modules/.bin/sequelize seed:generate --name demo-user\n\n在seeder 文件里\n\n\t'use strict';\n\tmodule.exports = {\n\t  up: (queryInterface, Sequelize) => {\n\t    return queryInterface.bulkInsert('Users', [{\n\t        firstName: 'John',\n\t        lastName: 'Doe',\n\t        email: 'demo@demo.com'\n\t      }], {});\n\t  },\n\n\t  down: (queryInterface, Sequelize) => {\n\t    return queryInterface.bulkDelete('Users', null, {});\n\t  }\n\t};\n\n\nReference： \n\n[Sequelize 手册](http://docs.sequelizejs.com/manual/tutorial/migrations.html)","source":"_posts/sequelize-cli-NodeJs-Learning-Notes.md","raw":"---\ntitle: sequelize cli NodeJs Learning Notes\ndate: 2018-06-04 00:52:04\ntags: NodeJs\n---\n\nSequlize cli 是一个后端对象控制的一个library\n\n## Introduce\n\n首先要install： npm install --save sequelize-cli\n\n然后init： node_modules/.bin/sequelize init\n\n如果你是用Mac 你可以重命名 sequelize:\talias sequelize='node_modules/.bin/sequelize'\n\n设置 config/config.json 基本上就是列出 database的schema username password 之类的\n\n## Migration\n\n一种是create migration by model \n\n\tnode_modules/.bin/sequelize model:generate --name User\n\n还有一种是直接create migration\n\n\tnode_modules/.bin/sequelize migration:create --name add-wechatid-in-user\n\n## Seed\n\n在数据库里生成一些demo 或者 测试dat啊\n\n生成 demo-user 的 seed \n\n\tnode_modules/.bin/sequelize seed:generate --name demo-user\n\n在seeder 文件里\n\n\t'use strict';\n\tmodule.exports = {\n\t  up: (queryInterface, Sequelize) => {\n\t    return queryInterface.bulkInsert('Users', [{\n\t        firstName: 'John',\n\t        lastName: 'Doe',\n\t        email: 'demo@demo.com'\n\t      }], {});\n\t  },\n\n\t  down: (queryInterface, Sequelize) => {\n\t    return queryInterface.bulkDelete('Users', null, {});\n\t  }\n\t};\n\n\nReference： \n\n[Sequelize 手册](http://docs.sequelizejs.com/manual/tutorial/migrations.html)","slug":"sequelize-cli-NodeJs-Learning-Notes","published":1,"updated":"2018-06-05T07:39:07.250Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiel0c000xim0rt0j7i7q0","content":"<p>Sequlize cli 是一个后端对象控制的一个library</p>\n<h2 id=\"Introduce\"><a href=\"#Introduce\" class=\"headerlink\" title=\"Introduce\"></a>Introduce</h2><p>首先要install： npm install –save sequelize-cli</p>\n<p>然后init： node_modules/.bin/sequelize init</p>\n<p>如果你是用Mac 你可以重命名 sequelize:    alias sequelize=’node_modules/.bin/sequelize’</p>\n<p>设置 config/config.json 基本上就是列出 database的schema username password 之类的</p>\n<h2 id=\"Migration\"><a href=\"#Migration\" class=\"headerlink\" title=\"Migration\"></a>Migration</h2><p>一种是create migration by model </p>\n<pre><code>node_modules/.bin/sequelize model:generate --name User\n</code></pre><p>还有一种是直接create migration</p>\n<pre><code>node_modules/.bin/sequelize migration:create --name add-wechatid-in-user\n</code></pre><h2 id=\"Seed\"><a href=\"#Seed\" class=\"headerlink\" title=\"Seed\"></a>Seed</h2><p>在数据库里生成一些demo 或者 测试dat啊</p>\n<p>生成 demo-user 的 seed </p>\n<pre><code>node_modules/.bin/sequelize seed:generate --name demo-user\n</code></pre><p>在seeder 文件里</p>\n<pre><code>&apos;use strict&apos;;\nmodule.exports = {\n  up: (queryInterface, Sequelize) =&gt; {\n    return queryInterface.bulkInsert(&apos;Users&apos;, [{\n        firstName: &apos;John&apos;,\n        lastName: &apos;Doe&apos;,\n        email: &apos;demo@demo.com&apos;\n      }], {});\n  },\n\n  down: (queryInterface, Sequelize) =&gt; {\n    return queryInterface.bulkDelete(&apos;Users&apos;, null, {});\n  }\n};\n</code></pre><p>Reference： </p>\n<p><a href=\"http://docs.sequelizejs.com/manual/tutorial/migrations.html\" target=\"_blank\" rel=\"noopener\">Sequelize 手册</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Sequlize cli 是一个后端对象控制的一个library</p>\n<h2 id=\"Introduce\"><a href=\"#Introduce\" class=\"headerlink\" title=\"Introduce\"></a>Introduce</h2><p>首先要install： npm install –save sequelize-cli</p>\n<p>然后init： node_modules/.bin/sequelize init</p>\n<p>如果你是用Mac 你可以重命名 sequelize:    alias sequelize=’node_modules/.bin/sequelize’</p>\n<p>设置 config/config.json 基本上就是列出 database的schema username password 之类的</p>\n<h2 id=\"Migration\"><a href=\"#Migration\" class=\"headerlink\" title=\"Migration\"></a>Migration</h2><p>一种是create migration by model </p>\n<pre><code>node_modules/.bin/sequelize model:generate --name User\n</code></pre><p>还有一种是直接create migration</p>\n<pre><code>node_modules/.bin/sequelize migration:create --name add-wechatid-in-user\n</code></pre><h2 id=\"Seed\"><a href=\"#Seed\" class=\"headerlink\" title=\"Seed\"></a>Seed</h2><p>在数据库里生成一些demo 或者 测试dat啊</p>\n<p>生成 demo-user 的 seed </p>\n<pre><code>node_modules/.bin/sequelize seed:generate --name demo-user\n</code></pre><p>在seeder 文件里</p>\n<pre><code>&apos;use strict&apos;;\nmodule.exports = {\n  up: (queryInterface, Sequelize) =&gt; {\n    return queryInterface.bulkInsert(&apos;Users&apos;, [{\n        firstName: &apos;John&apos;,\n        lastName: &apos;Doe&apos;,\n        email: &apos;demo@demo.com&apos;\n      }], {});\n  },\n\n  down: (queryInterface, Sequelize) =&gt; {\n    return queryInterface.bulkDelete(&apos;Users&apos;, null, {});\n  }\n};\n</code></pre><p>Reference： </p>\n<p><a href=\"http://docs.sequelizejs.com/manual/tutorial/migrations.html\" target=\"_blank\" rel=\"noopener\">Sequelize 手册</a></p>\n"},{"title":"中国剩余定理","date":"2018-07-03T04:32:36.000Z","_content":"\n在去年曾经tutor过一些数学竞赛，很多题目有关一个数除x余y， 然后求这个数是多少的。\n\n比如 一个数当除以3余2，除以5余3，除以7余2，那么这个数是多少\n\nx = 2 mod 3\n\nx = 3 mod 5\n\nx = 2 mod 7\n\n\n这个时候分成3个mod 部分分别求基础解。\n\n比如第一部分 另外两个mod的乘积为 5 x 7 = 35\n\n然后在mod 3 中求 35 的倒数\n\n35 x 2 = 1 mod 3\n\n再把 倒数 x 一开始余的数 x 另外两个mod 的乘积 = 2 x 2 x 35 = 140\n\n然后同理，求出另外两个mod的结果再加在一起 = 233，\n\n那么所有233 + (3x5x7) x k 的数都满足这个条件\n\n接下来我想尝试写一个python 程序解决所有诸如此类的问题\n\nTodo\n\n\tdef getTMod(t, mod):\n\t\tif (mod//t)*t == mod:\n\t\t\treturn false\n\n\t\tproduct = t%mod\n\t\tresult = 1\n\t\twhile (product != 1 and result < mod):\n\t\t\tproduct += t\n\t\t\tresult += 1:\n\n","source":"_posts/中国剩余定理.md","raw":"---\ntitle: 中国剩余定理\ndate: 2018-07-02 22:32:36\ntags: math\n---\n\n在去年曾经tutor过一些数学竞赛，很多题目有关一个数除x余y， 然后求这个数是多少的。\n\n比如 一个数当除以3余2，除以5余3，除以7余2，那么这个数是多少\n\nx = 2 mod 3\n\nx = 3 mod 5\n\nx = 2 mod 7\n\n\n这个时候分成3个mod 部分分别求基础解。\n\n比如第一部分 另外两个mod的乘积为 5 x 7 = 35\n\n然后在mod 3 中求 35 的倒数\n\n35 x 2 = 1 mod 3\n\n再把 倒数 x 一开始余的数 x 另外两个mod 的乘积 = 2 x 2 x 35 = 140\n\n然后同理，求出另外两个mod的结果再加在一起 = 233，\n\n那么所有233 + (3x5x7) x k 的数都满足这个条件\n\n接下来我想尝试写一个python 程序解决所有诸如此类的问题\n\nTodo\n\n\tdef getTMod(t, mod):\n\t\tif (mod//t)*t == mod:\n\t\t\treturn false\n\n\t\tproduct = t%mod\n\t\tresult = 1\n\t\twhile (product != 1 and result < mod):\n\t\t\tproduct += t\n\t\t\tresult += 1:\n\n","slug":"中国剩余定理","published":1,"updated":"2018-07-29T22:06:55.565Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiel0d0010im0r18nixpks","content":"<p>在去年曾经tutor过一些数学竞赛，很多题目有关一个数除x余y， 然后求这个数是多少的。</p>\n<p>比如 一个数当除以3余2，除以5余3，除以7余2，那么这个数是多少</p>\n<p>x = 2 mod 3</p>\n<p>x = 3 mod 5</p>\n<p>x = 2 mod 7</p>\n<p>这个时候分成3个mod 部分分别求基础解。</p>\n<p>比如第一部分 另外两个mod的乘积为 5 x 7 = 35</p>\n<p>然后在mod 3 中求 35 的倒数</p>\n<p>35 x 2 = 1 mod 3</p>\n<p>再把 倒数 x 一开始余的数 x 另外两个mod 的乘积 = 2 x 2 x 35 = 140</p>\n<p>然后同理，求出另外两个mod的结果再加在一起 = 233，</p>\n<p>那么所有233 + (3x5x7) x k 的数都满足这个条件</p>\n<p>接下来我想尝试写一个python 程序解决所有诸如此类的问题</p>\n<p>Todo</p>\n<pre><code>def getTMod(t, mod):\n    if (mod//t)*t == mod:\n        return false\n\n    product = t%mod\n    result = 1\n    while (product != 1 and result &lt; mod):\n        product += t\n        result += 1:\n</code></pre>","site":{"data":{}},"excerpt":"","more":"<p>在去年曾经tutor过一些数学竞赛，很多题目有关一个数除x余y， 然后求这个数是多少的。</p>\n<p>比如 一个数当除以3余2，除以5余3，除以7余2，那么这个数是多少</p>\n<p>x = 2 mod 3</p>\n<p>x = 3 mod 5</p>\n<p>x = 2 mod 7</p>\n<p>这个时候分成3个mod 部分分别求基础解。</p>\n<p>比如第一部分 另外两个mod的乘积为 5 x 7 = 35</p>\n<p>然后在mod 3 中求 35 的倒数</p>\n<p>35 x 2 = 1 mod 3</p>\n<p>再把 倒数 x 一开始余的数 x 另外两个mod 的乘积 = 2 x 2 x 35 = 140</p>\n<p>然后同理，求出另外两个mod的结果再加在一起 = 233，</p>\n<p>那么所有233 + (3x5x7) x k 的数都满足这个条件</p>\n<p>接下来我想尝试写一个python 程序解决所有诸如此类的问题</p>\n<p>Todo</p>\n<pre><code>def getTMod(t, mod):\n    if (mod//t)*t == mod:\n        return false\n\n    product = t%mod\n    result = 1\n    while (product != 1 and result &lt; mod):\n        product += t\n        result += 1:\n</code></pre>"},{"title":"有趣的生物灵魂","date":"2018-06-23T21:20:06.000Z","_content":"\n# 变性之差一段DNA\n\n30 年前就有一支生物团队发现一段Sry 基因，可以使雌性小白鼠发育成雄性。\n\n最近有新的进展发现Sry 基因可以控制合成SOX9的蛋白，动物会在这段蛋白的影响下发育出雄性的生殖器官。\n\n反之如果雄性缺少这段蛋白也能变成雌性。\n\n## Reference:\n\n(Evidence to exclude SOX9 as a candidate gene for XY sex reversal without skeletal malformation)[http://jmg.bmj.com/content/jmedgenet/33/9/800.full.pdf]\n\n# 非双螺旋的新DNA结构\n\n新的结构命名为“i-motif”, 类似于结。研究人员发现i-motif-随着时间的推移而出现和消失，所以它们正在形成，溶解并再次形成。\n\nI-基序（i-motif） 可能帮助开启或关闭基因，并影响基因是否被主动读取。\n\n另外一种天然的结构\n\nG四联体（G quadruplex，又称G-tetrads或G4-DNA）\n\n![](https://pic4.zhimg.com/v2-15a84b7be9cc709a932a465f524db957_b.gif)\n\n其中研究较为深入的区域，是染色体末端的端粒（telomere）。\n\n(DNA可不只有双螺旋结构)[https://zhuanlan.zhihu.com/p/36216255]","source":"_posts/有趣的生物灵魂.md","raw":"---\ntitle: 有趣的生物灵魂\ndate: 2018-06-23 15:20:06\ntags: Biology\n---\n\n# 变性之差一段DNA\n\n30 年前就有一支生物团队发现一段Sry 基因，可以使雌性小白鼠发育成雄性。\n\n最近有新的进展发现Sry 基因可以控制合成SOX9的蛋白，动物会在这段蛋白的影响下发育出雄性的生殖器官。\n\n反之如果雄性缺少这段蛋白也能变成雌性。\n\n## Reference:\n\n(Evidence to exclude SOX9 as a candidate gene for XY sex reversal without skeletal malformation)[http://jmg.bmj.com/content/jmedgenet/33/9/800.full.pdf]\n\n# 非双螺旋的新DNA结构\n\n新的结构命名为“i-motif”, 类似于结。研究人员发现i-motif-随着时间的推移而出现和消失，所以它们正在形成，溶解并再次形成。\n\nI-基序（i-motif） 可能帮助开启或关闭基因，并影响基因是否被主动读取。\n\n另外一种天然的结构\n\nG四联体（G quadruplex，又称G-tetrads或G4-DNA）\n\n![](https://pic4.zhimg.com/v2-15a84b7be9cc709a932a465f524db957_b.gif)\n\n其中研究较为深入的区域，是染色体末端的端粒（telomere）。\n\n(DNA可不只有双螺旋结构)[https://zhuanlan.zhihu.com/p/36216255]","slug":"有趣的生物灵魂","published":1,"updated":"2018-07-17T02:55:00.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjkdiel0g0012im0rfloml6ho","content":"<h1 id=\"变性之差一段DNA\"><a href=\"#变性之差一段DNA\" class=\"headerlink\" title=\"变性之差一段DNA\"></a>变性之差一段DNA</h1><p>30 年前就有一支生物团队发现一段Sry 基因，可以使雌性小白鼠发育成雄性。</p>\n<p>最近有新的进展发现Sry 基因可以控制合成SOX9的蛋白，动物会在这段蛋白的影响下发育出雄性的生殖器官。</p>\n<p>反之如果雄性缺少这段蛋白也能变成雌性。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h2><p>(Evidence to exclude SOX9 as a candidate gene for XY sex reversal without skeletal malformation)[<a href=\"http://jmg.bmj.com/content/jmedgenet/33/9/800.full.pdf\" target=\"_blank\" rel=\"noopener\">http://jmg.bmj.com/content/jmedgenet/33/9/800.full.pdf</a>]</p>\n<h1 id=\"非双螺旋的新DNA结构\"><a href=\"#非双螺旋的新DNA结构\" class=\"headerlink\" title=\"非双螺旋的新DNA结构\"></a>非双螺旋的新DNA结构</h1><p>新的结构命名为“i-motif”, 类似于结。研究人员发现i-motif-随着时间的推移而出现和消失，所以它们正在形成，溶解并再次形成。</p>\n<p>I-基序（i-motif） 可能帮助开启或关闭基因，并影响基因是否被主动读取。</p>\n<p>另外一种天然的结构</p>\n<p>G四联体（G quadruplex，又称G-tetrads或G4-DNA）</p>\n<p><img src=\"https://pic4.zhimg.com/v2-15a84b7be9cc709a932a465f524db957_b.gif\" alt=\"\"></p>\n<p>其中研究较为深入的区域，是染色体末端的端粒（telomere）。</p>\n<p>(DNA可不只有双螺旋结构)[<a href=\"https://zhuanlan.zhihu.com/p/36216255\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/36216255</a>]</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"变性之差一段DNA\"><a href=\"#变性之差一段DNA\" class=\"headerlink\" title=\"变性之差一段DNA\"></a>变性之差一段DNA</h1><p>30 年前就有一支生物团队发现一段Sry 基因，可以使雌性小白鼠发育成雄性。</p>\n<p>最近有新的进展发现Sry 基因可以控制合成SOX9的蛋白，动物会在这段蛋白的影响下发育出雄性的生殖器官。</p>\n<p>反之如果雄性缺少这段蛋白也能变成雌性。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h2><p>(Evidence to exclude SOX9 as a candidate gene for XY sex reversal without skeletal malformation)[<a href=\"http://jmg.bmj.com/content/jmedgenet/33/9/800.full.pdf\" target=\"_blank\" rel=\"noopener\">http://jmg.bmj.com/content/jmedgenet/33/9/800.full.pdf</a>]</p>\n<h1 id=\"非双螺旋的新DNA结构\"><a href=\"#非双螺旋的新DNA结构\" class=\"headerlink\" title=\"非双螺旋的新DNA结构\"></a>非双螺旋的新DNA结构</h1><p>新的结构命名为“i-motif”, 类似于结。研究人员发现i-motif-随着时间的推移而出现和消失，所以它们正在形成，溶解并再次形成。</p>\n<p>I-基序（i-motif） 可能帮助开启或关闭基因，并影响基因是否被主动读取。</p>\n<p>另外一种天然的结构</p>\n<p>G四联体（G quadruplex，又称G-tetrads或G4-DNA）</p>\n<p><img src=\"https://pic4.zhimg.com/v2-15a84b7be9cc709a932a465f524db957_b.gif\" alt=\"\"></p>\n<p>其中研究较为深入的区域，是染色体末端的端粒（telomere）。</p>\n<p>(DNA可不只有双螺旋结构)[<a href=\"https://zhuanlan.zhihu.com/p/36216255\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/36216255</a>]</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjkdiekzd0006im0re5f9zrz1","category_id":"cjkdiekzl0009im0rzqqfm9nu","_id":"cjkdiekzz000iim0rlxsfwx82"}],"PostTag":[{"post_id":"cjkdiekzb0004im0rlqyr3w2s","tag_id":"cjkdiekzf0007im0r1nyayaxb","_id":"cjkdiekzt000dim0rbm3dhkj0"},{"post_id":"cjkdiekzd0006im0re5f9zrz1","tag_id":"cjkdiekzf0007im0r1nyayaxb","_id":"cjkdiel01000lim0r8ta6v6gd"},{"post_id":"cjkdiekzd0006im0re5f9zrz1","tag_id":"cjkdiekzw000fim0r6xtxolr1","_id":"cjkdiel02000nim0rzt392kix"},{"post_id":"cjkdiekzh0008im0rqmsb2yqj","tag_id":"cjkdiel00000kim0rj6re46vb","_id":"cjkdiel06000rim0rsc9kvou3"},{"post_id":"cjkdiel03000oim0r76medl1q","tag_id":"cjkdiel00000kim0rj6re46vb","_id":"cjkdiel08000tim0r6vufq8iw"},{"post_id":"cjkdiekzn000aim0rm4b55fm9","tag_id":"cjkdiel05000pim0rpn3978w1","_id":"cjkdiel0b000wim0rn3b2zs23"},{"post_id":"cjkdiel09000vim0rv9utn3xw","tag_id":"cjkdiekzw000fim0r6xtxolr1","_id":"cjkdiel0d000yim0re63o13bo"},{"post_id":"cjkdiekzr000cim0ricobeir6","tag_id":"cjkdiel08000uim0r7xw3687c","_id":"cjkdiel0f0011im0r3oxgjw4l"},{"post_id":"cjkdiekzu000eim0r28san2f3","tag_id":"cjkdiekzf0007im0r1nyayaxb","_id":"cjkdiel0j0014im0r4jpso1ya"},{"post_id":"cjkdiekzu000eim0r28san2f3","tag_id":"cjkdiel0d000zim0rhfl5phms","_id":"cjkdiel0k0015im0rauiltt85"},{"post_id":"cjkdiekzw000gim0rc799wmka","tag_id":"cjkdiel00000kim0rj6re46vb","_id":"cjkdiel0l0017im0r49vr4x3f"},{"post_id":"cjkdiekzy000him0r1l9bwtje","tag_id":"cjkdiel00000kim0rj6re46vb","_id":"cjkdiel0m0019im0rrjc46bwu"},{"post_id":"cjkdiekzz000jim0raky08wqw","tag_id":"cjkdiel00000kim0rj6re46vb","_id":"cjkdiel0n001bim0rfch7qxyt"},{"post_id":"cjkdiel0c000xim0rt0j7i7q0","tag_id":"cjkdiel0m001aim0r58ts10qs","_id":"cjkdiel0o001dim0rx0bi97iz"},{"post_id":"cjkdiel0d0010im0r18nixpks","tag_id":"cjkdiel0n001cim0r61pj6ka4","_id":"cjkdiel0p001fim0rpe9xy6d3"},{"post_id":"cjkdiel0g0012im0rfloml6ho","tag_id":"cjkdiel0o001eim0r1exxw0js","_id":"cjkdiel0q001gim0rg8ph3ld9"}],"Tag":[{"name":"Algorithm","_id":"cjkdiekzf0007im0r1nyayaxb"},{"name":"Math","_id":"cjkdiekzw000fim0r6xtxolr1"},{"name":"Machine_Learning","_id":"cjkdiel00000kim0rj6re46vb"},{"name":"Web, Mobile","_id":"cjkdiel05000pim0rpn3978w1"},{"name":"python","_id":"cjkdiel08000uim0r7xw3687c"},{"name":"Machine-Learning","_id":"cjkdiel0d000zim0rhfl5phms"},{"name":"NodeJs","_id":"cjkdiel0m001aim0r58ts10qs"},{"name":"math","_id":"cjkdiel0n001cim0r61pj6ka4"},{"name":"Biology","_id":"cjkdiel0o001eim0r1exxw0js"}]}}